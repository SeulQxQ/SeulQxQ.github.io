<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="author" content="SeulQxQ"><meta name="keywords" content=""><meta name="description" content="Weekly report  Date: 23.09.04-23.09.10 Paper Title: Efficient Geometry-aware 3D Generative Adversarial Networks 1 Method, Contribution, Related Work. 1.1 Method: ​ A hybrid explicit-impli"><meta property="og:type" content="article"><meta property="og:title" content="每周总结(23.09.04-23.09.10)"><meta property="og:url" content="http://seulqxq.top/posts/38005/index.html"><meta property="og:site_name" content="Seul"><meta property="og:description" content="Weekly report  Date: 23.09.04-23.09.10 Paper Title: Efficient Geometry-aware 3D Generative Adversarial Networks 1 Method, Contribution, Related Work. 1.1 Method: ​ A hybrid explicit-impli"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://seulqxq.top/img/index/10.png"><meta property="article:published_time" content="2023-09-10T12:03:09.000Z"><meta property="article:modified_time" content="2023-12-21T12:22:04.508Z"><meta property="article:author" content="SeulQxQ"><meta property="article:tag" content="总结&amp;反思"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://seulqxq.top/img/index/10.png"><title>每周总结(23.09.04-23.09.10) - Seul</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.3.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/toubudaziji.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css"><link rel="stylesheet" href="/css/cloudedGlass.css"><link rel="stylesheet" href="/css/selection.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"seulqxq.top",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:60,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><header><div class="header-inner" style="height:90vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>SeulQxQ&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url('/img/2.jpg') no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.2)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="每周总结(23.09.04-23.09.10)"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-09-10 20:03" pubdate>2023年9月10日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 11k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 92 分钟</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">每周总结(23.09.04-23.09.10)</h1><div class="markdown-body"><div align="center"><h3>Weekly report</h3></div><p><strong>Date: 23.09.04-23.09.10</strong></p><h3 id="paper">Paper</h3><p><strong>Title: </strong><code>Efficient Geometry-aware 3D Generative Adversarial Networks</code></p><h5 id="method-contribution-related-work.">1 Method, Contribution, Related Work.</h5><p><strong>1.1 Method:</strong></p><p>​ A hybrid explicit-implicit 3D perception network has been designed, utilizing a <em>memory-efficient three-plane representation</em> to explicitly store features on axis-aligned planes aggregated by a <em>lightweight implicit feature decoder</em>. This approach aims to achieve efficient volume rendering and enhance the computational efficiency of 3D foundational rendering. It incorporates certain image space approximations deviating from traditional 3D foundational rendering, while introducing a dual-discriminative strategy that maintains consistency between neural rendering and the final output to regulate tendencies of view inconsistency.</p><blockquote><p>Explicit representation allows for fast evaluation but requires substantial memory, making it challenging to scale to high resolutions or complex scenes. Implicit representation, while advantageous in terms of memory efficiency and scene complexity, employs large fully connected networks for evaluation, leading to slow training speeds. Hence, explicit and implicit representations offer complementary benefits.</p></blockquote><figure><img src="1.png" srcset="/img/loading.gif" lazyload alt="三平面模型"><figcaption aria-hidden="true">三平面模型</figcaption></figure><p><strong>1.2 Contribution:</strong></p><ul><li>Introduced a three-plane-based 3D GAN framework that is both efficient and expressive, enabling high-resolution geometric perception image synthesis.</li><li>Developed a 3D GAN training strategy that promotes multi-view consistency through dual-discrimination and generator pose conditioning, while faithfully modeling pose-related attribute distributions present in real-world datasets, such as expressions.</li><li>Demonstrated the latest results in unconditional 3D perception image synthesis on the FFHQ and AFHQ Cats datasets, as well as high-quality 3D geometric graphics learned entirely from 2D outdoor images.</li></ul><p><strong>1.3 Related Work:</strong></p><ol type="1"><li>Neural scene representation and rendering</li></ol><p>​ A new hybrid explicit-implicit 3D perception network has been designed, which utilizes a memory-efficient three-plane representation to explicitly store features on axis-aligned planes aggregated by a lightweight implicit feature decoder, aiming to achieve efficient volume rendering.</p><ol start="2" type="1"><li>Generative 3D-aware image synthesis</li></ol><p>​ An efficient 3D GAN architecture with a 3D-based prior bias is crucial for successfully generating high-resolution, view-consistent images, and high-quality 3D shapes. Therefore, the authors adopted the following approaches:</p><ul><li>Directly leverage a 2D CNN feature generator, namely <em>StyleGAN2</em>.</li><li>The three-plane representation allows this paper's approach to utilize neural volume rendering as a prior bias, making it computationally more efficient than fully implicit 3D networks.</li><li>Employing an up-sampling based on 2D CNNs after neural rendering while introducing dual discriminators to mitigate view inconsistencies brought about by the up-sampling layers.</li></ul><h4 id="model-and-modules">2 Model and Modules</h4><p><strong>2.1 Tri-plane hybrid 3D represnetation</strong></p><p><code>IDEA:</code>Hybrid explicit-implicit tri-plane representation.</p><p><code>Implement:</code>Align explicit features along three axis-aligned orthogonal feature planes, each with a resolution of N×N×C, where N represents spatial dimensions and C stands for the number of channels. To query any 3D position point <em>x</em>, project it onto the three feature planes to retrieve the corresponding feature vector <span class="math inline">\((F_{xy} ~~,~~ F_{xz}~ , ~ F_{yz})\)</span> through bilinear interpolation and then aggregate these three feature vectors by summation. Finally, feed this aggregated feature F into a small decoder (MLP) to decode it into color and density.</p><p><strong>2.2 3D GAN framework</strong></p><p><code>IDEA:</code>Train a 3D GAN for collective perception image synthesis from 2D photos without the need for any explicit 3D or multi-view supervision. Simultaneously, use a pre-trained pose detector to associate each training image with a set of camera intrinsics and extrinsics(<em>Deep3DFaceReconstruction</em>).</p><p><code>Implement &amp;&amp; Overview:</code></p><figure><img src="2.jpg" srcset="/img/loading.gif" lazyload alt="Overview"><figcaption aria-hidden="true">Overview</figcaption></figure><blockquote><ol type="a"><li><p>A pose-conditioned StyleGAN2 feature generator and mapping network.</p></li><li><p>Three-plane 3D representation with a lightweight feature decoder.</p></li><li><p>A neural voxel renderer.</p></li><li><p>A super-resolution module.</p></li><li><p>A pose-conditioned StyleGAN2 discriminator with dual discrimination.</p></li></ol></blockquote><p>​ This architecture cleverly decouples feature generation and neural rendering, enabling the utilization of the powerful StyleGAN2 generator for generalizing 3D scenes. Furthermore, the lightweight three-plane 3D representation can effectively convey rich information and achieve high-quality 3D perception view synthesis in real-time. Additionally, a two-stage training strategy is employed to accelerate training speed. The first stage involves training with reduced <span class="math inline">\((64^2)\)</span> neural rendering resolution, while the second stage consists of short-term fine-tuning at full <span class="math inline">\((128^2)\)</span> neural rendering resolution.</p><p><strong>2.3 CNN generator backbone and rendering</strong></p><p><code>IDEA:</code>The features of the <em>three-plane representation are generated by the StyleGAN2 CNN generator</em>. Random latent codes and camera parameters are first processed by the mapping network to produce intermediate latent codes, which are then used to modulate the convolution kernels of the separately synthesized network.</p><p><code>Implement:</code>Change the output shape of the StyleGAN2 backbone network to generate a feature map of dimensions 256×256×96 instead of generating a three-channel RGB image. Sample features from the three planes and merge the features sampled from these three planes, which are then fed into a lightweight decoder (MLP) with a single hidden layer of 64 neurons and the activation function being <em>softplus</em>.</p><p><code>Module:</code> StyleGAN2, MLP</p><p><strong>2.4 Super resolution</strong></p><p><code>IDEA:</code> Perform volume rendering at intermediate resolution <span class="math inline">\((128^2)\)</span> and rely on image space convolutional upsampling for rendering to image sizes of <span class="math inline">\((256^2 ~ or ~ 512^2)\)</span>.</p><p><code>Implement:</code> Comprised of two blocks modulating the convolutional layers in StyleGAN2:</p><ul><li>Upsampling, increasing the resolution from 128×128×3 to 512×512×3.</li><li>Adapting the 32-channel feature map to the final RGB image.</li></ul><p><strong>2.5 Dual discrimination</strong></p><p><code>IDEA:</code>Utilize the StyleGAN2 discriminator and made it two modifications.</p><p><code>Implement:</code> A) Interpret the feature map as a low-resolution RGB image. The dual discriminator ensures consistency between low-resolution RGB images and high-resolution images by upsampling them using bilinear interpolation to the same 512×512×3 size and concatenating them with the adjusted <span class="math inline">\((I^+_{RGB})\)</span>, resulting in a 6-channel image. B) Concatenate the input 3-channel RGB image with its appropriately blurred counterpart to form a 6-channel image as the input to the discriminator.</p><p><code>Module:</code>StyleGAN2-ADA strategy</p><blockquote><p>StyleGAN2-ADA Strategy: Pass the camera's intrinsic and extrinsic matrices (P) to the discriminator as conditional labels. This modulation introduces additional information to guide the generator in learning the correct 3D priors.</p></blockquote><p><strong>2.6 Modeling pose-correlated attributions</strong></p><p><code>IDEA:</code>Introduced <em>generator pose conditioning</em> as a means to model and decouple the correlation between observed poses and other attributes in the training images.</p><p><code>Implement:</code>Following the StyleGAN2-ADA conditional generation strategy, propose a backbone mapping network that not only provides a latent code z but also takes camera parameters P as input.</p><h4 id="other-details">3 Other details</h4><p><strong>3.1 Pose Estimators</strong></p><p>​ Augment the dataset using horizontal flipping and utilize pre-existing pose estimation to extract approximate camera extrinsic parameters.</p><blockquote><p>Pre-existing pose estimation methods:</p><ol type="1"><li>https://github.com/Microsoft/Deep3DFaceReconstruction for generating pose data for faces (FFHQ).</li><li>https://github.com/kairess/cat_hipsterizer for generating pose data for cats (AFHQv2 Cats).</li></ol></blockquote><h3 id="other-work">Other Work</h3><p>​ To read the code of this paper, especially the pose estimators and run these code in the two links. Then I've been learning about StyleGAN2 to understand methods in this paper better.Summray</p><p>​ 阅读了一篇论文<code>Efficient Geometry-aware 3D Generative Adversarial Networks</code>，并将上次的论文重新总结了一下。去了解运行了姿态估计的代码，并且去了解了StyleGAN2的一些思想，和StyleGAN2-ADA自适应增强等一些知识。</p><h3 id="paper-1">Paper</h3><h4 id="title-efficient-geometry-aware-3d-generative-adversarial-networks"><strong>Title: </strong><code>Efficient Geometry-aware 3D Generative Adversarial Networks</code></h4><h5 id="一提出的方法贡献相关工作">一、提出的方法、贡献、相关工作</h5><p><strong>1.方法：</strong></p><p>​ 设计了一种混合显式-隐式3D感知网络，该网络使用内存高效的<code>三平面表示显式地</code>存储由轻量级<code>隐式特征解码器聚合</code>的轴对齐平面上的特征，以实现高效的体绘制，提高了3D基础渲染的计算效率。使用了一些偏离3D基础渲染的图像空间近似，同时引入了一种双重判别策略，该策略保持神经渲染和最终输出之间的一致性，以规范其视图不一致的趋势。</p><blockquote><p>显式表示可以进行快速评估，但是需要很大的内存，使得这种方式难以扩展到高分辨率或复杂场景。隐式表示虽然在内存效率和场景复杂性方面有优势，但是这种方法使用大型的全连接网络进行评估，使得训练速度缓慢。因此，显式和隐式表示提供了互补的好处。</p></blockquote><figure><img src="1.png" srcset="/img/loading.gif" lazyload alt="三平面模型"><figcaption aria-hidden="true">三平面模型</figcaption></figure><p><strong>2.贡献：</strong></p><ul><li>引入了一个基于三平面的3D GAN框架，该框架既高效又富有表现力，以实现高分辨率几何感知图像合成。</li><li>开发了一种3D GAN训练策略，通过双重判别和生成器姿势条件促进多视图一致性，同时忠实地建模现实世界数据集中存在的姿势相关属性分布（例如表达式）。</li><li>展示了在FFHQ和AFHQ Cats数据集上无条件3D感知图像合成的最新结果，以及完全从2D野外图像中学习的高质量3D几何图形。</li></ul><p><strong>3.相关工作：</strong></p><p>​ 1）Neural scene representation and rendering(神经场景表示和渲染)</p><p>​ 设计了一种新的混合显式隐式3D感知网络，该网络使用内存高效的三平面表示显式地存储由轻量级隐式特征解码器聚合的轴对齐平面上的特征，以实现高效的体绘制</p><p>​ 2）Generative 3D-aware image synthesis(生成式3D感知图像合成)</p><p>​ 具有基于3D的先验偏差的高效3D GAN架构对于成功生成高分辨率视图一致图像和高质量3D形状至关重要。所以作者采用了以下方法：</p><p>​ a. 直接利用基于2D CNN特征生成器，即<code>StyleGAN2</code>。</p><p>​ b. 三平面表示使得该论文的方法能利用神经体渲染作为先验偏差，在计算上比完全隐式3D网络更有效。</p><p>​ c. 在神经渲染后采用基于2D CNN的向上采样，同时引入双重辨别器去避免上采样层带来的视图不一致。</p><h5 id="二模型与模块">二、模型与模块</h5><p><strong>1. Tri-plane hybrid 3D representation(Tri-plane混合3D表示)</strong></p><p><code>思想：</code>hybrid explicit-implicit tri-plane representation(混合显式-隐式三平面表示)。</p><p><code>实现：</code>沿着三个轴对齐的正交特征平面对齐显式特征，每个特征平面的分辨率均为N×N×C，N为空间维度，C为通道数。通过将3D位置投影到三个特征平面中来查询任何3D位置点<code>x</code>，通过双线性插值检索相应的特征向量<span class="math inline">\((F_{xy} ~,~ F_{xz}~ , ~ F_{yz})\)</span>，然后通过求和来汇总这三个特征向量。最后将这个汇总的特征F输入到一个小型解码器(MLP)来解码为颜色和密度。</p><p><code>模块：</code>小型MLP网络。</p><p><strong>2. 3D GAN framework(3D GAN框架)</strong></p><p><code>思想：</code>训练一个3D GAN，用于从2D照片中进行集合感知图像合成，而无需任何显式3D或者多视图监督。同时使用现成的姿态检测器，将每个训练图像与一组相机内参和外参相关联(<code>Deep3DFaceReconstruction</code>)。</p><p><code>实现/Overview:</code></p><figure><img src="2.jpg" srcset="/img/loading.gif" lazyload alt="Overview"><figcaption aria-hidden="true">Overview</figcaption></figure><blockquote><p>​ a. 一个基于姿态条件的StyleGAN2特征生成器和映射网络。</p><p>​ b. 一个具有轻量级特征解码器的三平面3D表示。</p><p>​ c. 一个神经体素渲染器。</p><p>​ d. 一个超分辨率模块。</p><p>​ e. 一个基于姿态条件的具有双重辨别的StyleGAN2辨别器。</p></blockquote><p>​ 这个架构巧妙地将特征生成和神经渲染解耦，使得可以利用强大的StyleGAN2生成器进行3D场景的泛化。此外，轻量级的三平面3D表示既能够表达丰富的信息，又能够在实时中实现高质量的3D感知视图合成。同时，采用两阶段训练策略加速训练速度。第一个阶段：使用减少<span class="math inline">\((64^2)\)</span>神经渲染分辨率进行训练；第二个阶段：在完全<span class="math inline">\((128^2)\)</span>神经渲染分辨率上的短期微调。</p><p><strong>3. CNN generator backbone and rendering(CNN生成器主干和渲染)</strong></p><p><code>思想：</code>由<code>StyleGAN2 CNN生成器生成三平面表示的特征</code>。随机潜在代码和相机参数首先由映射网络处理以产生中间潜在代码，然后调制单独合成网络的卷积核。</p><p><code>实现：</code>改变StyleGAN2主干网络的输出形状，不是生成三通道RGB图像，而是生成一个256×256×96的特征图像。从三平面采样特征，并融汇从三个平面采样的特征，输入到轻量级解码器(MLP，64个神经元的单个隐藏层，激活函数：softplus)。</p><p><code>模块：</code>StyleGAN2、MLP</p><p><strong>4. Super resolution(超分辨率)</strong></p><p><code>思想：</code>使用中等分辨率<span class="math inline">\((128^2)\)</span>进行体渲染，并依靠图像空间卷积上采样神经渲染到<span class="math inline">\((256^2 ~ or ~ 512^2)\)</span>图像大小。</p><p><code>实现：</code>由StyleGAN2调制卷积层的两个块组成。1）上采样，将128×128×3分辨率提高到512×512×3的分辨率。2）调整32通道特征图到最终的RGB图像。</p><p><strong>5. Dual discrimination(双重辨别器)</strong></p><p><code>思想：</code>使用StyleGAN2的辨别器，并进行了两次修改。</p><p><code>实现：</code>1）将特征图解释为低分辨率RGB图像。双重辨别器确保低分辨率RGB图像与高分辨率图像的一致性，通过双线性上采样成同样512×512×3图像并与调整后的<span class="math inline">\((I^+_{RGB})\)</span>进行连接变成6通道图像。2）将输入的3通道RGB图像与其适当模糊后的图像进行连接，变成6通道图像作为辨别器的输入。</p><p><code>模块：</code>StyleGAN2-ADA策略</p><blockquote><p>​ StyleGAN2-ADA策略：将渲染相机的内外矩阵(P)传递给鉴别器作为条件标签。这种调节引入了额外的信息，指导生成器学习正确的3D先验。</p></blockquote><p><strong>6. Modeling pose-correlated attributes(建模姿态相关属性)</strong></p><p><code>思想：</code>引入了<code>generator pose conditioning(生成器姿势条件)</code>作为建模和解耦训练图像中观察到的姿势与其他属性之间的相关性的一种手段。</p><p><code>实现：</code>按照StyleGAN2-ADA条件生成策略，提出一个主干映射网络，不仅提供一个潜在代码z，同时提供相机参数P作为输入。</p><h5 id="三其他细节">三、其他细节</h5><p><strong>1. Pose Estimators(姿态估计)</strong></p><p>​ 用水平翻转的方法来扩充数据集，并使用现成的姿态估计来提取近似的相机外部参数。</p><blockquote><p>现成的姿态估计方法：</p><p>1）https://github.com/Microsoft/Deep3DFaceReconstruction，用来生成脸的数据集的姿态(FFHQ)。</p><p>2）https://github.com/kairess/cat_hipsterizer，用来生成猫的数据集的姿态(AFHQv2 Cats)。</p></blockquote><h4 id="titileshape-pose-and-appearance-from-a-single-image-via-bootstrapped-radiance-field-inversion">Titile：<code>Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion</code></h4><h5 id="一提出的方法与贡献">一、提出的方法与贡献</h5><p><strong>1.方法：</strong></p><p>​ 作者提出了一种新的方法，将无条件生成模型与混合反演范式相结合，从单个图像中恢复三维信息。具体来说，他们使用神经辐射场（NeRF）来表示三维场景，并使用编码器产生潜在表示和姿态的第一个猜测。然后，他们通过优化来细化这些初始估计，以获得更准确的重建。</p><p><strong>2.贡献：</strong></p><ul><li>引入了一个基于NeRF的端到端单视图三维重建管道。在这种情况下，我们成功地展示了CMR基准下自然图像的<span class="math inline">\(360^◦\)</span>对象重建。</li><li>提出了一种用于NeRF的混合反演方案，以加快预训练的3D感知生成器的反转。</li><li>受姿态估计文献的启发，我们提出了一种基于PnP的姿态估计器，它利用我们的框架并且不需要额外的数据假设。</li></ul><h5 id="二模型与模块-1">二、模型与模块</h5><p><strong>1. Unconditional generator pre-training（无条件生成器预训练框架）</strong></p><figure><img src="3.png" srcset="/img/loading.gif" lazyload alt="无条件生成器"><figcaption aria-hidden="true">无条件生成器</figcaption></figure><p><code>思想：</code>主要思想来自EG3D的主干网络，三平面编码。<code>该部分被框架使用基于NeRF的生成器G与2D图像鉴别器相结合。</code></p><p><code>模块：</code>StyleGAN2，SDF representation，Attention-based color mapping，Path Length Regularization revisited。</p><blockquote><p>StyleGAN2：生成模型，SDF representation：3D表示，</p><p><strong>Attention-based color mapping：提高颜色泛化性, Path Length Regularization revisited：使三平面解码器不正则化，提高学习率。</strong></p></blockquote><p><strong>2. Bootstrapping and pose estimation（自举和姿态估计）</strong></p><figure><img src="4.png" srcset="/img/loading.gif" lazyload alt="姿态估计"><figcaption aria-hidden="true">姿态估计</figcaption></figure><p><code>思想：</code>主要思想来自NOCS，<code>改进：是使用从无条件生成器生成的数据来训练编码器而不是手工数据。</code></p><p><code>实现：</code>1）冻结G并训练图像编码器E，联合估计对象的姿势及其潜在代码（自举）的初始猜测。2）对于姿态估计，我们采用了一种原则性的方法来预测屏幕空间中的规范映射通过透视n点(PnP)算法。<code>输入真实图像，将预测的规范映射转换为点云，并运行PnP求解器来恢复所有姿态参数(视图矩阵和焦距)。</code></p><p><code>模块：</code>SegFormer</p><blockquote><p><strong>训练SegFormer网络来从RGB图像中预测规范图和latent code w</strong></p></blockquote><p><strong>SegFormer分割网络图：</strong></p><figure><img src="5.png" srcset="/img/loading.gif" lazyload alt="分割网络"><figcaption aria-hidden="true">分割网络</figcaption></figure><p><strong>3. Reconstruction via hybrid GAN inversion（通过混合GAN反演重建）</strong></p><figure><img src="6.png" srcset="/img/loading.gif" lazyload alt="混合反演"><figcaption aria-hidden="true">混合反演</figcaption></figure><p>通过基于梯度的优化(混合反演)改进了几个步骤的姿态和潜在代码。损失函数：VGG</p><p><code>模块：</code>adaptive discriminator augmentation(ADA)</p><blockquote><p>有助于减少梯度的方差，使我们能够进一步提高学习率。</p></blockquote><h3 id="other-work-1">Other Work</h3><p>​ 阅读这篇论文的代码，特别是姿态估计器的部分，并在这两个链接中运行这些代码。然后，我一直在学习关于StyleGAN2，以更好地理解这篇论文中的方法。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%AF%8F%E5%91%A8%E5%9B%9E%E9%A1%BE/" class="category-chain-item">每周回顾</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%80%BB%E7%BB%93-%E5%8F%8D%E6%80%9D/">#总结&反思</a></div></div><div class="license-box my-3"><div class="license-title"><div>每周总结(23.09.04-23.09.10)</div><div>http://seulqxq.top/posts/38005/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>SeulQxQ</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年9月10日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/posts/51013/" title="每周总结(23.09.18-23.09.24)"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">每周总结(23.09.18-23.09.24)</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/posts/36354/" title="每周总结(23.08.28-23.09.03)"><span class="hidden-mobile">每周总结(23.08.28-23.09.03)</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><script type="text/javascript">Fluid.utils.loadComments("#comments",(function(){var t="github-light",e="github-dark",s=document.documentElement.getAttribute("data-user-color-scheme");s="dark"===s?e:t,window.UtterancesThemeLight=t,window.UtterancesThemeDark=e;var n=document.createElement("script");n.setAttribute("src","https://utteranc.es/client.js"),n.setAttribute("repo","SeulQxQ/blog-comments"),n.setAttribute("issue-term","pathname"),n.setAttribute("label","utterances"),n.setAttribute("theme",s),n.setAttribute("crossorigin","anonymous"),document.getElementById("comments").appendChild(n)}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://hexo.fluid-dev.com/docs/guide" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.3.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://cdn.staticfile.org/anchor-js/4.2.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach((t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")}))},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><script src="/js/backgroundize.js"></script></body></html>