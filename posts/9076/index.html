<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="dark"><head><meta charset="UTF-8"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@500&display=swap" rel="stylesheet"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="author" content="SeulQxQ"><meta name="keywords" content=""><meta name="description" content="Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation 1 提出的方法， 贡献，相关工作 1. 方法 基于π-GAN模型，用于无条件3D感知图像合成的生成模型，它讲随机latent code映射到一类对象的辐射场。作者同时优化两个目标: （1）π-G"><meta property="og:type" content="article"><meta property="og:title" content="Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation"><meta property="og:url" content="http://seulqxq.top/posts/9076/index.html"><meta property="og:site_name" content="Seul"><meta property="og:description" content="Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation 1 提出的方法， 贡献，相关工作 1. 方法 基于π-GAN模型，用于无条件3D感知图像合成的生成模型，它讲随机latent code映射到一类对象的辐射场。作者同时优化两个目标: （1）π-G"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://seulqxq.top/img/index/12.jpg"><meta property="article:published_time" content="2023-09-24T09:30:01.000Z"><meta property="article:modified_time" content="2024-01-08T02:00:41.760Z"><meta property="article:author" content="SeulQxQ"><meta property="article:tag" content="NeRF"><meta property="article:tag" content="精度"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://seulqxq.top/img/index/12.jpg"><title>Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation - Seul</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.3.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css"><link rel="stylesheet" href="/css/cloudedGlass.css"><link rel="stylesheet" href="/css/selection.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"seulqxq.top",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:50,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><header><div class="header-inner" style="height:90vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>SeulQxQ&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url('/img/2.jpg') no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.2)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-09-24 17:30" pubdate>2023年9月24日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 5.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 50 分钟</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="论文笔记" id="heading-0ae31ffbf610cbf8ad04d8ad0c630314" role="tab" data-toggle="collapse" href="#collapse-0ae31ffbf610cbf8ad04d8ad0c630314" aria-expanded="true">论文笔记 <span class="list-group-count">(9)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-0ae31ffbf610cbf8ad04d8ad0c630314" role="tabpanel" aria-labelledby="heading-0ae31ffbf610cbf8ad04d8ad0c630314"><div class="category-post-list"><a href="/posts/20173/" title="3D感知单视图图像合成" class="list-group-item list-group-item-action"><span class="category-post">3D感知单视图图像合成</span> </a><a href="/posts/29490/" title="Mip-NeRF - A Multiscale Representation for Anti-Aliasing Neural Radiance Fields" class="list-group-item list-group-item-action"><span class="category-post">Mip-NeRF - A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</span> </a><a href="/posts/32257/" title="TensoRF - Tensorial Radiance Fields" class="list-group-item list-group-item-action"><span class="category-post">TensoRF - Tensorial Radiance Fields</span> </a><a href="/posts/35133/" title="A Survey on Deep Generative 3D-aware Image Synthesis" class="list-group-item list-group-item-action"><span class="category-post">A Survey on Deep Generative 3D-aware Image Synthesis</span> </a><a href="/posts/43769/" title="Deep3DSketch+_ Rapid 3D Modeling from Single Free-hand Sketches" class="list-group-item list-group-item-action"><span class="category-post">Deep3DSketch+_ Rapid 3D Modeling from Single Free-hand Sketches</span> </a><a href="/posts/39032/" title="pi-GAN_ Periodic Implicit Generative Adversarial Networks for 3D-Aware" class="list-group-item list-group-item-action"><span class="category-post">pi-GAN_ Periodic Implicit Generative Adversarial Networks for 3D-Aware</span> </a><a href="/posts/7366/" title="Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion" class="list-group-item list-group-item-action"><span class="category-post">Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion</span> </a><a href="/posts/52856/" title="Efficient Geometry-aware 3D Generative Adversarial Networks" class="list-group-item list-group-item-action"><span class="category-post">Efficient Geometry-aware 3D Generative Adversarial Networks</span> </a><a href="/posts/9076/" title="Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation" class="list-group-item list-group-item-action active"><span class="category-post">Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</span></a></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</h1><div class="markdown-body"><h3 id="pix2nerf-unsupervised-conditional-π-gan-for-single-image-to-neural-radiance-fields-translation">Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</h3><h4 id="提出的方法-贡献相关工作">1 提出的方法， 贡献，相关工作</h4><p><strong>1. 方法</strong></p><p>基于π-GAN模型，用于无条件3D感知图像合成的生成模型，它讲随机latent code映射到一类对象的辐射场。作者同时优化两个目标: （1）π-GAN目标，以利用其高保真度的3D感知生成能力。（2）一个经过精心设计的重建目标，包括一个与π-GAN生成器耦合的编码器，形成一个自动编码器。</p><p>引入了将给定图像映射到latent space的编码器，并对其做了些优化：（1）训练π-GAN和添加的编码器将生成的图像映射回latent space。（2）将编码器与π-GAN的生成器结合形成带条件的GAN模型，同时使用对抗和重建损失对其进行训练。</p><p><strong>2. 贡献</strong></p><p>（1）提出Pix2NeRF，第一个无监督的单视图NeRF模型，可以从图像中学习场景辐射场，并且不需要3D信息，多视图或者姿态监督。</p><p>（2）提出基于NeRF的GAN反演。</p><p><strong>3. 相关工作</strong></p><p>作者的工作可以分类为特定类别的3D感知神经新视图合成方法，该方法基于NeRF和π-GAN。</p><ul><li>神经辐射场</li><li>基于NeRF的GAN</li><li>少样本NeRF</li></ul><h4 id="模型与模块">2 模型与模块</h4><p><strong>1. 总体架构</strong></p><p>​ Pix2NeRF由三个神经网络组成，一个生成器G、一个辨别器D，共同组成生成对抗网络，一个编码器E，与生成器G共同组成一个自动编码器。</p><p>​ 生成器约束：output view pose d and latent code z。</p><p>​ G：生成器，D：辨别器，E：编码器，I：RGB image，z：latent code，d：pose，p：prior distribution l：logit predict distribution。</p><figure><img src="1.png" srcset="/img/loading.gif" lazyload alt="1"><figcaption aria-hidden="true">1</figcaption></figure><p><strong>1.1 Overview</strong></p><p>​ 论文中的方法将编码器和生成器输入映射的图像的潜在表示解耦为内容<code>z</code>和姿态<code>d</code>，并对这两个部分进行单独处理。</p><p><img src="2.png" srcset="/img/loading.gif" lazyload alt="1" style="zoom:80%"></p><p>​ Pix2NeRF从一张输入的图像中解耦姿态和内容，并且生成一个内容的辐射场，该辐射场包括（1）解耦姿态下的输入一致性，（2）来自<span class="math inline">\(~p_d~\)</span>不同姿态下的一致性雨逼真性。为了完成这些目标，设计了几个训练目标：</p><ul><li>Generator（生成器）</li><li>Discriminator（辨别器）</li><li>GAN inversion（GAN反演）</li><li>Reconstruction（重建）</li><li>Conditional adversarial training（条件对抗训练）</li></ul><p><strong>2. GAN generator objective（GAN生成器）</strong></p><p>​ 对latent code <span class="math inline">\(z_{rand} \sim p_{z}\)</span> 和随机姿态<span class="math inline">\(d_{rand} \sim p_d\)</span> 成对采样，然后通过生成器去生成假的生成图像：<span class="math inline">\(I_{gen} ~ = ~ G(z_{rand},~d_{rand})\)</span>，最后输入到冻结的辨别器中：<span class="math inline">\(l_{gen}, ~ d_{gen} ~ = ~ D^*(I_{gen})\)</span>。（左上）</p><p>​ 使用 <code>MSE</code> 作为生成的姿态 <span class="math inline">\(d_{gen}\)</span> 和随机姿态输入 <span class="math inline">\(d_{rand}\)</span> 之间的损失函数，同时生成器G的损失函数如下： <span class="math display">\[ \begin{aligned} \mathcal{L}_{\mathrm{GAN}}(G)=\underset{\substack{z_{\text {rand }} \sim p_{\mathrm{z}} \\ d_{\text {rand }} \sim p_{d}}}{\mathbb{E}} [\text{softplus}\left(-l_{\text {gen}}\right)+ \left.\lambda_{\text {pos }}\left\|d_{\text {rand }}-d_{\text {gen}}\right\|_{2}^{2}\right] \end{aligned} \]</span> 其中 $ _{pos}$ 为微调权重因子。</p><p><strong>3. GAN discriminator objective（GAN辨别器）</strong></p><p>​ 对latent code <span class="math inline">\(z_{rand} \sim p_{z}\)</span> 和随机姿态<span class="math inline">\(d_{rand} \sim p_d\)</span> 成对采样，然后通过冻结的生成器去生成假的生成图像：<span class="math inline">\(I_{gen} ~ = ~ G^*(z_{rand},~d_{rand})\)</span>，然后辨别器D通过 <span class="math inline">\(G^*\)</span> 生成的的图像和真实图像 <span class="math inline">\(I_{real} \sim p_{real}\)</span> 来进行训练。（左下） <span class="math display">\[ l_{real},~ d_{real} ~=~D(I_{real}), \\ l_{gen}, ~ d_{gen} ~=~ D(I_{gen}). \]</span> ​ 考虑已知姿态的 <code>MSE</code> 监督而修改的鉴别器的损失函数 <span class="math inline">\(\mathcal{L}_{GAN}(D)\)</span> 可以表示为： <span class="math display">\[ \begin{aligned} \mathcal{L}_{\mathrm{GAN}}(D)~ = ~\underset{I_{\text {real}} \sim p_{\text {real}}}{\mathbb{E}} {\left[\operatorname{softplus}\left(-l_{\text {real}}\right)\right]~ + ~} \\ \underset{\substack{z_{\text {rand }} \sim p_{\mathrm{z}} \\ d_{\text {rand}} \sim p_{d}}}{\mathbb{E}} {\left[\operatorname{softplus}\left(l_{\text {gen}}\right)~ + ~\right.} \left.\lambda_{\text {pos}}\left\|d_{\text {rand}}-d_{\text {gen}}\right\|_{2}^{2}\right] \end{aligned} \]</span> 其中 $ _{pos}$ 为微调权重因子。</p><p><strong>4. GAN inversion objective（GAN反演）</strong></p><p>​ 联合优化编码器 E 和辨别器 D，并冻结生成器 G。目的是确保采样的内容和姿态与编码器从生成的图像中提取的内容和姿态之间的一致性。（左下） <span class="math display">\[ z_{pred}, ~ d_{pred} ~ = ~E(I_{gen}) \]</span> GAN反演使用 <code>MSE</code> 损失函数： <span class="math display">\[ \begin{array}{r} \mathcal{L}_{\mathrm{GAN}^{-1}}(E)=\underset{\substack{z_{\text {rand }} \sim p_{\mathrm{z}} \\ d_{\text {rand }} \sim p_{d}}}{\mathbb{E}}\left[\left\|z_{\text {pred }}-z_{\text {rand }}\right\|_{2}^{2} ~+ ~\right. \left.\left\|d_{\text {pred }}-d_{\text {rand }}\right\|_{2}^{2}\right] \end{array} \]</span> <strong>5. Reconstruction objective（重建）</strong></p><p>​ 通过使用编码器 E 提取其 latent code 和姿势预测来在真实图像上调节生成器 G，然后使用预测的姿势渲染其视图，<code>目的为促进图像空间中结构的的一致性，并使得图像更加清晰</code>。（右上） <span class="math display">\[ z_{pred}, ~ d_{pred} ~ = ~ E(I_{real}) \\ I_{recon} ~ = ~ G(z_{pred}, ~ d_{pred}) \]</span> 重建损失函数，基于 <code>MSE</code> 改进： <span class="math display">\[ \begin{array}{r} \mathcal{L}_{\text {recon }}(G, E)=\underset{\begin{array}{c} {I_{real} } \sim { p_{real} } \end{array}}{\mathbb{E}}\left[\left\|I_{\text {recon }}-I_{\text {real }}\right\|_{2}^{2} ~ + ~\right. \\ \lambda_{\text {ssim }} \mathcal{L}_{\text {ssim }}\left(I_{\text {recon }}, I_{\text {real }}\right) ~ + \\ \left.\lambda_{\text {vgg }} \mathcal{L}_{\text {vgg }}\left(I_{\text {recon }}, I_{\text {real }}\right)\right] \end{array} \]</span> 其中 <span class="math inline">\(\mathcal{L}_{ssim}\)</span> 为SSIM损失，<span class="math inline">\(\lambda_{ssim}\)</span> 为SSIM损失的权重因子；<span class="math inline">\(\mathcal{L}_{vgg}\)</span> 为 VGG 感知损失，<span class="math inline">\(\lambda_{vgg}\)</span> 为其权重因子。</p><p><strong>6. Conditional adversarial objective（条件对抗网络）</strong></p><p>​ 重建目标旨在提高由编码器E提取的单个视图的良好重建质量。这可能会导致网络组合趋向于预测微不重要的姿势，或者对从 <span class="math inline">\(p_d\)</span> 提取的其他姿势的重建进行不切实际的预测。为了缓解这一问题，当生成器在随机姿势下渲染出图像 <span class="math inline">\(I_{real}\)</span> 时，进一步应用对抗目标。 <span class="math display">\[ l_{cond}, ~ d_{cond} ~=~D^*(G(z_{pred}, ~d_{rand})) \]</span> 损失函数为： <span class="math display">\[ \mathcal{L}_{cond}(G, ~E) ~=~ \underset{\substack{\substack{I_{real}\sim p_{real}} \\ d_{rand} \sim p_d}}{\mathbb{E}} \left[\operatorname{softplus}(-l_{cond}) \right] \]</span> <strong>7. Encoder warm-up（编码器预热）</strong></p><p>​ 重建损失可能很容易占主导地位，导致模型过度拟合于输入视图，同时失去了表示3D的能力。因此，作者引入了一种简单的“预热”策略来应对这个问题。在训练协议的前半部分迭代中，冻结编码器，同时优化重建和条件对抗损失，并且仅优化生成器用于这两个目标。这作为生成器的预热，大致学习编码器输出与编码图像之间的对应关系。然后解冻编码器，使其能够进一步提炼其学习到的表示。在预热阶段之后，编码器和生成器直接形成了一个经过预训练的自动编码器，能够生成接近真实3D表示的结果，避免了繁琐的早期重建目标，这在与GAN目标相平衡非常困难的情况下尤为重要。作者通过消融研究展示了这一策略的必要性，并与仅为重建损失分配较小权重的情况进行了比较。</p><p><strong>8. Training and Inference（训练与推测）</strong></p><p>​ 判别器和 GAN 反演目标在每次迭代时都会进行优化；GAN 生成器目标在偶次迭代时进行优化；重建和条件对抗目标在奇数迭代期间通过加权因子 <span class="math inline">\(λrecon\)</span> 联合优化： <span class="math display">\[ \mathcal{L}_{odd} ~=~ \mathcal{L}_{cond} ~ + ~ \lambda_{recon}\mathcal{L}_{recon} \]</span> ​ 在推理阶段，Pix2NeRF 只需要一个输入图像，可以将其输入编码器 E，然后输入生成器 G，再加上任意选择的姿势以进行新颖的视图合成。同时，可以从先验分布 <span class="math inline">\(p_z\)</span> 中对其进行采样，而不是从编码器中获取 latent code z，以使模型像 π-GAN 一样合成新颖的样本。</p><h4 id="实验">3 实验</h4><p><strong>1.数据集</strong></p><p>CelebA：200k张==人脸照片==，使用==aligned==的版本，并且采用中心裁剪，裁剪出大致脸部面积，使用8k张照片作为测试集。</p><p>CARLA：包含 16 个==汽车模型==的 10k 图像，使用 Carla 驾驶模拟器以随机纹理渲染。</p><p>ShapeNet-SRN：该数据集包含来自 ShapeNet 的 50 个渲染视图，其中每个实例都有阿基米德螺旋相机姿势。由于 ShapeNet-SRN 数据集的验证和测试集中不包括下半球，因此作者过滤训练集以仅包含上半球。作者使用==椅子==分割来与之前的多视图方法进行比较。</p><p><strong>2.技术细节</strong></p><p><code>baseline：</code>==π-GAN（Pytorch）==,重用其生成器和鉴别器架构。</p><p><code>参数：</code>选择 latent code 先验分布 <span class="math inline">\(p_z\)</span> 作为 [-1, 1] 上的多元均匀分布。使用鉴别器架构作为编码器主干，并且在latent code 头部的末尾添加==tanh==，所有的模型使用Adam优化器进行300k迭代。CelebA模型在==resolution=64×64，batch size=48==，每条射线采样24个点的参数下进行训练，其辨别器、生成器、编码器的学习率分别为==2e-4,6e-5,2e-4==。对于其他模型在==resolution=32×32==，每条射线采样96个点的参数下进行训练，学习率分别为==4e-5,4e-4,4e-4==.</p><p><strong>3.结果对比</strong></p><p><img src="3.png" srcset="/img/loading.gif" lazyload alt="3" style="zoom:80%"></p><p><strong>4.消融实验</strong></p><p>通过逐个去除关键组件并在完整模型相同的设置下训练模型，来验证设计选择。</p><p>主要对比的模型组件有：==Naive GAN inversion, Auto-encoder, No GAN inversion, No conditional adversarial objective, Warm-up==</p><p><code>Naive GAN inversion:</code>在朴素的GAN反演中，使用一个预先训练好的GAN，冻结其权重，并训练一个编码器将图像映射到它们对应的 latent code。结果表明，编码器可以学习从图像到 latent code 的近似映射。</p><p><code>Auto-encoder:</code>利用 π-GAN 的架构作为自动编码器，并将==latent space==从pipeline中删除，只训练重建和条件对抗模型。得到的结果虽然又不错的质量，但是明显能观察到3D不一致现象。</p><p><code>No GAN inversion:</code>将==GAN inversion==从pipeline中删除之后，视觉结果变得模糊。这一步可能是 π-GAN 训练和重建之间的联系，影响整体性能。</p><p><code>No conditional adversarial objective:</code> 停用条件对抗损失并重新训练，导致渲染变得不完整，有明显的伪影，并且一致性降低。</p><p><code>Warm-up:</code>分别训练三个模型，==没有warm-up，没有unfreezing编码器（始终预热），为重建分配较低的权重而不是预热==。没有warm-up策略，导致过拟合输入视图，并且无法在新颖姿态中产生有意义的内容；没有解冻编码器，则蒸馏相对弱，导致细节很少；使用较低的重建权重而不是预热，使新试图合成的模式崩溃。</p><h4 id="其他">4 其他</h4><ol type="1"><li>利用 2D GAN 前馈反转等更成熟的编码器架构，例如Pixel2style，可能会显着提高 Pix2NeRF 的性能。</li></ol></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="category-chain-item">论文笔记</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/NeRF/">#NeRF</a> <a href="/tags/%E7%B2%BE%E5%BA%A6/">#精度</a></div></div><div class="license-box my-3"><div class="license-title"><div>Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</div><div>http://seulqxq.top/posts/9076/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>SeulQxQ</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年9月24日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/posts/52856/" title="Efficient Geometry-aware 3D Generative Adversarial Networks"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Efficient Geometry-aware 3D Generative Adversarial Networks</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/posts/51013/" title="每周总结(23.09.18-23.09.24)"><span class="hidden-mobile">每周总结(23.09.18-23.09.24)</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",(function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",(function(){var i=Object.assign({appId:"Qx9xEhNNylULQaxbl3lPUT12-gzGzoHsz",appKey:"bPPbrUlDDvOhU5HNAawchAXO",path:"window.location.pathname",placeholder:"留下你的足迹叭，如果愿意话，也可以留下昵称和邮箱哟~ ^_^",avatar:"monsterid",meta:["nick","mail","link"],requiredFields:[],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:null,emojiMaps:null,enableQQ:!0},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vcontent",(()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)}))}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="text-center py-1"><span id="timeDate">载入天数...</span> <span id="times">载入时分...</span><script>var now=new Date;function createtime(){var n=new Date("05/01/2023 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),document.getElementById("timeDate").innerHTML="🚀 for&nbsp"+dnum+"&nbspdays",document.getElementById("times").innerHTML=hnum+"&nbsphr&nbsp"+mnum+"&nbspmin&nbsp"}setInterval("createtime()",250)</script></div><div class="text-center py-1"><div><span>Copyright © 2023</span> <a href="https://erenspace.cool/" target="_blank" rel="nofollow noopener"><span>Eren‘s Spaceship</span></a><br></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.3.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://cdn.staticfile.org/anchor-js/4.2.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach((t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")}))},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><script src="/js/backgroundize.js"></script></body></html>