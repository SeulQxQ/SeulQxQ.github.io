<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="author" content="SeulQxQ"><meta name="keywords" content=""><meta name="description" content="Weekly report  Date: 23.09.18-23.09.24 Paper Title: Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation 1 Method, Contribution, Related Work 1."><meta property="og:type" content="article"><meta property="og:title" content="每周总结(23.09.18-23.09.24)"><meta property="og:url" content="http://seulqxq.top/posts/51013/index.html"><meta property="og:site_name" content="Seul"><meta property="og:description" content="Weekly report  Date: 23.09.18-23.09.24 Paper Title: Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation 1 Method, Contribution, Related Work 1."><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://seulqxq.top/img/index/11.png"><meta property="article:published_time" content="2023-09-24T08:53:48.000Z"><meta property="article:modified_time" content="2024-01-08T01:59:55.247Z"><meta property="article:author" content="SeulQxQ"><meta property="article:tag" content="总结&amp;反思"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://seulqxq.top/img/index/11.png"><title>每周总结(23.09.18-23.09.24) - Seul</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.3.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/toubudaziji.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css"><link rel="stylesheet" href="/css/cloudedGlass.css"><link rel="stylesheet" href="/css/selection.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"seulqxq.top",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:50,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><header><div class="header-inner" style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>SeulQxQ&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url('/img/2.jpg') no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.2)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="每周总结(23.09.18-23.09.24)"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-09-24 16:53" pubdate>2023年9月24日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 11k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 93 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="每周回顾" id="heading-180e4edc6a4249e78abaf014494632a1" role="tab" data-toggle="collapse" href="#collapse-180e4edc6a4249e78abaf014494632a1" aria-expanded="true">每周回顾 <span class="list-group-count">(13)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-180e4edc6a4249e78abaf014494632a1" role="tabpanel" aria-labelledby="heading-180e4edc6a4249e78abaf014494632a1"><div class="category-post-list"><a href="/posts/20422/" title="每周总结 -- 一个人并不会感到孤独，两个人才会" class="list-group-item list-group-item-action"><span class="category-post">每周总结 -- 一个人并不会感到孤独，两个人才会</span> </a><a href="/posts/6012/" title="每周总结 -- 心平气和，不急不躁" class="list-group-item list-group-item-action"><span class="category-post">每周总结 -- 心平气和，不急不躁</span> </a><a href="/posts/46941/" title="每周总结 -- 解铃还须系铃人" class="list-group-item list-group-item-action"><span class="category-post">每周总结 -- 解铃还须系铃人</span> </a><a href="/posts/57809/" title="每周总结 -- 爱意随风起，风止意难平" class="list-group-item list-group-item-action"><span class="category-post">每周总结 -- 爱意随风起，风止意难平</span> </a><a href="/posts/27646/" title="每周总结 -- 勇敢表达，无需害怕" class="list-group-item list-group-item-action"><span class="category-post">每周总结 -- 勇敢表达，无需害怕</span> </a><a href="/posts/51013/" title="每周总结(23.09.18-23.09.24)" class="list-group-item list-group-item-action active"><span class="category-post">每周总结(23.09.18-23.09.24)</span> </a><a href="/posts/38005/" title="每周总结(23.09.04-23.09.10)" class="list-group-item list-group-item-action"><span class="category-post">每周总结(23.09.04-23.09.10)</span> </a><a href="/posts/36354/" title="每周总结(23.08.28-23.09.03)" class="list-group-item list-group-item-action"><span class="category-post">每周总结(23.08.28-23.09.03)</span> </a><a href="/posts/22085/" title="每周总结(23.08.14-23.08.20)" class="list-group-item list-group-item-action"><span class="category-post">每周总结(23.08.14-23.08.20)</span> </a><a href="/posts/8519/" title="每周总结(23.08.07-23.08.13)" class="list-group-item list-group-item-action"><span class="category-post">每周总结(23.08.07-23.08.13)</span> </a><a href="/categories/%E6%AF%8F%E5%91%A8%E5%9B%9E%E9%A1%BE/" class="list-group-item list-group-item-action"><span class="category-post">More...</span></a></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">每周总结(23.09.18-23.09.24)</h1><div class="markdown-body"><div align="center"><h3>Weekly report</h3></div><p><strong>Date: 23.09.18-23.09.24</strong></p><h3 id="paper">Paper</h3><p><strong>Title:</strong> <code>Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</code></p><h4 id="method-contribution-related-work">1 Method, Contribution, Related Work</h4><p><strong>1. Method</strong></p><p>​ Based on the π-GAN model, it is a generative model for unconditional 3D perceptual image synthesis, which speaks of random latent code mapping to the radiation field of a class of objects. The authors optimize two goals at the same time: (1) π-GAN targets to take advantage of their high-fidelity 3D perception generation capabilities. (2) A well-designed reconstruction target consisting of an encoder coupled to a π-GAN generator to form an autoencoder.</p><p>An encoder that maps a given image to a latent space is introduced and optimized: (1) The π-GAN is trained and the added encoder maps the generated image back to latent space. (2) Combine the encoder with a generator of π-GAN to form a conditional GAN model, and train it using adversarial and reconstruction losses.</p><p><strong>2. Contribution</strong></p><ol type="1"><li><p>The Pix2NeRF, the first unsupervised single-view NeRF model, can learn the scene radiation field from the image, and does not require 3D information, multi-view or attitude supervision.</p></li><li><p>A GAN inversion based on NeRF is proposed.</p></li></ol><p><strong>3. Related Work</strong></p><p>​ The author's work can be categorized into specific types of 3D perception neural new view synthesis methods, which are based on NeRF and π-GAN.</p><ul><li>Neural Radiance Fields</li><li>NeRF-based GAN</li><li>Few-shot NeRF</li></ul><h4 id="model-and-modules">2 Model and Modules</h4><p><strong>1. Overall Architecture</strong></p><p>​ Pix2NeRF consists of three neural networks: a generator G, a discriminator D, which together form a Generative Adversarial Network (GAN), and an encoder E, which works in conjunction with the generator G to create an autoencoder.</p><p>​ Constraints on the generator: It generates output for view pose (d) and latent code (z).</p><p>​ G: Generator, D: Discriminator, E: Encoder, I: RGB image, z: latent code, d: pose, p: prior distribution, l: logit predict.</p><figure><img src="1.png" srcset="/img/loading.gif" lazyload alt="arc"><figcaption aria-hidden="true">arc</figcaption></figure><p><strong>1.1 Overview</strong></p><p>​ The method in the paper decouples the latent representation of the input images for the encoder and generator into content <code>z</code> and pose <code>d</code>, and processes these two parts separately.</p><p><img src="2.png?80*80" srcset="/img/loading.gif" lazyload alt="overview"> Pix2NeRF decouples pose and content from a single input image, generating a content radiance field that includes (1) pose-consistency under decoupled poses and (2) realism consistency across different poses from <span class="math display">\[p_d\]</span>. To achieve these objectives, several training objectives were designed:</p><ul><li>Generator（生成器）</li><li>Discriminator（辨别器）</li><li>GAN inversion（GAN反演）</li><li>Reconstruction（重建）</li><li>Conditional adversarial training（条件对抗训练）</li></ul><p><strong>2. GAN generator objective（GAN生成器）</strong></p><p>​ Sampling pairs of latent code <span class="math inline">\(z_{rand} \sim p_{z}\)</span> and random poses <span class="math inline">\(d_{rand} \sim p_d\)</span>, and then generating fake images using the generator: <span class="math inline">\(I_{gen} ~ = ~ G(z_{rand},~d_{rand})\)</span>, finally inputting them into the frozen discriminator: <span class="math inline">\(l_{gen}, ~ d_{gen} ~ = ~ D^*(I_{gen})\)</span> (top-left).</p><p>Using <code>MSE</code> as the loss function between the generated pose <span class="math inline">\(d_{gen}\)</span> and the random pose input <span class="math inline">\(d_{rand}\)</span>, the loss function for the generator G is as follows: <span class="math display">\[ \begin{aligned} \mathcal{L}_{\mathrm{GAN}}(G)=\underset{\substack{z_{\text {rand }} \sim p_{\mathrm{z}} \\ d_{\text {rand }} \sim p_{d}}}{\mathbb{E}} [\text{softplus}\left(-l_{\text {gen}}\right)+ \left.\lambda_{\text {pos }}\left\|d_{\text {rand }}-d_{\text {gen}}\right\|_{2}^{2}\right] \end{aligned} \]</span> where $ _{pos}$ is the fine-tuning weight factor.</p><p><strong>3. GAN discriminator objective（GAN辨别器）</strong></p><p>Sampling pairs of latent code <span class="math inline">\(z_{rand} \sim p_{z}\)</span> and random poses <span class="math inline">\(d_{rand} \sim p_d\)</span>, and then generating fake images using the frozen generator: <span class="math inline">\(I_{gen} ~ = ~ G^*(z_{rand},~d_{rand})\)</span>, and training the discriminator D using images generated by <span class="math inline">\(G^*\)</span> and real images <span class="math inline">\(I_{real} \sim p_{real}\)</span> (bottom-left). <span class="math display">\[ l_{real},~ d_{real} ~=~D(I_{real}), \\ l_{gen}, ~ d_{gen} ~=~ D(I_{gen}). \]</span> The modified discriminator's GAN loss function <span class="math inline">\(\mathcal{L}_{GAN}(D)\)</span>, considering the known pose <code>MSE</code> supervision, can be expressed as: <span class="math display">\[ \begin{aligned} \mathcal{L}_{\mathrm{GAN}}(D)~ = ~\underset{I_{\text {real}} \sim p_{\text {real}}}{\mathbb{E}} {\left[\operatorname{softplus}\left(-l_{\text {real}}\right)\right]~ + ~} \\ \underset{\substack{z_{\text {rand }} \sim p_{\mathrm{z}} \\ d_{\text {rand}} \sim p_{d}}}{\mathbb{E}} {\left[\operatorname{softplus}\left(l_{\text {gen}}\right)~ + ~\right.} \left.\lambda_{\text {pos}}\left\|d_{\text {rand}}-d_{\text {gen}}\right\|_{2}^{2}\right] \end{aligned} \]</span> Where $ _{pos}$ is the fine-tuning weight factor.</p><p><strong>4. GAN inversion objective（GAN反演）</strong></p><p>​ Jointly optimizing the encoder E and discriminator D while freezing the generator G, the goal is to ensure consistency between the sampled content and pose and the content and pose extracted by the encoder from the generated images (bottom-left). <span class="math display">\[ z_{pred}, ~ d_{pred} ~ = ~E(I_{gen}) \]</span> GAN inversion is performed using the <code>MSE</code> loss function: <span class="math display">\[ \begin{array}{r} \mathcal{L}_{\mathrm{GAN}^{-1}}(E)=\underset{\substack{z_{\text {rand }} \sim p_{\mathrm{z}} \\ d_{\text {rand }} \sim p_{d}}}{\mathbb{E}}\left[\left\|z_{\text {pred }}-z_{\text {rand }}\right\|_{2}^{2} ~+ ~\right. \left.\left\|d_{\text {pred }}-d_{\text {rand }}\right\|_{2}^{2}\right] \end{array} \]</span> <strong>5. Reconstruction objective（重建）</strong></p><p>​ By using the encoder E to extract its latent code and pose predictions, adjusting the generator G on real images, and rendering their views using the predicted poses, the goal is to promote structural consistency in the image space and make the images clearer (top-right).</p><p><span class="math display">\[ z_{pred}, ~ d_{pred} ~ = ~ E(I_{real}) \\ I_{recon} ~ = ~ G(z_{pred}, ~ d_{pred}) \]</span></p><p>The reconstruction loss function, improved based on <code>MSE</code>, is as follows:</p><p><span class="math display">\[ \begin{array}{r} \mathcal{L}_{\text {recon }}(G, E)=\underset{\begin{array}{c} {I_{real} } \sim { p_{real} } \end{array}}{\mathbb{E}}\left[\left\|I_{\text {recon }}-I_{\text {real }}\right\|_{2}^{2} ~ + ~\right. \\ \lambda_{\text {ssim }} \mathcal{L}_{\text {ssim }}\left(I_{\text {recon }}, I_{\text {real }}\right) ~ + \\ \left.\lambda_{\text {vgg }} \mathcal{L}_{\text {vgg }}\left(I_{\text {recon }}, I_{\text {real }}\right)\right] \end{array} \]</span></p><p>Where <span class="math inline">\(\mathcal{L}_{ssim}\)</span> is the SSIM loss, and <span class="math inline">\(\lambda_{ssim}\)</span> is the weight factor for the SSIM loss, while <span class="math inline">\(\mathcal{L}_{vgg}\)</span> is the VGG perceptual loss, and <span class="math inline">\(\lambda_{vgg}\)</span> is its weight factor.</p><p><strong>6. Conditional adversarial objective（条件对抗网络）</strong></p><p>​ The reconstruction objective aims to improve the quality of well-reconstructed individual views extracted by the encoder E. This may lead to a tendency for the network to predict unimportant poses or make unrealistic predictions for the reconstruction of other poses extracted from <span class="math inline">\(p_d\)</span>. To mitigate this issue, an additional adversarial objective is applied when the generator renders an image <span class="math inline">\(I_{real}\)</span> under random poses:</p><p><span class="math display">\[ l_{cond}, ~ d_{cond} ~=~D^*(G(z_{pred}, ~d_{rand})) \]</span></p><p>The loss function for this conditional adversarial objective is:</p><p><span class="math display">\[ \mathcal{L}_{cond}(G, ~E) ~=~ \underset{\substack{\substack{I_{real}\sim p_{real}} \\ d_{rand} \sim p_d}}{\mathbb{E}} \left[\operatorname{softplus}(-l_{cond}) \right] \]</span></p><p>​ This additional adversarial objective helps in ensuring that the generated images under random poses are realistic and maintain consistency with the real image distribution.</p><p><strong>7. Encoder warm-up（编码器预热）</strong></p><p>​ The reconstruction loss can easily dominate and lead to overfitting to input views, potentially causing the model to lose its ability to represent 3D structures. To address this issue, the authors introduced a simple "pre-warmup" strategy. In the first half of the training iterations, the encoder is frozen while optimizing both the reconstruction and conditional adversarial losses. Only the generator is optimized for these two objectives. This serves as a "pre-warmup" for the generator to roughly learn the correspondence between encoder outputs and encoded images. Subsequently, the encoder is unfrozen, allowing it to further refine its learned representations. After the pre-warmup phase, the encoder and generator form a pretrained autoencoder that can generate representations close to real 3D representations, avoiding the cumbersome early reconstruction objective, which can be challenging to balance with the GAN objective.</p><p>The authors demonstrated the necessity of this strategy through ablation studies and compared it to cases where only a small weight was allocated to the reconstruction loss. This strategy helps strike a balance between representation learning and reconstruction, ensuring that the model can effectively capture 3D structures while generating realistic images.</p><p><strong>8. Training and Inference（训练与推测）</strong></p><p>​ The discriminator and GAN inversion objectives are optimized at each iteration, while the GAN generator objective is optimized every other iteration. The reconstruction and conditional adversarial objectives are jointly optimized during odd-numbered iterations with a weighting factor <span class="math inline">\(λ_{recon}\)</span>:</p><p><span class="math display">\[ \mathcal{L}_{odd} ~=~ \mathcal{L}_{cond} ~ + ~ \lambda_{recon}\mathcal{L}_{recon} \]</span></p><p>​ During the inference phase, Pix2NeRF requires only one input image. This image can be fed into the encoder E, then into the generator G, followed by an arbitrary choice of pose for novel view synthesis. Additionally, sampling from the prior distribution <span class="math inline">\(p_z\)</span> can be done instead of obtaining the latent code z from the encoder. This allows the model to synthesize novel samples similar to π-GAN.</p><h4 id="experiment">3 Experiment</h4><p><strong>1. Datasets</strong></p><p>CelebA: 200,000 images of ==human faces==, using the <em>aligned</em> version and employing center cropping to capture the approximate facial area. 8,000 of these images are utilized as a test set.</p><p>CARLA: Comprising 10,000 images with 16 different ==car models==, generated using the CARLA driving simulator with random texture rendering.</p><p>ShapeNet-SRN: This dataset consists of 50 rendering views from ShapeNet, with each instance having an Archimedean spiral camera pose. The training set is filtered by the authors to include only the upper hemisphere since the validation and test sets of the ShapeNet-SRN dataset do not include the lower hemisphere. The authors utilize ==chair== segmentation for comparison with previous multi-view methods.</p><p><strong>2. Technical details</strong></p><p><code>Baseline:</code> ==π-GAN (PyTorch)==, reusing its generator and discriminator architecture.</p><p><code>Parameters:</code> The latent code prior distribution <span class="math inline">\(p_z\)</span> is chosen as a multivariate uniform distribution on [-1, 1]. The discriminator architecture is used as the encoder backbone, and a ==tanh== activation function is added at the end of the latent code head. All models are optimized using the Adam optimizer for 300,000 iterations. For the CelebA model, training is done at a resolution of 64x64 with a batch size of 48, and each ray samples 24 points. The learning rates for the discriminator, generator, and encoder are set to ==2e-4, 6e-5, and 2e-4==, respectively. For other models, training is conducted at a resolution of 32x32, with each ray sampling 96 points. The learning rates are set to ==4e-5, 4e-4, and 4e-4== for the discriminator, generator, and encoder, respectively.</p><p><strong>3. Comparison of Results</strong></p><figure><img src="3.png?80*80" srcset="/img/loading.gif" lazyload alt="result"><figcaption aria-hidden="true">result</figcaption></figure><p><strong>4. Ablation studies</strong></p><p>​ The primary model components compared are: <em>Naive GAN inversion, Auto-encoder, No GAN inversion, No conditional adversarial objective, Warm-up</em>.</p><p><code>Naive GAN inversion:</code>In the naive GAN inversion, a pre-trained GAN is used, its weights are frozen, and an encoder is trained to map images to their corresponding latent codes. The results show that the encoder can learn an approximate mapping from images to latent codes.</p><p><code>Auto-encoder:</code>The architecture of π-GAN is used as an auto-encoder, and the ==latent space is removed from the pipeline==. Only reconstruction and conditional adversarial models are trained. The results yield good quality, but noticeable 3D inconsistencies are observed.</p><p><code>No GAN inversion:</code> After ==removing GAN inversion from the pipeline==, the visual results become blurry. This step may be the connection between π-GAN training and reconstruction, affecting overall performance.</p><p><code>No conditional adversarial objective:</code> Disabling the conditional adversarial loss and retraining results in incomplete rendering, noticeable artifacts, and reduced consistency.</p><p><code>Warm-up:</code> Three models are trained separately with different warm-up strategies: ==no warm-up, no unfreezing of the encoder (always warm), and assigning lower weights for reconstruction instead of warming up==. Without a warm-up strategy, it leads to overfitting of input views and an inability to generate meaningful content in novel poses. Not unfreezing the encoder results in relatively weak distillation with few details. Using lower reconstruction weights instead of warming up causes the breakdown of patterns in newly attempted synthesis.</p><h4 id="other">4 Other</h4><ol type="1"><li>Leveraging more mature encoder architectures like Pixel2Style2Pixel in 2D GAN forward inversion may significantly improve the performance of Pix2NeRF.</li></ol></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%AF%8F%E5%91%A8%E5%9B%9E%E9%A1%BE/" class="category-chain-item">每周回顾</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%80%BB%E7%BB%93-%E5%8F%8D%E6%80%9D/">#总结&反思</a></div></div><div class="license-box my-3"><div class="license-title"><div>每周总结(23.09.18-23.09.24)</div><div>http://seulqxq.top/posts/51013/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>SeulQxQ</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年9月24日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/posts/9076/" title="Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/posts/38005/" title="每周总结(23.09.04-23.09.10)"><span class="hidden-mobile">每周总结(23.09.04-23.09.10)</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><script type="text/javascript">Fluid.utils.loadComments("#comments",(function(){var t="github-light",e="github-dark",s=document.documentElement.getAttribute("data-user-color-scheme");s="dark"===s?e:t,window.UtterancesThemeLight=t,window.UtterancesThemeDark=e;var n=document.createElement("script");n.setAttribute("src","https://utteranc.es/client.js"),n.setAttribute("repo","SeulQxQ/blog-comments"),n.setAttribute("issue-term","pathname"),n.setAttribute("label","utterances"),n.setAttribute("theme",s),n.setAttribute("crossorigin","anonymous"),document.getElementById("comments").appendChild(n)}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://hexo.fluid-dev.com/docs/guide" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.3.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://cdn.staticfile.org/anchor-js/4.2.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach((t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")}))},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><script src="/js/backgroundize.js"></script><script src="/js/backgroundize.js"></script><link defer rel="stylesheet" href="/css/backgroundize.css"></body></html>