<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="author" content="SeulQxQ"><meta name="keywords" content=""><meta name="description" content="Efficient Geometry-aware 3D Generative Adversarial Networks 一、提出的方法、贡献、相关工作 1.方法： ​ 设计了一种混合显式-隐式3D感知网络，该网络使用内存高效的三平面表示显式地存储由轻量级隐式特征解码器聚合的轴对齐平面上的特征，以实现高效的体绘制，提高了3D基础渲染的计算效率。使用了一些偏离3D基础渲染的图像空间近似，同时"><meta property="og:type" content="article"><meta property="og:title" content="Efficient Geometry-aware 3D Generative Adversarial Networks"><meta property="og:url" content="http://seulqxq.top/posts/52856/index.html"><meta property="og:site_name" content="Seul"><meta property="og:description" content="Efficient Geometry-aware 3D Generative Adversarial Networks 一、提出的方法、贡献、相关工作 1.方法： ​ 设计了一种混合显式-隐式3D感知网络，该网络使用内存高效的三平面表示显式地存储由轻量级隐式特征解码器聚合的轴对齐平面上的特征，以实现高效的体绘制，提高了3D基础渲染的计算效率。使用了一些偏离3D基础渲染的图像空间近似，同时"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://seulqxq.top/img/index/14.jpg"><meta property="article:published_time" content="2023-10-09T13:48:18.000Z"><meta property="article:modified_time" content="2023-10-17T07:31:19.920Z"><meta property="article:author" content="SeulQxQ"><meta property="article:tag" content="3D"><meta property="article:tag" content="精读"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://seulqxq.top/img/index/14.jpg"><title>Efficient Geometry-aware 3D Generative Adversarial Networks - Seul</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.3.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/toubudaziji.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css"><link rel="stylesheet" href="/css/cloudedGlass.css"><link rel="stylesheet" href="/css/selection.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"seulqxq.top",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:60,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><header><div class="header-inner" style="height:90vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>SeulQxQ&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url('/img/2.jpg') no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.2)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="Efficient Geometry-aware 3D Generative Adversarial Networks"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-10-09 21:48" pubdate>2023年10月9日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.8k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 24 分钟</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">Efficient Geometry-aware 3D Generative Adversarial Networks</h1><div class="markdown-body"><h3 id="efficient-geometry-aware-3d-generative-adversarial-networks">Efficient Geometry-aware 3D Generative Adversarial Networks</h3><h5 id="一提出的方法贡献相关工作">一、提出的方法、贡献、相关工作</h5><p><strong>1.方法：</strong></p><p>​ 设计了一种混合显式-隐式3D感知网络，该网络使用内存高效的<code>三平面表示显式地</code>存储由轻量级<code>隐式特征解码器聚合</code>的轴对齐平面上的特征，以实现高效的体绘制，提高了3D基础渲染的计算效率。使用了一些偏离3D基础渲染的图像空间近似，同时引入了一种双重判别策略，该策略保持神经渲染和最终输出之间的一致性，以规范其视图不一致的趋势。</p><blockquote><p>显式表示可以进行快速评估，但是需要很大的内存，使得这种方式难以扩展到高分辨率或复杂场景。隐式表示虽然在内存效率和场景复杂性方面有优势，但是这种方法使用大型的全连接网络进行评估，使得训练速度缓慢。因此，显式和隐式表示提供了互补的好处。</p></blockquote><figure><img src="1.png" srcset="/img/loading.gif" lazyload alt="3-plane"><figcaption aria-hidden="true">3-plane</figcaption></figure><p><strong>2.贡献：</strong></p><ul><li>引入了一个基于三平面的3D GAN框架，该框架既高效又富有表现力，以实现高分辨率几何感知图像合成。</li><li>开发了一种3D GAN训练策略，通过双重判别和生成器姿势条件促进多视图一致性，同时忠实地建模现实世界数据集中存在的姿势相关属性分布（例如表达式）。</li><li>展示了在FFHQ和AFHQ Cats数据集上无条件3D感知图像合成的最新结果，以及完全从2D野外图像中学习的高质量3D几何图形。</li></ul><p><strong>3.相关工作：</strong></p><p>​ 1）Neural scene representation and rendering(神经场景表示和渲染)</p><p>​ 设计了一种新的混合显式隐式3D感知网络，该网络使用内存高效的三平面表示显式地存储由轻量级隐式特征解码器聚合的轴对齐平面上的特征，以实现高效的体绘制</p><p>​ 2）Generative 3D-aware image synthesis(生成式3D感知图像合成)</p><p>​ 具有基于3D的先验偏差的高效3D GAN架构对于成功生成高分辨率视图一致图像和高质量3D形状至关重要。所以作者采用了以下方法：</p><p>​ a. 直接利用基于2D CNN特征生成器，即<code>StyleGAN2</code>。</p><p>​ b. 三平面表示使得该论文的方法能利用神经体渲染作为先验偏差，在计算上比完全隐式3D网络更有效。</p><p>​ c. 在神经渲染后采用基于2D CNN的向上采样，同时引入双重辨别器去避免上采样层带来的视图不一致。</p><h5 id="二模型与模块">二、模型与模块</h5><p><strong>1. Tri-plane hybrid 3D representation(Tri-plane混合3D表示)</strong></p><p><code>思想：</code>hybrid explicit-implicit tri-plane representation(混合显式-隐式三平面表示)。</p><p><code>实现：</code>沿着三个轴对齐的正交特征平面对齐显式特征，每个特征平面的分辨率均为N×N×C，N为空间维度，C为通道数。通过将3D位置投影到三个特征平面中来查询任何3D位置点<code>x</code>，通过双线性插值检索相应的特征向量<span class="math inline">\((F_{xy} ~,~ F_{xz}~ , ~ F_{yz})\)</span>，然后通过求和来汇总这三个特征向量。最后将这个汇总的特征F输入到一个小型解码器(MLP)来解码为颜色和密度。</p><p><code>模块：</code>小型MLP网络。</p><p><strong>2. 3D GAN framework(3D GAN框架)</strong></p><p><code>思想：</code>训练一个3D GAN，用于从2D照片中进行集合感知图像合成，而无需任何显式3D或者多视图监督。同时使用现成的姿态检测器，将每个训练图像与一组相机内参和外参相关联(<code>Deep3DFaceReconstruction</code>)。</p><p><code>实现/Overview:</code></p><figure><img src="2.png" srcset="/img/loading.gif" lazyload alt="overview"><figcaption aria-hidden="true">overview</figcaption></figure><blockquote><p>​ a. 一个基于姿态条件的StyleGAN2特征生成器和映射网络。</p><p>​ b. 一个具有轻量级特征解码器的三平面3D表示。</p><p>​ c. 一个神经体素渲染器。</p><p>​ d. 一个超分辨率模块。</p><p>​ e. 一个基于姿态条件的具有双重辨别的StyleGAN2辨别器。</p></blockquote><p>​ 这个架构巧妙地将特征生成和神经渲染解耦，使得可以利用强大的StyleGAN2生成器进行3D场景的泛化。此外，轻量级的三平面3D表示既能够表达丰富的信息，又能够在实时中实现高质量的3D感知视图合成。同时，采用两阶段训练策略加速训练速度。第一个阶段：使用减少<span class="math inline">\((64^2)\)</span>神经渲染分辨率进行训练；第二个阶段：在完全<span class="math inline">\((128^2)\)</span>神经渲染分辨率上的短期微调。</p><p><strong>3. CNN generator backbone and rendering(CNN生成器主干和渲染)</strong></p><p><code>思想：</code>由<code>StyleGAN2 CNN生成器生成三平面表示的特征</code>。随机潜在代码和相机参数首先由映射网络处理以产生中间潜在代码，然后调制单独合成网络的卷积核。</p><p><code>实现：</code>改变StyleGAN2主干网络的输出形状，不是生成三通道RGB图像，而是生成一个256×256×96的特征图像。从三平面采样特征，并融汇从三个平面采样的特征，输入到轻量级解码器(MLP，64个神经元的单个隐藏层，激活函数：softplus)。</p><p><code>模块：</code>StyleGAN2、MLP</p><p><strong>4. Super resolution(超分辨率)</strong></p><p><code>思想：</code>使用中等分辨率<span class="math inline">\((128^2)\)</span>进行体渲染，并依靠图像空间卷积上采样神经渲染到<span class="math inline">\((256^2 ~ or ~ 512^2)\)</span>图像大小。</p><p><code>实现：</code>由StyleGAN2调制卷积层的两个块组成。1）上采样，将128×128×3分辨率提高到512×512×3的分辨率。2）调整32通道特征图到最终的RGB图像。</p><p><strong>5. Dual discrimination(双重辨别器)</strong></p><p><code>思想：</code>使用StyleGAN2的辨别器，并进行了两次修改。</p><p><code>实现：</code>1）将特征图解释为低分辨率RGB图像。双重辨别器确保低分辨率RGB图像与高分辨率图像的一致性，通过双线性上采样成同样512×512×3图像并与调整后的<span class="math inline">\((I^+_{RGB})\)</span>进行连接变成6通道图像。2）将输入的3通道RGB图像与其适当模糊后的图像进行连接，变成6通道图像作为辨别器的输入。</p><p><code>模块：</code>StyleGAN2-ADA策略</p><blockquote><p>​ StyleGAN2-ADA策略：将渲染相机的内外矩阵(P)传递给鉴别器作为条件标签。这种调节引入了额外的信息，指导生成器学习正确的3D先验。</p></blockquote><p><strong>6. Modeling pose-correlated attributes(建模姿态相关属性)</strong></p><p><code>思想：</code>引入了<code>generator pose conditioning(生成器姿势条件)</code>作为建模和解耦训练图像中观察到的姿势与其他属性之间的相关性的一种手段。</p><p><code>实现：</code>按照StyleGAN2-ADA条件生成策略，提出一个主干映射网络，不仅提供一个潜在代码z，同时提供相机参数P作为输入。</p><h5 id="三其他细节">三、其他细节</h5><p><strong>1. Pose Estimators(姿态估计)</strong></p><p>​ 用水平翻转的方法来扩充数据集，并使用现成的姿态估计来提取近似的相机外部参数。</p><blockquote><p>现成的姿态估计方法：</p><p>1）https://github.com/Microsoft/Deep3DFaceReconstruction，用来生成脸的数据集的姿态(FFHQ)。</p><p>2）https://github.com/kairess/cat_hipsterizer，用来生成猫的数据集的姿态(AFHQv2 Cats)。</p></blockquote></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="category-chain-item">论文笔记</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/3D/">#3D</a> <a href="/tags/%E7%B2%BE%E8%AF%BB/">#精读</a></div></div><div class="license-box my-3"><div class="license-title"><div>Efficient Geometry-aware 3D Generative Adversarial Networks</div><div>http://seulqxq.top/posts/52856/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>SeulQxQ</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年10月9日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/posts/7366/" title="Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/posts/9076/" title="Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation"><span class="hidden-mobile">Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="gitalk-container"></div><script type="text/javascript">Fluid.utils.loadComments("#gitalk-container",(function(){Fluid.utils.createCssLink("/css/gitalk.css"),Fluid.utils.createScript("https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js",(function(){var e=Object.assign({clientID:"b59ee8148eaef7298208",clientSecret:"48a3e3c78109034b0baae5280e6807f780f8d949",repo:"seulqxqgitalk",owner:"SeulQxQ",admin:["SeulQxQ"],language:"zh-CN",labels:["Gitalk"],perPage:10,pagerDirection:"last",distractionFreeMode:!0,createIssueManually:!0,proxy:"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},{id:"739624ee788482c0339eeeae85016337"});new Gitalk(e).render("gitalk-container")}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://hexo.fluid-dev.com/docs/guide" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.3.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://cdn.staticfile.org/anchor-js/4.2.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach((t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")}))},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><script src="/js/backgroundize.js"></script></body></html>