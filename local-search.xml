<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Deep3DSketch+_ Rapid 3D Modeling from Single Free-hand Sketches</title>
    <link href="/posts/43769/"/>
    <url>/posts/43769/</url>
    
    <content type="html"><![CDATA[<h3id="deep3dsketch-rapid-3d-modeling-from-single-free-hand-sketches">Deep3DSketch+:Rapid 3D Modeling from Single Free-hand Sketches</h3><h4 id="引言-相关工作">1 引言 &amp; 相关工作</h4><p><strong>1. 引言</strong></p><p>​本文提出了一种名为Deep3DSketch+的新型3D建模方法，该方法可以从单个手绘草图中生成高保真度的3D模型。Deep3DSketch+采用了端到端的神经网络结构，包括==轻量级生成网络==和==结构感知的对抗训练方法==。该方法还引入了笔画增强模块==(SEM)==，以提高网络的结构特征提取能力。</p><p><strong>2. 相关工作</strong></p><p>​现有的手绘草图3D建模方法可以分为两类：端到端方法和交互式方法。交互式方法需要进行顺序步骤分解或特定的绘画手势或注释。本文提出的Deep3DSketch+方法采用了端到端的神经网络结构，不需要输入多个草图或视图信息，可以从单个手绘草图中生成高保真度的3D模型。</p><p>​基于手绘草图的建模和传统的单目3D重建有很大的区别，草图的稀疏性和抽象性以及缺乏纹理需要额外的线索来产生高质量的3D形状。<code>需要解决</code>。</p><h4 id="方法-模型">2 方法 &amp; 模型</h4><p><strong>1. Overview:</strong></p><figure><img src="image-20231015163031452.png" alt="overview" /><figcaption aria-hidden="true">overview</figcaption></figure><p><strong>2. View-aware and Structure-aware 3D Modeling</strong></p><p><code>Mesh Generation G:</code> 主干为编码器-解码器结构</p><p><code>Encoder E:</code>由于草图是稀疏且模糊的输入形式，编码器E首先将输入的草图转换为 <em>latentshape code</em> <span class="math inline">\(z_s\)</span>，这样可以在涉及语义类别和概念形状的粗略级别上概括草图。</p><p><code>Decoder D:</code>级联的上采样块组成的解码器D用于计算模板网格的顶点偏移，并通过以增加的空间分辨率逐渐推断3D形状信息来使其变形以得到具有精细细节的输出网格<span class="math inline">\(M_Θ = D{(z_s)}\)</span>。</p><p>接下来，利用可微分渲染器渲染所生成的网格 <spanclass="math inline">\(M_Θ\)</span>，来生成轮廓 <spanclass="math inline">\(S_Θ\)</span>。该网络是端到端的训练，通过近似梯度的微分渲染器的监督渲染。</p><p><strong>3. Shape discriminator and Multi-view Sampling</strong></p><p>​由于草图的稀疏性质和单视图轮廓约束的唯一监督，编码器-解码器结构化生成器G不能有效地获得高质量的3D形状。必须使用额外的线索来关注细粒度和逼真的对象结构。因此引入==形状匹配与多视点采样==。</p><p>​该辨别器为CNN网络，它在训练过程中==引入来自真实的数据集的3D形状==，以迫使网格生成器G生成逼真的形状，同时在推理过程中保持生成过程的效率。具体地说，将从预测网格生成的轮廓和从手动设计的网格渲染的轮廓输入到神经网络。同时，随机采样N个相机姿态从姿态分布p中，保证生成的网格细节合理、逼真。</p><p><strong>4. Stroke EnhancementModule</strong>(新模块，但是大概率用不上)</p><figure><img src="image-20231015171503579.png" alt="image-20231015171503579" /><figcaption aria-hidden="true">image-20231015171503579</figcaption></figure><p>​由于输入草图和投影轮廓是单一颜色的，不能有效地获得深度预测结果。因此通过引入笔划增强模块（SEM）来充分利用单色信息进行特征提取。SEM由一个位置感知注意力模块组成，该模块将广泛的上下文信息编码到局部特征中以学习特征的空间相互依赖性。<span class="math display">\[s_{i j}=\frac{\exp \left(B_{i} * C_{j}\right)}{\sum_{i=1}^{W} \exp\left(B_{i} * C_{j}\right)}\]</span> 来自轮廓 $A R^{c×n×m} $ 的局部特征被送到卷积层形成两个局部特征<span class="math inline">\(B,~C \in R^{C\times W}\)</span>，其中 <spanclass="math inline">\(W~ = ~M \times N\)</span>为像素的数量，而另一个卷积层用于形成特征图 $D R^{C N M} $。C和B的转置进行矩阵乘法，然后由 <em>softmax</em> 层生成注意力图 <spanclass="math inline">\(S \in R^{W \times W}\)</span>，从而增强了利用由轮廓表示的关键结构信息的能力。注意力图用于通过原始特征和所有位置上的特征的加权和来产生输出F:<span class="math display">\[F_{j} ~ = ~ \lambda\sum_{i=1}^{W}(s_jD_j) ~ + ~ A_j\]</span> <strong>5. Loss Function</strong></p><p>​ 损失函数来自三个组件，包括有：multi-scale <em>mIoU loss</em> <spanclass="math inline">\(\mathcal{L}_{sp}\)</span>，<em>flattenloss</em>，<em>laplacian smooth loss</em> <spanclass="math inline">\(\mathcal{L}_{r}\)</span>，<em>structure-aware GANloss</em> <span class="math inline">\(\mathcal{L}_{sd}\)</span>。 <spanclass="math display">\[\mathcal{L}_{sp} ~=~ \sum_{i=1}^{N} \lambda_{si} \mathcal{L}^i_{iou} \\\mathcal{L}_{iou}(S_1, ~ S_2) ~= ~1~ - ~\frac{\left \| S_1\otimes S_2\right \|_1 }{\left \| S_1 \otimes S_2-S_1 \otimes S_2 \right \|_1 }\]</span> S1和S2是渲染的轮廓。</p><p>​ 非饱和GAN损失： <span class="math display">\[\begin{aligned}\mathcal{L}_{s d} &amp; =\mathbf{E}_{\mathbf{z}_{\mathbf{v}} \simp_{z_{v}}, \xi \sim p_{\xi}}\left[f\left(C N N_{\theta_{D}}(R(M,\xi))\right)\right] \\&amp; +\mathbf{E}_{\mathbf{z}_{\mathbf{v r}} \sim p_{z_{v r}}, \xi \simp_{\xi}}\left[f\left(-C N N_{\theta_{D}}\left(R\left(M_{r},\xi\right)\right)\right)\right] \\&amp; \text { wheref }(u)=-\log (1+\exp (-u))\end{aligned}\]</span> ​ 总损失函数Loss计算为三个分量的加权和： <spanclass="math display">\[Loss = \mathcal{L}_{sp}+\mathcal{L}_{r}+\lambda_{sd}\mathcal{L}_{sd}\]</span></p><h4 id="实验-分析">3 实验 &amp; 分析</h4><p><strong>1. 消融实验</strong></p><p>​ 对形状鉴别器(SD)与笔划增强模块进行效用实验。</p><figure><img src="image-20231015180807656.png" alt="image-20231015180807656" /><figcaption aria-hidden="true">image-20231015180807656</figcaption></figure><blockquote><p>随机视点采样与形状识别（SD）相结合，以真实的形状作为输入，允许神经网络从多个角度“看到”真实的形状，从而能够预测合理的结构信息，这些信息甚至不存在于草图中（由于视点约束，可能无法表示）。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>3D</tag>
      
      <tag>略读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pi-GAN_ Periodic Implicit Generative Adversarial Networks for 3D-Aware</title>
    <link href="/posts/39032/"/>
    <url>/posts/39032/</url>
    
    <content type="html"><![CDATA[<h3id="pi-gan-periodic-implicit-generative-adversarial-networks-for-3d-aware">pi-GAN:Periodic Implicit Generative Adversarial Networks for 3D-Aware</h3><h4 id="提出的方法贡献相关工作">1 提出的方法，贡献，相关工作</h4><p><strong>1. 方法</strong></p><p>​所提出的方法利用==基于<em>SIREN</em>==的神经辐射场表示来鼓励多视图一致性，从而允许从较宽的范围进行渲染相机姿势范围并提供可解释的3D 结构。<em>SIREN</em>隐式场景表示利用周期性激活函数，比<em>ReLU</em>隐式表示更能表示精细细节，并使<em>π-GAN</em>能够渲染比以前的工作更清晰的图像。</p><p>​使用映射网络通过==特征线性调制<em>FiLM</em>==来调节<em>SIREN</em>中的层。这一方法可以更广泛地应用于<em>GAN</em>之外的<em>SIREN</em>架构。其次，受2D卷积<em>GAN</em>先前成功的启发，引入了==渐进式增长策略==，以加速训练并抵消3D GAN 增加的计算复杂性。</p><p><strong>2. 贡献</strong></p><ul><li>引入基于 SIREN 的隐式 GAN 作为卷积 GAN 架构的可行替代方案。</li><li>提出了一个以 FiLM调节和渐进式增长判别器作为关键组件的映射网络，以通过我们新颖的基于 SIREN的隐式 GAN 实现高质量结果。</li><li>证明视图一致性和显式相机控制是依赖于底层神经辐射场表示和经典渲染的方法的优势。</li><li>根据 CelebA 、Cats 和 CARLA 数据集上的无监督 2D 数据进行 3D感知图像合成，取得了最先进的结果。</li></ul><h4 id="方法-模型">2 方法 &amp; 模型</h4><p><strong>1. Overview: </strong></p><p><img src="image-20231013161535382.png" alt="image-20231013161535382" style="zoom:80%;" /></p><p>​ 相比于直接从输入的<em>noise, z</em>生成2D图像，π-GAN的生成器 <spanclass="math inline">\(G_{θG} (z, \xi)\)</span> 产生一个以 z为条件的隐式辐射场。该辐射场是使用体积渲染来渲染的，以根据某个相机姿势<span class="math inline">\(\xi\)</span> 生成 2D 图像。</p><p><strong>2. SIREN-Based Implicit Radiance Field</strong></p><p>​使用隐式神经辐射场(SIREN)表示3D对象，该神经辐射场被参数化为多层感知机(MLP)，输入为空间3D坐标x 和视图方向 d；输出为密度 <span class="math inline">\(\sigma\)</span>以及视图相关颜色 <span class="math inline">\((r,g,b)=c(x,d)\)</span>。然后，利用StyleGAN的映射网络，通过<em>FiLM</em>将<em>SIREN</em>调节到早上向量z 上。而映射网络采用简单的<em>ReLU MLP</em>，输入为噪声向量 z 输出为频率<span class="math inline">\(\gamma_i\)</span> 和相移 <spanclass="math inline">\(\beta_i\)</span> ，做为FiLM SIREN的输入。</p><p>​<code>优势：</code>这种插入的映射网络相比基于串联的调节更具有表现力，提高了图像质量。</p><blockquote><p>FiLM：feature-wise linear modulation(特征线性调制)</p></blockquote><p>​ 将FiLM SIREN 骨干网络形式化表示为： <span class="math display">\[\begin{aligned}\Phi(\mathbf{x})= &amp; \phi_{n-1} \circ \phi_{n-2} \circ \ldots \circ\phi_{0}(\mathbf{x}), \\&amp; \phi_{i}\left(\mathbf{x}_{i}\right)=\sin\left(\boldsymbol{\gamma}_{i} \cdot\left(\mathbf{W}_{i}\mathbf{x}_{i}+\mathbf{b}_{i}\right)+\boldsymbol{\beta}_{i}\right),\end{aligned}\]</span> 其中，<span class="math inline">\(\phi_{i}\)</span> 为MLP的第i 层，它由一个仿射变换定义，包括权重矩阵 <spanclass="math inline">\(W_i\)</span> 和偏置 <spanclass="math inline">\(b_i\)</span> 组成，同时作用于输入 <spanclass="math inline">\(x_i\)</span>。</p><p>​ 将隐式体积密度和颜色定义为： <span class="math display">\[\begin{aligned}\sigma(\mathbf{x}) &amp; =\mathbf{W}_{\sigma}\Phi(\mathbf{x})+\mathbf{b}_{\sigma}\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}\mathbf{c}(\mathbf{x}, \mathbf{d}) &amp; =\mathbf{W}_{c}\phi_{c}\left([\Phi(\mathbf{x}), \mathbf{d}]^{T}\right)+\mathbf{b}_{c}\end{aligned}\]</span></p><p>其中，<span class="math inline">\(\mathbf{W}_{\sigma/c}\)</span> 和<span class="math inline">\(b_{\sigma/c}\)</span>为额外的权重和偏差参数。</p><p>==<strong>NeRF的MLP网络：</strong>==</p><p><img src="image-20231014160022990.png" style="zoom:80%;" /></p><p>​原始的NeRF它的MLP网络为8层，都是使用的ReLU激活函数；然后一个额外特征层输出<span class="math inline">\(\sigma\)</span> ，一个特征层输出 RGB信息。而 pi-gan 提出的网络通过<em>FiLM</em> 调制，在每个层的后面把<em>SIREN</em> 加进去，得到一个新的 MLP 网络，从而提高图像质量。</p><blockquote><p>SIREN网络是指一个用于表示辐射场的神经网络。SIREN(SinusoidalRepresentationNetworks)，它是一种特殊类型的神经网络结构。这种网络使用周期性的正弦激活函数，通常被用于表示连续的函数或场景中的信号。</p></blockquote><p><strong>3. Discriminator</strong></p><p>​ 采用参数为 <span class="math inline">\(\theta_{D}\)</span>逐渐增长的卷积判别器 <span class="math inline">\(D_{\theta{D}}\)</span>，开始训练时，采用低分率和高批量大小进行训练，生成器生成粗糙的形状。随着训练的进行，提高图像分辨率并向鉴别器添加新层以处理更高分辨率并区分精细细节。</p><blockquote><p>由于计算复杂性随着图像大小呈二次方增长，因此从低分辨率开始的渐进式增长允许在训练开始时使用更大的批量大小。大批量有助于稳定训练，同时还可以提高每次迭代的图像吞吐量。渐进式增长及其支持的更大批量大小有助于确保生成图像的质量和多样性。</p><p>PS：神经渲染采用与 NeRF 相同的渲染公式。</p></blockquote><h4 id="实验-分析">3 实验 &amp; 分析</h4><p><strong>1. 训练</strong></p><p>​ 在训练时，从分布 <span class="math inline">\(p_{\xi}\)</span>中随机采样相机姿势 <span class="math inline">\(\xi\)</span>。每个数据集的姿势分布都是先验已知的，对于 CelebA 和 Cats近似为高斯分布，对于 CARLA近似为均匀分布。==将相机位置限制在单位球体的表面，并将相机指向原点==。在训练时，沿球体的俯仰和偏航是从根据数据集调整的分布中采样的。真实图像I 是从具有分布 <span class="math inline">\(p_{I}\)</span>的训练集中采样的。使用带有 R1 正则化的非饱和 GAN 损失函数： <spanclass="math display">\[\mathcal{L}(\theta , \phi ) = E_{z \sim p_{z, ~ \xi \simp_{\xi}}}[f(D_{\theta G}G_{\theta G}(z, \xi)]~ + ~ E_{I\sim p_{D}}[f(-D_{\theta D}(I)) + \lambda |\bigtriangledownD_{\theta  D}(I)|^2], \\where \quad f(u) = -log(1+exp(-u))\]</span> 生成器尝试最小化等式，而判别器同时尝试最大化等式。使用 Adam优化器，β1 = 0，β2 = 0.9。</p><p><strong>2. 结果对比</strong></p><p><code>Baseline:</code> HoloGAN, Generative Radiance Fields(GRAF)</p><p><img src="image-20231015082031213.png" alt="image-20231015082031213" style="zoom:80%;" /></p><p><code>评价指标:</code> Frechet Inception Distance(FID), KernelIncerption Distance(KID), Inception Score</p><p><strong>3. 消融实验</strong></p><p>​对==正弦激活==和==映射网络调节==进行消融，来体现它们在网络中的作用。将==正弦激活==的辐射场与==ReLU激活==和位置编码的辐射场进行比较。将通过==映射网络和FiLM条件调节==的辐射场与通过==级联调节==的辐射场进行比较。</p><figure><img src="/image-20231015102018486.png" alt="image-20231015102018486" /><figcaption aria-hidden="true">image-20231015102018486</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>3D</tag>
      
      <tag>精度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion</title>
    <link href="/posts/7366/"/>
    <url>/posts/7366/</url>
    
    <content type="html"><![CDATA[<h3id="shape-pose-and-appearance-from-a-single-image-via-bootstrapped-radiance-field-inversion">Shape,Pose, and Appearance from a Single Image via Bootstrapped Radiance FieldInversion</h3><h4 id="一提出的方法与贡献">一、提出的方法与贡献</h4><p><strong>1.方法：</strong></p><p>​作者提出了一种新的方法，将无条件生成模型与混合反演范式相结合，从单个图像中恢复三维信息。具体来说，他们使用神经辐射场（NeRF）来表示三维场景，并使用编码器产生潜在表示和姿态的第一个猜测。然后，他们通过优化来细化这些初始估计，以获得更准确的重建。</p><p><strong>2.贡献：</strong></p><ul><li>引入了一个基于NeRF的端到端单视图三维重建管道。在这种情况下，我们成功地展示了CMR基准下自然图像的<spanclass="math inline">\(360^◦\)</span>对象重建。</li><li>提出了一种用于NeRF的混合反演方案，以加快预训练的3D感知生成器的反转。</li><li>受姿态估计文献的启发，我们提出了一种基于PnP的姿态估计器，它利用我们的框架并且不需要额外的数据假设。</li></ul><h4 id="二模型与模块">二、模型与模块</h4><p><strong>1. Unconditional generatorpre-training（无条件生成器预训练框架）</strong></p><figure><img src="image-20230904201413230.png" alt="image-20230904201413230" /><figcaption aria-hidden="true">image-20230904201413230</figcaption></figure><p><code>思想：</code>主要思想来自EG3D的主干网络，三平面编码。<code>该部分被框架使用基于NeRF的生成器G与2D图像鉴别器相结合。</code></p><p><code>模块：</code>StyleGAN2，SDF representation，Attention-basedcolor mapping，Path Length Regularization revisited。</p><blockquote><p>StyleGAN2：生成模型，SDF representation：3D表示，</p><p><strong>Attention-based color mapping：提高颜色泛化性, Path LengthRegularizationrevisited：使三平面解码器不正则化，提高学习率。</strong></p></blockquote><p><strong>2. Bootstrapping and poseestimation（自举和姿态估计）</strong></p><figure><img src="image-20230904201324234.png" alt="image-20230904201324234" /><figcaption aria-hidden="true">image-20230904201324234</figcaption></figure><p><code>思想：</code>主要思想来自NOCS，<code>改进：是使用从无条件生成器生成的数据来训练编码器而不是手工数据。</code></p><p><code>实现：</code>1）冻结G并训练图像编码器E，联合估计对象的姿势及其潜在代码（自举）的初始猜测。2）对于姿态估计，我们采用了一种原则性的方法来预测屏幕空间中的规范映射通过透视n点(PnP)算法。<code>输入真实图像，将预测的规范映射转换为点云，并运行PnP求解器来恢复所有姿态参数(视图矩阵和焦距)。</code></p><p><code>模块：</code>SegFormer</p><blockquote><p><strong>训练SegFormer网络来从RGB图像中预测规范图和latent codew</strong></p></blockquote><p><strong>SegFormer分割网络图：</strong></p><figure><img src="image-20230904201508184.png" alt="image-20230904201508184" /><figcaption aria-hidden="true">image-20230904201508184</figcaption></figure><p><strong>3. Reconstruction via hybrid GANinversion（通过混合GAN反演重建）</strong></p><figure><img src="image-20230904202404525.png" alt="image-20230904202404525" /><figcaption aria-hidden="true">image-20230904202404525</figcaption></figure><p>通过基于梯度的优化(混合反演)改进了几个步骤的姿态和潜在代码。损失函数：VGG</p><p><code>模块：</code>adaptive discriminator augmentation(ADA)</p><blockquote><p>有助于减少梯度的方差，使我们能够进一步提高学习率。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NeRF</tag>
      
      <tag>精读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Efficient Geometry-aware 3D Generative Adversarial Networks</title>
    <link href="/posts/52856/"/>
    <url>/posts/52856/</url>
    
    <content type="html"><![CDATA[<h3id="efficient-geometry-aware-3d-generative-adversarial-networks">EfficientGeometry-aware 3D Generative Adversarial Networks</h3><h5 id="一提出的方法贡献相关工作">一、提出的方法、贡献、相关工作</h5><p><strong>1.方法：</strong></p><p>​设计了一种混合显式-隐式3D感知网络，该网络使用内存高效的<code>三平面表示显式地</code>存储由轻量级<code>隐式特征解码器聚合</code>的轴对齐平面上的特征，以实现高效的体绘制，提高了3D基础渲染的计算效率。使用了一些偏离3D基础渲染的图像空间近似，同时引入了一种双重判别策略，该策略保持神经渲染和最终输出之间的一致性，以规范其视图不一致的趋势。</p><blockquote><p>显式表示可以进行快速评估，但是需要很大的内存，使得这种方式难以扩展到高分辨率或复杂场景。隐式表示虽然在内存效率和场景复杂性方面有优势，但是这种方法使用大型的全连接网络进行评估，使得训练速度缓慢。因此，显式和隐式表示提供了互补的好处。</p></blockquote><figure><img src="1.png" alt="3-plane" /><figcaption aria-hidden="true">3-plane</figcaption></figure><p><strong>2.贡献：</strong></p><ul><li>引入了一个基于三平面的3DGAN框架，该框架既高效又富有表现力，以实现高分辨率几何感知图像合成。</li><li>开发了一种3DGAN训练策略，通过双重判别和生成器姿势条件促进多视图一致性，同时忠实地建模现实世界数据集中存在的姿势相关属性分布（例如表达式）。</li><li>展示了在FFHQ和AFHQCats数据集上无条件3D感知图像合成的最新结果，以及完全从2D野外图像中学习的高质量3D几何图形。</li></ul><p><strong>3.相关工作：</strong></p><p>​ 1）Neural scene representation and rendering(神经场景表示和渲染)</p><p>​设计了一种新的混合显式隐式3D感知网络，该网络使用内存高效的三平面表示显式地存储由轻量级隐式特征解码器聚合的轴对齐平面上的特征，以实现高效的体绘制</p><p>​ 2）Generative 3D-aware image synthesis(生成式3D感知图像合成)</p><p>​ 具有基于3D的先验偏差的高效3DGAN架构对于成功生成高分辨率视图一致图像和高质量3D形状至关重要。所以作者采用了以下方法：</p><p>​ a. 直接利用基于2D CNN特征生成器，即<code>StyleGAN2</code>。</p><p>​ b.三平面表示使得该论文的方法能利用神经体渲染作为先验偏差，在计算上比完全隐式3D网络更有效。</p><p>​ c. 在神经渲染后采用基于2DCNN的向上采样，同时引入双重辨别器去避免上采样层带来的视图不一致。</p><h5 id="二模型与模块">二、模型与模块</h5><p><strong>1. Tri-plane hybrid 3Drepresentation(Tri-plane混合3D表示)</strong></p><p><code>思想：</code>hybrid explicit-implicit tri-planerepresentation(混合显式-隐式三平面表示)。</p><p><code>实现：</code>沿着三个轴对齐的正交特征平面对齐显式特征，每个特征平面的分辨率均为N×N×C，N为空间维度，C为通道数。通过将3D位置投影到三个特征平面中来查询任何3D位置点<code>x</code>，通过双线性插值检索相应的特征向量<spanclass="math inline">\((F_{xy} ~,~ F_{xz}~ , ~F_{yz})\)</span>，然后通过求和来汇总这三个特征向量。最后将这个汇总的特征F输入到一个小型解码器(MLP)来解码为颜色和密度。</p><p><code>模块：</code>小型MLP网络。</p><p><strong>2. 3D GAN framework(3D GAN框架)</strong></p><p><code>思想：</code>训练一个3DGAN，用于从2D照片中进行集合感知图像合成，而无需任何显式3D或者多视图监督。同时使用现成的姿态检测器，将每个训练图像与一组相机内参和外参相关联(<code>Deep3DFaceReconstruction</code>)。</p><p><code>实现/Overview:</code></p><figure><img src="2.png" alt="overview" /><figcaption aria-hidden="true">overview</figcaption></figure><blockquote><p>​ a. 一个基于姿态条件的StyleGAN2特征生成器和映射网络。</p><p>​ b. 一个具有轻量级特征解码器的三平面3D表示。</p><p>​ c. 一个神经体素渲染器。</p><p>​ d. 一个超分辨率模块。</p><p>​ e. 一个基于姿态条件的具有双重辨别的StyleGAN2辨别器。</p></blockquote><p>​这个架构巧妙地将特征生成和神经渲染解耦，使得可以利用强大的StyleGAN2生成器进行3D场景的泛化。此外，轻量级的三平面3D表示既能够表达丰富的信息，又能够在实时中实现高质量的3D感知视图合成。同时，采用两阶段训练策略加速训练速度。第一个阶段：使用减少<spanclass="math inline">\((64^2)\)</span>神经渲染分辨率进行训练；第二个阶段：在完全<spanclass="math inline">\((128^2)\)</span>神经渲染分辨率上的短期微调。</p><p><strong>3. CNN generator backbone andrendering(CNN生成器主干和渲染)</strong></p><p><code>思想：</code>由<code>StyleGAN2 CNN生成器生成三平面表示的特征</code>。随机潜在代码和相机参数首先由映射网络处理以产生中间潜在代码，然后调制单独合成网络的卷积核。</p><p><code>实现：</code>改变StyleGAN2主干网络的输出形状，不是生成三通道RGB图像，而是生成一个256×256×96的特征图像。从三平面采样特征，并融汇从三个平面采样的特征，输入到轻量级解码器(MLP，64个神经元的单个隐藏层，激活函数：softplus)。</p><p><code>模块：</code>StyleGAN2、MLP</p><p><strong>4. Super resolution(超分辨率)</strong></p><p><code>思想：</code>使用中等分辨率<spanclass="math inline">\((128^2)\)</span>进行体渲染，并依靠图像空间卷积上采样神经渲染到<spanclass="math inline">\((256^2 ~ or ~ 512^2)\)</span>图像大小。</p><p><code>实现：</code>由StyleGAN2调制卷积层的两个块组成。1）上采样，将128×128×3分辨率提高到512×512×3的分辨率。2）调整32通道特征图到最终的RGB图像。</p><p><strong>5. Dual discrimination(双重辨别器)</strong></p><p><code>思想：</code>使用StyleGAN2的辨别器，并进行了两次修改。</p><p><code>实现：</code>1）将特征图解释为低分辨率RGB图像。双重辨别器确保低分辨率RGB图像与高分辨率图像的一致性，通过双线性上采样成同样512×512×3图像并与调整后的<spanclass="math inline">\((I^+_{RGB})\)</span>进行连接变成6通道图像。2）将输入的3通道RGB图像与其适当模糊后的图像进行连接，变成6通道图像作为辨别器的输入。</p><p><code>模块：</code>StyleGAN2-ADA策略</p><blockquote><p>​StyleGAN2-ADA策略：将渲染相机的内外矩阵(P)传递给鉴别器作为条件标签。这种调节引入了额外的信息，指导生成器学习正确的3D先验。</p></blockquote><p><strong>6. Modeling pose-correlatedattributes(建模姿态相关属性)</strong></p><p><code>思想：</code>引入了<code>generator pose conditioning(生成器姿势条件)</code>作为建模和解耦训练图像中观察到的姿势与其他属性之间的相关性的一种手段。</p><p><code>实现：</code>按照StyleGAN2-ADA条件生成策略，提出一个主干映射网络，不仅提供一个潜在代码z，同时提供相机参数P作为输入。</p><h5 id="三其他细节">三、其他细节</h5><p><strong>1. Pose Estimators(姿态估计)</strong></p><p>​用水平翻转的方法来扩充数据集，并使用现成的姿态估计来提取近似的相机外部参数。</p><blockquote><p>现成的姿态估计方法：</p><p>1）https://github.com/Microsoft/Deep3DFaceReconstruction，用来生成脸的数据集的姿态(FFHQ)。</p><p>2）https://github.com/kairess/cat_hipsterizer，用来生成猫的数据集的姿态(AFHQv2Cats)。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>3D</tag>
      
      <tag>精读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pix2NeRF_ Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</title>
    <link href="/posts/9076/"/>
    <url>/posts/9076/</url>
    
    <content type="html"><![CDATA[<h3id="pix2nerf-unsupervised-conditional-π-gan-for-single-image-to-neural-radiance-fields-translation">Pix2NeRF:Unsupervised Conditional π-GAN for Single Image to Neural RadianceFields Translation</h3><h4 id="提出的方法-贡献相关工作">1 提出的方法， 贡献，相关工作</h4><p><strong>1. 方法</strong></p><p>基于π-GAN模型，用于无条件3D感知图像合成的生成模型，它讲随机latentcode映射到一类对象的辐射场。作者同时优化两个目标:（1）π-GAN目标，以利用其高保真度的3D感知生成能力。（2）一个经过精心设计的重建目标，包括一个与π-GAN生成器耦合的编码器，形成一个自动编码器。</p><p>引入了将给定图像映射到latentspace的编码器，并对其做了些优化：（1）训练π-GAN和添加的编码器将生成的图像映射回latentspace。（2）将编码器与π-GAN的生成器结合形成带条件的GAN模型，同时使用对抗和重建损失对其进行训练。</p><p><strong>2. 贡献</strong></p><p>（1）提出Pix2NeRF，第一个无监督的单视图NeRF模型，可以从图像中学习场景辐射场，并且不需要3D信息，多视图或者姿态监督。</p><p>（2）提出基于NeRF的GAN反演。</p><p><strong>3. 相关工作</strong></p><p>作者的工作可以分类为特定类别的3D感知神经新视图合成方法，该方法基于NeRF和π-GAN。</p><ul><li>神经辐射场</li><li>基于NeRF的GAN</li><li>少样本NeRF</li></ul><h4 id="模型与模块">2 模型与模块</h4><p><strong>1. 总体架构</strong></p><p>​Pix2NeRF由三个神经网络组成，一个生成器G、一个辨别器D，共同组成生成对抗网络，一个编码器E，与生成器G共同组成一个自动编码器。</p><p>​ 生成器约束：output view pose d and latent code z。</p><p>​ G：生成器，D：辨别器，E：编码器，I：RGB image，z：latentcode，d：pose，p：prior distribution l：logit predict distribution。</p><figure><img src="1.png" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><p><strong>1.1 Overview</strong></p><p>​论文中的方法将编码器和生成器输入映射的图像的潜在表示解耦为内容<code>z</code>和姿态<code>d</code>，并对这两个部分进行单独处理。</p><p><img src="2.png" alt="1" style="zoom:80%;" /></p><p>​Pix2NeRF从一张输入的图像中解耦姿态和内容，并且生成一个内容的辐射场，该辐射场包括（1）解耦姿态下的输入一致性，（2）来自<spanclass="math inline">\(~p_d~\)</span>不同姿态下的一致性雨逼真性。为了完成这些目标，设计了几个训练目标：</p><ul><li>Generator（生成器）</li><li>Discriminator（辨别器）</li><li>GAN inversion（GAN反演）</li><li>Reconstruction（重建）</li><li>Conditional adversarial training（条件对抗训练）</li></ul><p><strong>2. GAN generator objective（GAN生成器）</strong></p><p>​ 对latent code <span class="math inline">\(z_{rand} \simp_{z}\)</span> 和随机姿态<span class="math inline">\(d_{rand} \simp_d\)</span> 成对采样，然后通过生成器去生成假的生成图像：<spanclass="math inline">\(I_{gen} ~ = ~G(z_{rand},~d_{rand})\)</span>，最后输入到冻结的辨别器中：<spanclass="math inline">\(l_{gen}, ~ d_{gen} ~ = ~D^*(I_{gen})\)</span>。（左上）</p><p>​ 使用 <code>MSE</code> 作为生成的姿态 <spanclass="math inline">\(d_{gen}\)</span> 和随机姿态输入 <spanclass="math inline">\(d_{rand}\)</span>之间的损失函数，同时生成器G的损失函数如下： <spanclass="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{GAN}}(G)=\underset{\substack{z_{\text {rand }} \simp_{\mathrm{z}} \\d_{\text {rand }} \simp_{d}}}{\mathbb{E}}  [\text{softplus}\left(-l_{\text {gen}}\right)+\left.\lambda_{\text {pos }}\left\|d_{\text {rand }}-d_{\text{gen}}\right\|_{2}^{2}\right]\end{aligned}\]</span> 其中 $ _{pos}$ 为微调权重因子。</p><p><strong>3. GAN discriminator objective（GAN辨别器）</strong></p><p>​ 对latent code <span class="math inline">\(z_{rand} \simp_{z}\)</span> 和随机姿态<span class="math inline">\(d_{rand} \simp_d\)</span> 成对采样，然后通过冻结的生成器去生成假的生成图像：<spanclass="math inline">\(I_{gen} ~ = ~G^*(z_{rand},~d_{rand})\)</span>，然后辨别器D通过 <spanclass="math inline">\(G^*\)</span> 生成的的图像和真实图像 <spanclass="math inline">\(I_{real} \sim p_{real}\)</span>来进行训练。（左下） <span class="math display">\[l_{real},~ d_{real} ~=~D(I_{real}), \\l_{gen}, ~ d_{gen} ~=~ D(I_{gen}).\]</span> ​ 考虑已知姿态的 <code>MSE</code> 监督而修改的鉴别器的损失函数<span class="math inline">\(\mathcal{L}_{GAN}(D)\)</span> 可以表示为：<span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{GAN}}(D)~ = ~\underset{I_{\text {real}} \simp_{\text {real}}}{\mathbb{E}}{\left[\operatorname{softplus}\left(-l_{\text {real}}\right)\right]~ +~} \\\underset{\substack{z_{\text {rand }} \sim p_{\mathrm{z}} \\d_{\text {rand}} \sim p_{d}}}{\mathbb{E}}{\left[\operatorname{softplus}\left(l_{\text {gen}}\right)~ + ~\right.}\left.\lambda_{\text {pos}}\left\|d_{\text {rand}}-d_{\text{gen}}\right\|_{2}^{2}\right]\end{aligned}\]</span> 其中 $ _{pos}$ 为微调权重因子。</p><p><strong>4. GAN inversion objective（GAN反演）</strong></p><p>​ 联合优化编码器 E 和辨别器 D，并冻结生成器G。目的是确保采样的内容和姿态与编码器从生成的图像中提取的内容和姿态之间的一致性。（左下）<span class="math display">\[z_{pred}, ~ d_{pred} ~ = ~E(I_{gen})\]</span> GAN反演使用 <code>MSE</code> 损失函数： <spanclass="math display">\[\begin{array}{r}\mathcal{L}_{\mathrm{GAN}^{-1}}(E)=\underset{\substack{z_{\text {rand }}\sim p_{\mathrm{z}} \\d_{\text {rand }} \sim p_{d}}}{\mathbb{E}}\left[\left\|z_{\text {pred}}-z_{\text {rand }}\right\|_{2}^{2} ~+ ~\right.\left.\left\|d_{\text {pred }}-d_{\text {rand }}\right\|_{2}^{2}\right]\end{array}\]</span> <strong>5. Reconstruction objective（重建）</strong></p><p>​ 通过使用编码器 E 提取其 latent code和姿势预测来在真实图像上调节生成器G，然后使用预测的姿势渲染其视图，<code>目的为促进图像空间中结构的的一致性，并使得图像更加清晰</code>。（右上）<span class="math display">\[z_{pred}, ~ d_{pred} ~ = ~ E(I_{real}) \\I_{recon} ~ = ~ G(z_{pred}, ~ d_{pred})\]</span> 重建损失函数，基于 <code>MSE</code> 改进： <spanclass="math display">\[\begin{array}{r}\mathcal{L}_{\text {recon }}(G, E)=\underset{\begin{array}{c}{I_{real} } \sim { p_{real} }\end{array}}{\mathbb{E}}\left[\left\|I_{\text {recon }}-I_{\text {real}}\right\|_{2}^{2} ~ + ~\right. \\\lambda_{\text {ssim }} \mathcal{L}_{\text {ssim }}\left(I_{\text {recon}}, I_{\text {real }}\right) ~ + \\\left.\lambda_{\text {vgg }} \mathcal{L}_{\text {vgg }}\left(I_{\text{recon }}, I_{\text {real }}\right)\right]\end{array}\]</span> 其中 <span class="math inline">\(\mathcal{L}_{ssim}\)</span>为SSIM损失，<span class="math inline">\(\lambda_{ssim}\)</span>为SSIM损失的权重因子；<spanclass="math inline">\(\mathcal{L}_{vgg}\)</span> 为 VGG 感知损失，<spanclass="math inline">\(\lambda_{vgg}\)</span> 为其权重因子。</p><p><strong>6. Conditional adversarialobjective（条件对抗网络）</strong></p><p>​重建目标旨在提高由编码器E提取的单个视图的良好重建质量。这可能会导致网络组合趋向于预测微不重要的姿势，或者对从<span class="math inline">\(p_d\)</span>提取的其他姿势的重建进行不切实际的预测。为了缓解这一问题，当生成器在随机姿势下渲染出图像<span class="math inline">\(I_{real}\)</span> 时，进一步应用对抗目标。<span class="math display">\[l_{cond}, ~ d_{cond} ~=~D^*(G(z_{pred}, ~d_{rand}))\]</span> 损失函数为： <span class="math display">\[\mathcal{L}_{cond}(G, ~E) ~=~\underset{\substack{\substack{I_{real}\sim p_{real}} \\d_{rand} \sim p_d}}{\mathbb{E}}\left[\operatorname{softplus}(-l_{cond}) \right]\]</span> <strong>7. Encoder warm-up（编码器预热）</strong></p><p>​重建损失可能很容易占主导地位，导致模型过度拟合于输入视图，同时失去了表示3D的能力。因此，作者引入了一种简单的“预热”策略来应对这个问题。在训练协议的前半部分迭代中，冻结编码器，同时优化重建和条件对抗损失，并且仅优化生成器用于这两个目标。这作为生成器的预热，大致学习编码器输出与编码图像之间的对应关系。然后解冻编码器，使其能够进一步提炼其学习到的表示。在预热阶段之后，编码器和生成器直接形成了一个经过预训练的自动编码器，能够生成接近真实3D表示的结果，避免了繁琐的早期重建目标，这在与GAN目标相平衡非常困难的情况下尤为重要。作者通过消融研究展示了这一策略的必要性，并与仅为重建损失分配较小权重的情况进行了比较。</p><p><strong>8. Training and Inference（训练与推测）</strong></p><p>​ 判别器和 GAN 反演目标在每次迭代时都会进行优化；GAN生成器目标在偶次迭代时进行优化；重建和条件对抗目标在奇数迭代期间通过加权因子<span class="math inline">\(λrecon\)</span> 联合优化： <spanclass="math display">\[\mathcal{L}_{odd} ~=~ \mathcal{L}_{cond} ~ + ~\lambda_{recon}\mathcal{L}_{recon}\]</span> ​ 在推理阶段，Pix2NeRF 只需要一个输入图像，可以将其输入编码器E，然后输入生成器G，再加上任意选择的姿势以进行新颖的视图合成。同时，可以从先验分布 <spanclass="math inline">\(p_z\)</span> 中对其进行采样，而不是从编码器中获取latent code z，以使模型像 π-GAN 一样合成新颖的样本。</p><h4 id="实验">3 实验</h4><p><strong>1.数据集</strong></p><p>CelebA：200k张==人脸照片==，使用==aligned==的版本，并且采用中心裁剪，裁剪出大致脸部面积，使用8k张照片作为测试集。</p><p>CARLA：包含 16 个==汽车模型==的 10k 图像，使用 Carla驾驶模拟器以随机纹理渲染。</p><p>ShapeNet-SRN：该数据集包含来自 ShapeNet 的 50个渲染视图，其中每个实例都有阿基米德螺旋相机姿势。由于 ShapeNet-SRN数据集的验证和测试集中不包括下半球，因此作者过滤训练集以仅包含上半球。作者使用==椅子==分割来与之前的多视图方法进行比较。</p><p><strong>2.技术细节</strong></p><p><code>baseline：</code>==π-GAN（Pytorch）==,重用其生成器和鉴别器架构。</p><p><code>参数：</code>选择 latent code 先验分布 <spanclass="math inline">\(p_z\)</span> 作为 [-1, 1]上的多元均匀分布。使用鉴别器架构作为编码器主干，并且在latent code头部的末尾添加==tanh==，所有的模型使用Adam优化器进行300k迭代。CelebA模型在==resolution=64×64，batchsize=48==，每条射线采样24个点的参数下进行训练，其辨别器、生成器、编码器的学习率分别为==2e-4,6e-5,2e-4==。对于其他模型在==resolution=32×32==，每条射线采样96个点的参数下进行训练，学习率分别为==4e-5,4e-4,4e-4==.</p><p><strong>3.结果对比</strong></p><p><img src="3.png" alt="3" style="zoom:80%;" /></p><p><strong>4.消融实验</strong></p><p>通过逐个去除关键组件并在完整模型相同的设置下训练模型，来验证设计选择。</p><p>主要对比的模型组件有：==Naive GAN inversion, Auto-encoder, No GANinversion, No conditional adversarial objective, Warm-up==</p><p><code>Naive GAN inversion:</code>在朴素的GAN反演中，使用一个预先训练好的GAN，冻结其权重，并训练一个编码器将图像映射到它们对应的latent code。结果表明，编码器可以学习从图像到 latent code的近似映射。</p><p><code>Auto-encoder:</code>利用 π-GAN的架构作为自动编码器，并将==latentspace==从pipeline中删除，只训练重建和条件对抗模型。得到的结果虽然又不错的质量，但是明显能观察到3D不一致现象。</p><p><code>No GAN inversion:</code>将==GANinversion==从pipeline中删除之后，视觉结果变得模糊。这一步可能是 π-GAN训练和重建之间的联系，影响整体性能。</p><p><code>No conditional adversarial objective:</code>停用条件对抗损失并重新训练，导致渲染变得不完整，有明显的伪影，并且一致性降低。</p><p><code>Warm-up:</code>分别训练三个模型，==没有warm-up，没有unfreezing编码器（始终预热），为重建分配较低的权重而不是预热==。没有warm-up策略，导致过拟合输入视图，并且无法在新颖姿态中产生有意义的内容；没有解冻编码器，则蒸馏相对弱，导致细节很少；使用较低的重建权重而不是预热，使新试图合成的模式崩溃。</p><h4 id="其他">4 其他</h4><ol type="1"><li>利用 2D GAN前馈反转等更成熟的编码器架构，例如Pixel2style，可能会显着提高 Pix2NeRF的性能。</li></ol>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NeRF</tag>
      
      <tag>精度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>每周总结(23.09.18-23.09.24)</title>
    <link href="/posts/51013/"/>
    <url>/posts/51013/</url>
    
    <content type="html"><![CDATA[<div align='center'><H3>Weekly report</div><p><strong>Date: 23.09.18-23.09.24</strong></p><h3 id="paper">Paper</h3><p><strong>Title:</strong><code>Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</code></p><h4 id="method-contribution-related-work">1 Method, Contribution,Related Work</h4><p><strong>1. Method</strong></p><p>​ Based on the π-GAN model, it is a generative model for unconditional3D perceptual image synthesis, which speaks of random latent codemapping to the radiation field of a class of objects. The authorsoptimize two goals at the same time: (1) π-GAN targets to take advantageof their high-fidelity 3D perception generation capabilities. (2) Awell-designed reconstruction target consisting of an encoder coupled toa π-GAN generator to form an autoencoder.</p><p>An encoder that maps a given image to a latent space is introducedand optimized: (1) The π-GAN is trained and the added encoder maps thegenerated image back to latent space. (2) Combine the encoder with agenerator of π-GAN to form a conditional GAN model, and train it usingadversarial and reconstruction losses.</p><p><strong>2. Contribution</strong></p><ol type="1"><li><p>The Pix2NeRF, the first unsupervised single-view NeRF model, canlearn the scene radiation field from the image, and does not require 3Dinformation, multi-view or attitude supervision.</p></li><li><p>A GAN inversion based on NeRF is proposed.</p></li></ol><p><strong>3. Related Work</strong></p><p>​ The author's work can be categorized into specific types of 3Dperception neural new view synthesis methods, which are based on NeRFand π-GAN.</p><ul><li>Neural Radiance Fields</li><li>NeRF-based GAN</li><li>Few-shot NeRF</li></ul><h4 id="model-and-modules">2 Model and Modules</h4><p><strong>1. Overall Architecture</strong></p><p>​ Pix2NeRF consists of three neural networks: a generator G, adiscriminator D, which together form a Generative Adversarial Network(GAN), and an encoder E, which works in conjunction with the generator Gto create an autoencoder.</p><p>​ Constraints on the generator: It generates output for view pose (d)and latent code (z).</p><p>​ G: Generator, D: Discriminator, E: Encoder, I: RGB image, z: latentcode, d: pose, p: prior distribution, l: logit predict.</p><figure><img src="1.png" alt="arc" /><figcaption aria-hidden="true">arc</figcaption></figure><p><strong>1.1 Overview</strong></p><p>​ The method in the paper decouples the latent representation of theinput images for the encoder and generator into content <code>z</code>and pose <code>d</code>, and processes these two parts separately.</p><p><img src="2.png?80*80" alt="overview" /> Pix2NeRF decouples pose andcontent from a single input image, generating a content radiance fieldthat includes (1) pose-consistency under decoupled poses and (2) realismconsistency across different poses from <spanclass="math display">\[p_d\]</span>. To achieve these objectives,several training objectives were designed:</p><ul><li>Generator（生成器）</li><li>Discriminator（辨别器）</li><li>GAN inversion（GAN反演）</li><li>Reconstruction（重建）</li><li>Conditional adversarial training（条件对抗训练）</li></ul><p><strong>2. GAN generator objective（GAN生成器）</strong></p><p>​ Sampling pairs of latent code <span class="math inline">\(z_{rand}\sim p_{z}\)</span> and random poses <spanclass="math inline">\(d_{rand} \sim p_d\)</span>, and then generatingfake images using the generator: <span class="math inline">\(I_{gen} ~ =~ G(z_{rand},~d_{rand})\)</span>, finally inputting them into the frozendiscriminator: <span class="math inline">\(l_{gen}, ~ d_{gen} ~ = ~D^*(I_{gen})\)</span> (top-left).</p><p>Using <code>MSE</code> as the loss function between the generatedpose <span class="math inline">\(d_{gen}\)</span> and the random poseinput <span class="math inline">\(d_{rand}\)</span>, the loss functionfor the generator G is as follows: <span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{GAN}}(G)=\underset{\substack{z_{\text {rand }} \simp_{\mathrm{z}} \\d_{\text {rand }} \simp_{d}}}{\mathbb{E}}  [\text{softplus}\left(-l_{\text {gen}}\right)+\left.\lambda_{\text {pos }}\left\|d_{\text {rand }}-d_{\text{gen}}\right\|_{2}^{2}\right]\end{aligned}\]</span> where $ _{pos}$ is the fine-tuning weight factor.</p><p><strong>3. GAN discriminator objective（GAN辨别器）</strong></p><p>Sampling pairs of latent code <span class="math inline">\(z_{rand}\sim p_{z}\)</span> and random poses <spanclass="math inline">\(d_{rand} \sim p_d\)</span>, and then generatingfake images using the frozen generator: <spanclass="math inline">\(I_{gen} ~ = ~ G^*(z_{rand},~d_{rand})\)</span>,and training the discriminator D using images generated by <spanclass="math inline">\(G^*\)</span> and real images <spanclass="math inline">\(I_{real} \sim p_{real}\)</span> (bottom-left).<span class="math display">\[l_{real},~ d_{real} ~=~D(I_{real}), \\l_{gen}, ~ d_{gen} ~=~ D(I_{gen}).\]</span> The modified discriminator's GAN loss function <spanclass="math inline">\(\mathcal{L}_{GAN}(D)\)</span>, considering theknown pose <code>MSE</code> supervision, can be expressed as: <spanclass="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{GAN}}(D)~ = ~\underset{I_{\text {real}} \simp_{\text {real}}}{\mathbb{E}}{\left[\operatorname{softplus}\left(-l_{\text {real}}\right)\right]~ +~} \\\underset{\substack{z_{\text {rand }} \sim p_{\mathrm{z}} \\d_{\text {rand}} \sim p_{d}}}{\mathbb{E}}{\left[\operatorname{softplus}\left(l_{\text {gen}}\right)~ + ~\right.}\left.\lambda_{\text {pos}}\left\|d_{\text {rand}}-d_{\text{gen}}\right\|_{2}^{2}\right]\end{aligned}\]</span> Where $ _{pos}$ is the fine-tuning weight factor.</p><p><strong>4. GAN inversion objective（GAN反演）</strong></p><p>​ Jointly optimizing the encoder E and discriminator D while freezingthe generator G, the goal is to ensure consistency between the sampledcontent and pose and the content and pose extracted by the encoder fromthe generated images (bottom-left). <span class="math display">\[z_{pred}, ~ d_{pred} ~ = ~E(I_{gen})\]</span> GAN inversion is performed using the <code>MSE</code> lossfunction: <span class="math display">\[\begin{array}{r}\mathcal{L}_{\mathrm{GAN}^{-1}}(E)=\underset{\substack{z_{\text {rand }}\sim p_{\mathrm{z}} \\d_{\text {rand }} \sim p_{d}}}{\mathbb{E}}\left[\left\|z_{\text {pred}}-z_{\text {rand }}\right\|_{2}^{2} ~+ ~\right.\left.\left\|d_{\text {pred }}-d_{\text {rand }}\right\|_{2}^{2}\right]\end{array}\]</span> <strong>5. Reconstruction objective（重建）</strong></p><p>​ By using the encoder E to extract its latent code and posepredictions, adjusting the generator G on real images, and renderingtheir views using the predicted poses, the goal is to promote structuralconsistency in the image space and make the images clearer(top-right).</p><p><span class="math display">\[z_{pred}, ~ d_{pred} ~ = ~ E(I_{real}) \\I_{recon} ~ = ~ G(z_{pred}, ~ d_{pred})\]</span></p><p>The reconstruction loss function, improved based on <code>MSE</code>,is as follows:</p><p><span class="math display">\[\begin{array}{r}\mathcal{L}_{\text {recon }}(G, E)=\underset{\begin{array}{c}{I_{real} } \sim { p_{real} }\end{array}}{\mathbb{E}}\left[\left\|I_{\text {recon }}-I_{\text {real}}\right\|_{2}^{2} ~ + ~\right. \\\lambda_{\text {ssim }} \mathcal{L}_{\text {ssim }}\left(I_{\text {recon}}, I_{\text {real }}\right) ~ + \\\left.\lambda_{\text {vgg }} \mathcal{L}_{\text {vgg }}\left(I_{\text{recon }}, I_{\text {real }}\right)\right]\end{array}\]</span></p><p>Where <span class="math inline">\(\mathcal{L}_{ssim}\)</span> is theSSIM loss, and <span class="math inline">\(\lambda_{ssim}\)</span> isthe weight factor for the SSIM loss, while <spanclass="math inline">\(\mathcal{L}_{vgg}\)</span> is the VGG perceptualloss, and <span class="math inline">\(\lambda_{vgg}\)</span> is itsweight factor.</p><p><strong>6. Conditional adversarialobjective（条件对抗网络）</strong></p><p>​ The reconstruction objective aims to improve the quality ofwell-reconstructed individual views extracted by the encoder E. This maylead to a tendency for the network to predict unimportant poses or makeunrealistic predictions for the reconstruction of other poses extractedfrom <span class="math inline">\(p_d\)</span>. To mitigate this issue,an additional adversarial objective is applied when the generatorrenders an image <span class="math inline">\(I_{real}\)</span> underrandom poses:</p><p><span class="math display">\[l_{cond}, ~ d_{cond} ~=~D^*(G(z_{pred}, ~d_{rand}))\]</span></p><p>The loss function for this conditional adversarial objective is:</p><p><span class="math display">\[\mathcal{L}_{cond}(G, ~E) ~=~\underset{\substack{\substack{I_{real}\sim p_{real}} \\d_{rand} \sim p_d}}{\mathbb{E}}\left[\operatorname{softplus}(-l_{cond}) \right]\]</span></p><p>​ This additional adversarial objective helps in ensuring that thegenerated images under random poses are realistic and maintainconsistency with the real image distribution.</p><p><strong>7. Encoder warm-up（编码器预热）</strong></p><p>​ The reconstruction loss can easily dominate and lead to overfittingto input views, potentially causing the model to lose its ability torepresent 3D structures. To address this issue, the authors introduced asimple "pre-warmup" strategy. In the first half of the trainingiterations, the encoder is frozen while optimizing both thereconstruction and conditional adversarial losses. Only the generator isoptimized for these two objectives. This serves as a "pre-warmup" forthe generator to roughly learn the correspondence between encoderoutputs and encoded images. Subsequently, the encoder is unfrozen,allowing it to further refine its learned representations. After thepre-warmup phase, the encoder and generator form a pretrainedautoencoder that can generate representations close to real 3Drepresentations, avoiding the cumbersome early reconstruction objective,which can be challenging to balance with the GAN objective.</p><p>The authors demonstrated the necessity of this strategy throughablation studies and compared it to cases where only a small weight wasallocated to the reconstruction loss. This strategy helps strike abalance between representation learning and reconstruction, ensuringthat the model can effectively capture 3D structures while generatingrealistic images.</p><p><strong>8. Training and Inference（训练与推测）</strong></p><p>​ The discriminator and GAN inversion objectives are optimized at eachiteration, while the GAN generator objective is optimized every otheriteration. The reconstruction and conditional adversarial objectives arejointly optimized during odd-numbered iterations with a weighting factor<span class="math inline">\(λ_{recon}\)</span>:</p><p><span class="math display">\[\mathcal{L}_{odd} ~=~ \mathcal{L}_{cond} ~ + ~\lambda_{recon}\mathcal{L}_{recon}\]</span></p><p>​ During the inference phase, Pix2NeRF requires only one input image.This image can be fed into the encoder E, then into the generator G,followed by an arbitrary choice of pose for novel view synthesis.Additionally, sampling from the prior distribution <spanclass="math inline">\(p_z\)</span> can be done instead of obtaining thelatent code z from the encoder. This allows the model to synthesizenovel samples similar to π-GAN.</p><h4 id="experiment">3 Experiment</h4><p><strong>1. Datasets</strong></p><p>CelebA: 200,000 images of ==human faces==, using the <em>aligned</em>version and employing center cropping to capture the approximate facialarea. 8,000 of these images are utilized as a test set.</p><p>CARLA: Comprising 10,000 images with 16 different ==car models==,generated using the CARLA driving simulator with random texturerendering.</p><p>ShapeNet-SRN: This dataset consists of 50 rendering views fromShapeNet, with each instance having an Archimedean spiral camera pose.The training set is filtered by the authors to include only the upperhemisphere since the validation and test sets of the ShapeNet-SRNdataset do not include the lower hemisphere. The authors utilize==chair== segmentation for comparison with previous multi-viewmethods.</p><p><strong>2. Technical details</strong></p><p><code>Baseline:</code> ==π-GAN (PyTorch)==, reusing its generator anddiscriminator architecture.</p><p><code>Parameters:</code> The latent code prior distribution <spanclass="math inline">\(p_z\)</span> is chosen as a multivariate uniformdistribution on [-1, 1]. The discriminator architecture is used as theencoder backbone, and a ==tanh== activation function is added at the endof the latent code head. All models are optimized using the Adamoptimizer for 300,000 iterations. For the CelebA model, training is doneat a resolution of 64x64 with a batch size of 48, and each ray samples24 points. The learning rates for the discriminator, generator, andencoder are set to ==2e-4, 6e-5, and 2e-4==, respectively. For othermodels, training is conducted at a resolution of 32x32, with each raysampling 96 points. The learning rates are set to ==4e-5, 4e-4, and4e-4== for the discriminator, generator, and encoder, respectively.</p><p><strong>3. Comparison of Results</strong></p><figure><img src="3.png?80*80" alt="result" /><figcaption aria-hidden="true">result</figcaption></figure><p><strong>4. Ablation studies</strong></p><p>​ The primary model components compared are: <em>Naive GAN inversion,Auto-encoder, No GAN inversion, No conditional adversarial objective,Warm-up</em>.</p><p><code>Naive GAN inversion:</code>In the naive GAN inversion, apre-trained GAN is used, its weights are frozen, and an encoder istrained to map images to their corresponding latent codes. The resultsshow that the encoder can learn an approximate mapping from images tolatent codes.</p><p><code>Auto-encoder:</code>The architecture of π-GAN is used as anauto-encoder, and the ==latent space is removed from the pipeline==.Only reconstruction and conditional adversarial models are trained. Theresults yield good quality, but noticeable 3D inconsistencies areobserved.</p><p><code>No GAN inversion:</code> After ==removing GAN inversion fromthe pipeline==, the visual results become blurry. This step may be theconnection between π-GAN training and reconstruction, affecting overallperformance.</p><p><code>No conditional adversarial objective:</code> Disabling theconditional adversarial loss and retraining results in incompleterendering, noticeable artifacts, and reduced consistency.</p><p><code>Warm-up:</code> Three models are trained separately withdifferent warm-up strategies: ==no warm-up, no unfreezing of the encoder(always warm), and assigning lower weights for reconstruction instead ofwarming up==. Without a warm-up strategy, it leads to overfitting ofinput views and an inability to generate meaningful content in novelposes. Not unfreezing the encoder results in relatively weakdistillation with few details. Using lower reconstruction weightsinstead of warming up causes the breakdown of patterns in newlyattempted synthesis.</p><h4 id="other">4 Other</h4><ol type="1"><li>Leveraging more mature encoder architectures like Pixel2Style2Pixelin 2D GAN forward inversion may significantly improve the performance ofPix2NeRF.</li></ol>]]></content>
    
    
    <categories>
      
      <category>工作总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>每周总结(23.09.04-23.09.10)</title>
    <link href="/posts/38005/"/>
    <url>/posts/38005/</url>
    
    <content type="html"><![CDATA[<div align='center'><H3>Weekly report</div><p><strong>Date: 23.09.04-23.09.10</strong></p><h3 id="paper">Paper</h3><p><strong>Title:</strong><code>Efficient Geometry-aware 3D Generative Adversarial Networks</code></p><h5 id="method-contribution-related-work.">1 Method, Contribution,Related Work.</h5><p><strong>1.1 Method:</strong></p><p>​ A hybrid explicit-implicit 3D perception network has been designed,utilizing a <em>memory-efficient three-plane representation</em> toexplicitly store features on axis-aligned planes aggregated by a<em>lightweight implicit feature decoder</em>. This approach aims toachieve efficient volume rendering and enhance the computationalefficiency of 3D foundational rendering. It incorporates certain imagespace approximations deviating from traditional 3D foundationalrendering, while introducing a dual-discriminative strategy thatmaintains consistency between neural rendering and the final output toregulate tendencies of view inconsistency.</p><blockquote><p>Explicit representation allows for fast evaluation but requiressubstantial memory, making it challenging to scale to high resolutionsor complex scenes. Implicit representation, while advantageous in termsof memory efficiency and scene complexity, employs large fully connectednetworks for evaluation, leading to slow training speeds. Hence,explicit and implicit representations offer complementary benefits.</p></blockquote><figure><img src="1.png" alt="三平面模型" /><figcaption aria-hidden="true">三平面模型</figcaption></figure><p><strong>1.2 Contribution:</strong></p><ul><li>Introduced a three-plane-based 3D GAN framework that is bothefficient and expressive, enabling high-resolution geometric perceptionimage synthesis.</li><li>Developed a 3D GAN training strategy that promotes multi-viewconsistency through dual-discrimination and generator pose conditioning,while faithfully modeling pose-related attribute distributions presentin real-world datasets, such as expressions.</li><li>Demonstrated the latest results in unconditional 3D perception imagesynthesis on the FFHQ and AFHQ Cats datasets, as well as high-quality 3Dgeometric graphics learned entirely from 2D outdoor images.</li></ul><p><strong>1.3 Related Work:</strong></p><ol type="1"><li>Neural scene representation and rendering</li></ol><p>​ A new hybrid explicit-implicit 3D perception network has beendesigned, which utilizes a memory-efficient three-plane representationto explicitly store features on axis-aligned planes aggregated by alightweight implicit feature decoder, aiming to achieve efficient volumerendering.</p><ol start="2" type="1"><li>Generative 3D-aware image synthesis</li></ol><p>​ An efficient 3D GAN architecture with a 3D-based prior bias iscrucial for successfully generating high-resolution, view-consistentimages, and high-quality 3D shapes. Therefore, the authors adopted thefollowing approaches:</p><ul><li>Directly leverage a 2D CNN feature generator, namely<em>StyleGAN2</em>.</li><li>The three-plane representation allows this paper's approach toutilize neural volume rendering as a prior bias, making itcomputationally more efficient than fully implicit 3D networks.</li><li>Employing an up-sampling based on 2D CNNs after neural renderingwhile introducing dual discriminators to mitigate view inconsistenciesbrought about by the up-sampling layers.</li></ul><h4 id="model-and-modules">2 Model and Modules</h4><p><strong>2.1 Tri-plane hybrid 3D represnetation</strong></p><p><code>IDEA:</code>Hybrid explicit-implicit tri-planerepresentation.</p><p><code>Implement:</code>Align explicit features along threeaxis-aligned orthogonal feature planes, each with a resolution of N×N×C,where N represents spatial dimensions and C stands for the number ofchannels. To query any 3D position point <em>x</em>, project it onto thethree feature planes to retrieve the corresponding feature vector <spanclass="math inline">\((F_{xy} ~~,~~ F_{xz}~ , ~ F_{yz})\)</span> throughbilinear interpolation and then aggregate these three feature vectors bysummation. Finally, feed this aggregated feature F into a small decoder(MLP) to decode it into color and density.</p><p><strong>2.2 3D GAN framework</strong></p><p><code>IDEA:</code>Train a 3D GAN for collective perception imagesynthesis from 2D photos without the need for any explicit 3D ormulti-view supervision. Simultaneously, use a pre-trained pose detectorto associate each training image with a set of camera intrinsics andextrinsics(<em>Deep3DFaceReconstruction</em>).</p><p><code>Implement &amp;&amp; Overview:</code></p><figure><img src="2.jpg" alt="Overview" /><figcaption aria-hidden="true">Overview</figcaption></figure><blockquote><ol type="a"><li><p>A pose-conditioned StyleGAN2 feature generator and mappingnetwork.</p></li><li><p>Three-plane 3D representation with a lightweight featuredecoder.</p></li><li><p>A neural voxel renderer.</p></li><li><p>A super-resolution module.</p></li><li><p>A pose-conditioned StyleGAN2 discriminator with dualdiscrimination.</p></li></ol></blockquote><p>​ This architecture cleverly decouples feature generation and neuralrendering, enabling the utilization of the powerful StyleGAN2 generatorfor generalizing 3D scenes. Furthermore, the lightweight three-plane 3Drepresentation can effectively convey rich information and achievehigh-quality 3D perception view synthesis in real-time. Additionally, atwo-stage training strategy is employed to accelerate training speed.The first stage involves training with reduced <spanclass="math inline">\((64^2)\)</span> neural rendering resolution, whilethe second stage consists of short-term fine-tuning at full <spanclass="math inline">\((128^2)\)</span> neural rendering resolution.</p><p><strong>2.3 CNN generator backbone and rendering</strong></p><p><code>IDEA:</code>The features of the <em>three-plane representationare generated by the StyleGAN2 CNN generator</em>. Random latent codesand camera parameters are first processed by the mapping network toproduce intermediate latent codes, which are then used to modulate theconvolution kernels of the separately synthesized network.</p><p><code>Implement:</code>Change the output shape of the StyleGAN2backbone network to generate a feature map of dimensions 256×256×96instead of generating a three-channel RGB image. Sample features fromthe three planes and merge the features sampled from these three planes,which are then fed into a lightweight decoder (MLP) with a single hiddenlayer of 64 neurons and the activation function being<em>softplus</em>.</p><p><code>Module:</code> StyleGAN2, MLP</p><p><strong>2.4 Super resolution</strong></p><p><code>IDEA:</code> Perform volume rendering at intermediateresolution <span class="math inline">\((128^2)\)</span> and rely onimage space convolutional upsampling for rendering to image sizes of<span class="math inline">\((256^2 ~ or ~ 512^2)\)</span>.</p><p><code>Implement:</code> Comprised of two blocks modulating theconvolutional layers in StyleGAN2:</p><ul><li>Upsampling, increasing the resolution from 128×128×3 to512×512×3.</li><li>Adapting the 32-channel feature map to the final RGB image.</li></ul><p><strong>2.5 Dual discrimination</strong></p><p><code>IDEA:</code>Utilize the StyleGAN2 discriminator and made it twomodifications.</p><p><code>Implement:</code> A) Interpret the feature map as alow-resolution RGB image. The dual discriminator ensures consistencybetween low-resolution RGB images and high-resolution images byupsampling them using bilinear interpolation to the same 512×512×3 sizeand concatenating them with the adjusted <spanclass="math inline">\((I^+_{RGB})\)</span>, resulting in a 6-channelimage. B) Concatenate the input 3-channel RGB image with itsappropriately blurred counterpart to form a 6-channel image as the inputto the discriminator.</p><p><code>Module:</code>StyleGAN2-ADA strategy</p><blockquote><p>StyleGAN2-ADA Strategy: Pass the camera's intrinsic and extrinsicmatrices (P) to the discriminator as conditional labels. This modulationintroduces additional information to guide the generator in learning thecorrect 3D priors.</p></blockquote><p><strong>2.6 Modeling pose-correlated attributions</strong></p><p><code>IDEA:</code>Introduced <em>generator pose conditioning</em> asa means to model and decouple the correlation between observed poses andother attributes in the training images.</p><p><code>Implement:</code>Following the StyleGAN2-ADA conditionalgeneration strategy, propose a backbone mapping network that not onlyprovides a latent code z but also takes camera parameters P asinput.</p><h4 id="other-details">3 Other details</h4><p><strong>3.1 Pose Estimators</strong></p><p>​ Augment the dataset using horizontal flipping and utilizepre-existing pose estimation to extract approximate camera extrinsicparameters.</p><blockquote><p>Pre-existing pose estimation methods:</p><ol type="1"><li>https://github.com/Microsoft/Deep3DFaceReconstruction for generatingpose data for faces (FFHQ).</li><li>https://github.com/kairess/cat_hipsterizer for generating pose datafor cats (AFHQv2 Cats).</li></ol></blockquote><h3 id="other-work">Other Work</h3><p>​ To read the code of this paper, especially the pose estimators andrun these code in the two links. Then I've been learning about StyleGAN2to understand methods in this paper better.Summray</p><p>​阅读了一篇论文<code>Efficient Geometry-aware 3D Generative Adversarial Networks</code>，并将上次的论文重新总结了一下。去了解运行了姿态估计的代码，并且去了解了StyleGAN2的一些思想，和StyleGAN2-ADA自适应增强等一些知识。</p><h3 id="paper-1">Paper</h3><h4id="title-efficient-geometry-aware-3d-generative-adversarial-networks"><strong>Title:</strong><code>Efficient Geometry-aware 3D Generative Adversarial Networks</code></h4><h5 id="一提出的方法贡献相关工作">一、提出的方法、贡献、相关工作</h5><p><strong>1.方法：</strong></p><p>​设计了一种混合显式-隐式3D感知网络，该网络使用内存高效的<code>三平面表示显式地</code>存储由轻量级<code>隐式特征解码器聚合</code>的轴对齐平面上的特征，以实现高效的体绘制，提高了3D基础渲染的计算效率。使用了一些偏离3D基础渲染的图像空间近似，同时引入了一种双重判别策略，该策略保持神经渲染和最终输出之间的一致性，以规范其视图不一致的趋势。</p><blockquote><p>显式表示可以进行快速评估，但是需要很大的内存，使得这种方式难以扩展到高分辨率或复杂场景。隐式表示虽然在内存效率和场景复杂性方面有优势，但是这种方法使用大型的全连接网络进行评估，使得训练速度缓慢。因此，显式和隐式表示提供了互补的好处。</p></blockquote><figure><img src="1.png" alt="三平面模型" /><figcaption aria-hidden="true">三平面模型</figcaption></figure><p><strong>2.贡献：</strong></p><ul><li>引入了一个基于三平面的3DGAN框架，该框架既高效又富有表现力，以实现高分辨率几何感知图像合成。</li><li>开发了一种3DGAN训练策略，通过双重判别和生成器姿势条件促进多视图一致性，同时忠实地建模现实世界数据集中存在的姿势相关属性分布（例如表达式）。</li><li>展示了在FFHQ和AFHQCats数据集上无条件3D感知图像合成的最新结果，以及完全从2D野外图像中学习的高质量3D几何图形。</li></ul><p><strong>3.相关工作：</strong></p><p>​ 1）Neural scene representation and rendering(神经场景表示和渲染)</p><p>​设计了一种新的混合显式隐式3D感知网络，该网络使用内存高效的三平面表示显式地存储由轻量级隐式特征解码器聚合的轴对齐平面上的特征，以实现高效的体绘制</p><p>​ 2）Generative 3D-aware image synthesis(生成式3D感知图像合成)</p><p>​ 具有基于3D的先验偏差的高效3DGAN架构对于成功生成高分辨率视图一致图像和高质量3D形状至关重要。所以作者采用了以下方法：</p><p>​ a. 直接利用基于2D CNN特征生成器，即<code>StyleGAN2</code>。</p><p>​ b.三平面表示使得该论文的方法能利用神经体渲染作为先验偏差，在计算上比完全隐式3D网络更有效。</p><p>​ c. 在神经渲染后采用基于2DCNN的向上采样，同时引入双重辨别器去避免上采样层带来的视图不一致。</p><h5 id="二模型与模块">二、模型与模块</h5><p><strong>1. Tri-plane hybrid 3Drepresentation(Tri-plane混合3D表示)</strong></p><p><code>思想：</code>hybrid explicit-implicit tri-planerepresentation(混合显式-隐式三平面表示)。</p><p><code>实现：</code>沿着三个轴对齐的正交特征平面对齐显式特征，每个特征平面的分辨率均为N×N×C，N为空间维度，C为通道数。通过将3D位置投影到三个特征平面中来查询任何3D位置点<code>x</code>，通过双线性插值检索相应的特征向量<spanclass="math inline">\((F_{xy} ~,~ F_{xz}~ , ~F_{yz})\)</span>，然后通过求和来汇总这三个特征向量。最后将这个汇总的特征F输入到一个小型解码器(MLP)来解码为颜色和密度。</p><p><code>模块：</code>小型MLP网络。</p><p><strong>2. 3D GAN framework(3D GAN框架)</strong></p><p><code>思想：</code>训练一个3DGAN，用于从2D照片中进行集合感知图像合成，而无需任何显式3D或者多视图监督。同时使用现成的姿态检测器，将每个训练图像与一组相机内参和外参相关联(<code>Deep3DFaceReconstruction</code>)。</p><p><code>实现/Overview:</code></p><figure><img src="2.jpg" alt="Overview" /><figcaption aria-hidden="true">Overview</figcaption></figure><blockquote><p>​ a. 一个基于姿态条件的StyleGAN2特征生成器和映射网络。</p><p>​ b. 一个具有轻量级特征解码器的三平面3D表示。</p><p>​ c. 一个神经体素渲染器。</p><p>​ d. 一个超分辨率模块。</p><p>​ e. 一个基于姿态条件的具有双重辨别的StyleGAN2辨别器。</p></blockquote><p>​这个架构巧妙地将特征生成和神经渲染解耦，使得可以利用强大的StyleGAN2生成器进行3D场景的泛化。此外，轻量级的三平面3D表示既能够表达丰富的信息，又能够在实时中实现高质量的3D感知视图合成。同时，采用两阶段训练策略加速训练速度。第一个阶段：使用减少<spanclass="math inline">\((64^2)\)</span>神经渲染分辨率进行训练；第二个阶段：在完全<spanclass="math inline">\((128^2)\)</span>神经渲染分辨率上的短期微调。</p><p><strong>3. CNN generator backbone andrendering(CNN生成器主干和渲染)</strong></p><p><code>思想：</code>由<code>StyleGAN2 CNN生成器生成三平面表示的特征</code>。随机潜在代码和相机参数首先由映射网络处理以产生中间潜在代码，然后调制单独合成网络的卷积核。</p><p><code>实现：</code>改变StyleGAN2主干网络的输出形状，不是生成三通道RGB图像，而是生成一个256×256×96的特征图像。从三平面采样特征，并融汇从三个平面采样的特征，输入到轻量级解码器(MLP，64个神经元的单个隐藏层，激活函数：softplus)。</p><p><code>模块：</code>StyleGAN2、MLP</p><p><strong>4. Super resolution(超分辨率)</strong></p><p><code>思想：</code>使用中等分辨率<spanclass="math inline">\((128^2)\)</span>进行体渲染，并依靠图像空间卷积上采样神经渲染到<spanclass="math inline">\((256^2 ~ or ~ 512^2)\)</span>图像大小。</p><p><code>实现：</code>由StyleGAN2调制卷积层的两个块组成。1）上采样，将128×128×3分辨率提高到512×512×3的分辨率。2）调整32通道特征图到最终的RGB图像。</p><p><strong>5. Dual discrimination(双重辨别器)</strong></p><p><code>思想：</code>使用StyleGAN2的辨别器，并进行了两次修改。</p><p><code>实现：</code>1）将特征图解释为低分辨率RGB图像。双重辨别器确保低分辨率RGB图像与高分辨率图像的一致性，通过双线性上采样成同样512×512×3图像并与调整后的<spanclass="math inline">\((I^+_{RGB})\)</span>进行连接变成6通道图像。2）将输入的3通道RGB图像与其适当模糊后的图像进行连接，变成6通道图像作为辨别器的输入。</p><p><code>模块：</code>StyleGAN2-ADA策略</p><blockquote><p>​StyleGAN2-ADA策略：将渲染相机的内外矩阵(P)传递给鉴别器作为条件标签。这种调节引入了额外的信息，指导生成器学习正确的3D先验。</p></blockquote><p><strong>6. Modeling pose-correlatedattributes(建模姿态相关属性)</strong></p><p><code>思想：</code>引入了<code>generator pose conditioning(生成器姿势条件)</code>作为建模和解耦训练图像中观察到的姿势与其他属性之间的相关性的一种手段。</p><p><code>实现：</code>按照StyleGAN2-ADA条件生成策略，提出一个主干映射网络，不仅提供一个潜在代码z，同时提供相机参数P作为输入。</p><h5 id="三其他细节">三、其他细节</h5><p><strong>1. Pose Estimators(姿态估计)</strong></p><p>​用水平翻转的方法来扩充数据集，并使用现成的姿态估计来提取近似的相机外部参数。</p><blockquote><p>现成的姿态估计方法：</p><p>1）https://github.com/Microsoft/Deep3DFaceReconstruction，用来生成脸的数据集的姿态(FFHQ)。</p><p>2）https://github.com/kairess/cat_hipsterizer，用来生成猫的数据集的姿态(AFHQv2Cats)。</p></blockquote><h4id="titileshape-pose-and-appearance-from-a-single-image-via-bootstrapped-radiance-field-inversion">Titile：<code>Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion</code></h4><h5 id="一提出的方法与贡献">一、提出的方法与贡献</h5><p><strong>1.方法：</strong></p><p>​作者提出了一种新的方法，将无条件生成模型与混合反演范式相结合，从单个图像中恢复三维信息。具体来说，他们使用神经辐射场（NeRF）来表示三维场景，并使用编码器产生潜在表示和姿态的第一个猜测。然后，他们通过优化来细化这些初始估计，以获得更准确的重建。</p><p><strong>2.贡献：</strong></p><ul><li>引入了一个基于NeRF的端到端单视图三维重建管道。在这种情况下，我们成功地展示了CMR基准下自然图像的<spanclass="math inline">\(360^◦\)</span>对象重建。</li><li>提出了一种用于NeRF的混合反演方案，以加快预训练的3D感知生成器的反转。</li><li>受姿态估计文献的启发，我们提出了一种基于PnP的姿态估计器，它利用我们的框架并且不需要额外的数据假设。</li></ul><h5 id="二模型与模块-1">二、模型与模块</h5><p><strong>1. Unconditional generatorpre-training（无条件生成器预训练框架）</strong></p><figure><img src="3.png" alt="无条件生成器" /><figcaption aria-hidden="true">无条件生成器</figcaption></figure><p><code>思想：</code>主要思想来自EG3D的主干网络，三平面编码。<code>该部分被框架使用基于NeRF的生成器G与2D图像鉴别器相结合。</code></p><p><code>模块：</code>StyleGAN2，SDF representation，Attention-basedcolor mapping，Path Length Regularization revisited。</p><blockquote><p>StyleGAN2：生成模型，SDF representation：3D表示，</p><p><strong>Attention-based color mapping：提高颜色泛化性, Path LengthRegularizationrevisited：使三平面解码器不正则化，提高学习率。</strong></p></blockquote><p><strong>2. Bootstrapping and poseestimation（自举和姿态估计）</strong></p><figure><img src="4.png" alt="姿态估计" /><figcaption aria-hidden="true">姿态估计</figcaption></figure><p><code>思想：</code>主要思想来自NOCS，<code>改进：是使用从无条件生成器生成的数据来训练编码器而不是手工数据。</code></p><p><code>实现：</code>1）冻结G并训练图像编码器E，联合估计对象的姿势及其潜在代码（自举）的初始猜测。2）对于姿态估计，我们采用了一种原则性的方法来预测屏幕空间中的规范映射通过透视n点(PnP)算法。<code>输入真实图像，将预测的规范映射转换为点云，并运行PnP求解器来恢复所有姿态参数(视图矩阵和焦距)。</code></p><p><code>模块：</code>SegFormer</p><blockquote><p><strong>训练SegFormer网络来从RGB图像中预测规范图和latent codew</strong></p></blockquote><p><strong>SegFormer分割网络图：</strong></p><figure><img src="5.png" alt="分割网络" /><figcaption aria-hidden="true">分割网络</figcaption></figure><p><strong>3. Reconstruction via hybrid GANinversion（通过混合GAN反演重建）</strong></p><figure><img src="6.png" alt="混合反演" /><figcaption aria-hidden="true">混合反演</figcaption></figure><p>通过基于梯度的优化(混合反演)改进了几个步骤的姿态和潜在代码。损失函数：VGG</p><p><code>模块：</code>adaptive discriminator augmentation(ADA)</p><blockquote><p>有助于减少梯度的方差，使我们能够进一步提高学习率。</p></blockquote><h3 id="other-work-1">Other Work</h3><p>​阅读这篇论文的代码，特别是姿态估计器的部分，并在这两个链接中运行这些代码。然后，我一直在学习关于StyleGAN2，以更好地理解这篇论文中的方法。</p>]]></content>
    
    
    <categories>
      
      <category>工作总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>每周总结(23.08.28-23.09.03)</title>
    <link href="/posts/36354/"/>
    <url>/posts/36354/</url>
    
    <content type="html"><![CDATA[<div align="center"><H3>Weekly report</div><p><strong>Date: 23.08.28-23.09.03</strong></p><h4 id="paper">1.Paper</h4><p><strong>Title:</strong><code>Efficient Geometry-aware 3D Generative Adversarial Networks</code></p><p>​这篇论文介绍了一种高效的几何感知3D生成对抗网络（GANs）方法，该方法可以使用单视角2D照片集合生成高质量的多视角一致图像和3D形状。该方法采用了混合显式-隐式网络架构，可以实现实时合成高分辨率的多视角一致图像和高质量的3D几何形状。该方法还提出了双重歧视和生成器姿势调节来减少表情扭曲，提高多视角一致性。作者在ShapeNet和CelebA数据集上进行了实验，结果表明该方法在多视角一致性和3D形状质量方面优于现有方法，并且可以合成具有逼真面部表情和姿势的高质量图像。此外，作者还讨论了该方法的一些局限性和可能的改进方向。</p><p><strong>Framework:</strong></p><p><img src="23.08.28-23.09.03/image-20231009220945991.png" alt="image-20231009220945991" style="zoom:80%;" /></p><p>​设计了一种新的混合显式隐式3D感知网络，该网络使用内存高效的三平面表示显式地将特征存储在由轻量级隐式特征解码器聚合的轴对齐平面上，以实现高效的体绘制。（还没看完，代码跑通了）</p><p><strong>Title:</strong><code>Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion</code></p><p>​这篇论文介绍了一种从单一图像中重建对象的形状、姿态和外观的新方法。该方法利用了近期在NeRF表示方面的进展，并将问题构建为一个3D感知的GAN反演任务。通过学习一个编码器来加速这个过程，编码器提供了解决方案的初步猜测，并包括了一个有原则的姿态估计技术。该方法在合成和真实基准测试中都达到了最先进的性能，并且在小数据集上表现出了高效和有效的特点。在未来，作者希望将这种方法扩展到更高的分辨率，并通过利用额外视图或形状先验的半监督来改进重建表面质量。他们还希望探索从数据中自动推断姿态分布的方法。</p><p><strong>Framework:</strong></p><p>​对论文的框架进行分析，作者使用EG3D中提出的的三平面表示方法设计了一个无条件生成器预训练模型，提高渲染速度的同时保证了图像的分辨率。同时论文中使用3DGAN反演的方法来进行迭代优化，将反演出来的图像与真实图像进行损失计算，从而对模型进行调整。并且对生成的图像和真实图像做了图像增强处理，加快收敛。因为模型中使用了PnP(Perspective-n-point)方法用于从2D图像中估计物体姿态，这种设计虽然能够获得较为准确的物体姿态，但是会导致训练的速度很慢，同时还要求及其高的GPU缓存。</p>]]></content>
    
    
    <categories>
      
      <category>工作总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>每周总结(23.08.14-23.08.20)</title>
    <link href="/posts/22085/"/>
    <url>/posts/22085/</url>
    
    <content type="html"><![CDATA[<div align="center"><H3>Weekly report</div><p><strong>Date: 23.08.14-23.08.20</strong></p><h4 id="paper">1.Paper</h4><p><strong>Title:</strong><code>Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation</code></p><p><strong>Summary:</strong></p><p>​ Pix2NeRF consists of two main components: a generative model and areconstruction objective. The generative model uses a combination of agenerative adversarial network (GAN) and an autoencoder to encode inputimages into vector representations in latent space and generatehigh-quality 3D-aware images. The reconstruction objective uses acombination of reconstruction error and MSE supervision to penalize thegenerator and learn a "canonical" 3D space. These two componentstogether form the Pix2NeRF method for generating Neural Radiance Fields(NeRF) based on a single input image. Overall, Pix2NeRF is an excitingnew approach to generating high-quality 3D-aware images and has thepotential to be used in a wide range of applications.</p><p><strong>Framework:</strong></p><p>​ The framework of the paper consists of two parts: a generative modeland a reconstruction objective. The generative model uses a combinationof a generative adversarial network (GAN) and an autoencoder to encodeinput images into vector representations in latent space and generatehigh-quality 3D-aware images. The reconstruction objective uses acombination of reconstruction error and MSE supervision to penalize thegenerator G and learn a "canonical" 3D space. These two componentstogether form the Pix2NeRF method for generating Neural Radiance Fields(NeRF) based on a single input image.</p><p><strong>Generative mode and Reconstruction objective: </strong></p><p>​ The generative model uses a combination of a generative adversarialnetwork (GAN) and an autoencoder. The goal of the GAN is to train thegenerator G and discriminator D to generate high-quality 3D-awareimages. The goal of the autoencoder is to encode input images intovector representations in latent space, which can be used for imagereconstruction or generating new viewpoints. We use the π-GAN objectiveto optimize the training of the generator G and discriminator D, toimprove the 3D consistency of the generated images. Specifically, thegenerator G is trained to "fool" the discriminator D by progressivelygenerating realistic images. The generator G and discriminator D aretrained together to generate high-quality 3D-aware images.</p><p>​ The reconstruction objective uses a combination of reconstructionerror and MSE supervision to penalize the generator G and learn a"canonical" 3D space. The reconstruction error refers to the differencebetween the generated 3D-aware image and the ground truth image. The MSEsupervision penalizes the generator G if the image pose recovered by thediscriminator does not correspond to the sampled pose. This helps tolearn a "canonical" 3D space, especially when the pose distribution ofreal data is noisy. These two components together form the Pix2NeRFmethod for generating Neural Radiance Fields (NeRF) based on a singleinput image.</p><p><strong>The steps of Pix2NeRF:</strong></p><blockquote><ol type="1"><li>Extract 2D features from a single input image.</li><li>Encode the 2D features into vector representations in latent spaceusing a combination of a generative adversarial network (GAN) and anautoencoder.</li><li>Sample random vectors from the latent space and use the generator togenerate 3D-aware images.</li><li>Penalize the generator using a combination of reconstruction errorand MSE supervision to learn a "canonical" 3D space.</li><li>Repeat steps 3 and 4 until the difference between the generated3D-aware image and the ground truth image is minimized.</li><li>Use the generated NeRF model to generate new viewpoints orreconstruct the input image.</li></ol></blockquote><p>In summary, the steps of Pix2NeRF are: extract 2D features, encodeinto vector representations in latent space, generate 3D-aware images,learn a "canonical" 3D space, and generate new viewpoints or reconstructthe input image.</p><h4 id="code">2.Code</h4><p>​ I tested the trained model of 'Shape, Pose, and Appearance from aSingle Image via Bootstrapped Radiance Field Inversion' and trained anlarger model(batch size is 16). Built the environment of this paper.</p>]]></content>
    
    
    <categories>
      
      <category>工作总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>每周总结(23.08.07-23.08.13)</title>
    <link href="/posts/8519/"/>
    <url>/posts/8519/</url>
    
    <content type="html"><![CDATA[<div align="center"><H3>Weekly report</div><p><strong>Date:2023.08.07-2023.08.13</strong></p><h4 id="paper">1.Paper</h4><p><strong>Title:</strong><code>Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion</code></p><p><strong>Summary:</strong></p><p>​ This paper presents a new method for reconstructing the shape, pose,and appearance of an object from a single image. The approach leveragesrecent advances in NeRF representations and frames the problem as a3D-aware GAN inversion task. The process is accelerated by learning anencoder that provides a first guess of the solution and incorporates aprincipled pose estimation technique. The method achievesstate-of-the-art performance on both synthetic and real benchmarks andis shown to be efficient and effective on small datasets. In the future,the authors hope to scale the method to higher resolutions and improvethe reconstructed surface quality by leveraging semi-supervision onextra views or shape priors. They also hope to explore ways toautomatically infer the pose distribution from the data.</p><p><strong>Framework:</strong></p><p>​ The overall framework of this paper is an end-to-end reconstructionframework for reconstructing 3D scenes from a single 2D image. Theframework includes the following steps:</p><blockquote><ol type="1"><li>Generating 3D scenes using NeRF representation.</li><li>Converting 3D scenes to 2D images using GAN inversion.</li><li>Accelerating the reconstruction process using a hybrid inversionmethod.</li><li>Estimating the 3D pose of the object using a deep learning-basedpose estimation technique.</li><li>Providing the first guess of the solution using an encoder.</li></ol></blockquote><p><strong>NeRF representation and GAN inversion:</strong></p><ol type="1"><li>Firstly, an unconditional generator G based on NeRF representationis trained in combination with a 2D image discriminator, following theliterature on 3D-aware GANs. This framework requires minimalassumptions, namely 2D images and the corresponding posedistribution.</li></ol><figure><img src="1.png?100*100" alt="NeRF representation" /><figcaption aria-hidden="true">NeRF representation</figcaption></figure><ol start="2" type="1"><li>Then, the generator G is frozen and an image encoder E is trained tojointly estimate the pose of the object as well as an initial guess ofits latent code (bootstrapping). For pose estimation, a principledapproach is adopted that predicts a canonical map in screen spacefollowed by a Perspective-n-Point (PnP) algorithm for estimation.</li></ol><figure><img src="2.png?100*100" alt="GAN inversion" /><figcaption aria-hidden="true">GAN inversion</figcaption></figure><ol start="3" type="1"><li>Finally, NeRF representation is used to generate 3D scenes.Specifically, the generator G and the encoder E are used to generate 3Dscenes, and a hybrid inversion method is used to convert the 3D scenesto 2D images.</li></ol><figure><img src="3.png?100*100" alt="hybrid inversion" /><figcaption aria-hidden="true">hybrid inversion</figcaption></figure><p><strong>Hybrid inversion:</strong></p><p>​ The encoder E and the generator G are used to generate 3D scenes,which are then converted to 2D images using the hybrid inversionmethod.</p><p>​ The hybrid inversion method consists of two steps: the first stepinvolves generating 3D scenes using the encoder E and the generator G,which the second step involves iteratively optimizing the 3D scenerepresentation using an optimization algorithm to obtain a more accuraterepresentation.</p><p>​ This iterative optimization method can be implemented usingbackpropagation and can be seamlessly integrated with the GAN framework.By using the hybrid inversion method, this paper is able to acceleratethe reconstruction process without sacrificing reconstructionquality.</p><h4 id="code">2. Code</h4><p>​ The code from this paper has been fully implemented; however, due tothe requirement of a minimum of 160GB GPU memory in their environment, Ihad to adjust the batch size to 8 (originally 32) to successfully trainthe model. I plan to test the trained model next week and explore thepossibility of training an even larger model afterward.</p>]]></content>
    
    
    <categories>
      
      <category>工作总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Summary</title>
    <link href="/posts/39277/"/>
    <url>/posts/39277/</url>
    
    <content type="html"><![CDATA[<div align= "center"><H3>Summary</div><p>​ Over the past one month, I have delved into the realm of computergraphics, exploring concepts like Camera Transformation, ProjectionTransformation, and Viewport Transformation. During this time, I alsogained insights into camera parameters, camera coordinate systems, andworld coordinate systems, developing proficiency in transformingcoordinates between them. Furthermore, I manually deduced renderingformulas to deepen my understanding. In parallel, I devoted time tostudying Deep Learning, broadening my knowledge in this fascinatingfield.</p><p>Following my theoretical endeavors, I embarked on a coding journey torestructure NeRF (Neural Radiance Fields). My aim was to rewrite theNeRF code from the Tensorflow framework to PyTorch framework. Thishands-on process not only enhanced my coding skills but also provided mewith a profound comprehension of NeRF, rendering techniques, and theworkings of Multi-Layer Perceptrons (MLP).</p><p>​ In particular, I came to understand two essential aspects of NeRF'scode:</p><p>​ Position Encoding: The authors of NeRF utilized Position Encoding toaddress challenges associated with accurately representinghigh-frequency variations in color and geometry. To achieve this, theyemployed a clever mapping of the input coordinates (x, y, z, and viewcoordinate) to a higher-dimensional space using high-frequencyfunctions. Notably, they used sine and cosine functions (sin^2(Lx) andcos^2(Lx), with L=10) to transform the three-dimensional input into asixty-dimensional space.</p><p>​ NeRF Model: In the code implementation, NeRF comprises an MLPnetwork with eight fully-connected ReLU layers. Each layer features 256dimensions, except for the fifth layer, which acts as a skip connectionand contains 316 features (256 from the previous layers and 60 from thepositional encoding). Additionally, there is an extra layer responsiblefor outputting the volume density and a 256-dimensional feature vector.This feature vector is combined with the positional encoding of theinput viewing direction (γ(d)), and the resulting combination isprocessed by an additional fully-connected ReLU layer with 128 channels.Finally, a final layer with a sigmoid activation function yields theemitted RGB radiance at position x, as viewed by a ray with directiond.</p><p>​ Loss: The loss function employed in the code is straightforward. Itcalculates the L2 loss between the rendered image and the ground truthimage, and subsequently optimizes the model based on this loss.</p><p>​ Besides, I read tow papers, named ‘Instant Neural GraphicsPrimitives with a Multiresolution Hash Encoding’ and ‘Shape, Pose, andAppearance from a Single Image via Bootstrapped Radiance FieldInversion’ respectively.</p><p>​ The paper "Instant Neural Graphics Primitives with a MultiresolutionHash Encoding" introduces a novel approach called "Instant NeuralGraphics Primitives (Instant-NGP)" with the integration of amultiresolution hash encoding. This method aims to improve theefficiency and performance of existing neural rendering algorithms.</p><p>The core idea of the paper is to utilize multiresolution hashencoding to encode the geometry and material information of scenes,enabling efficient rendering of complex 3D scenes. Instant-NGPrepresents scenes as a collection of basic graphics primitives (such asspheres, cubes, etc.) and associates each primitive with itscorresponding geometry and material information using hash encoding,resulting in an efficient scene representation and renderingprocess.</p><p>In the experimental section, the paper demonstrates the outstandingperformance of the Instant-NGP method in various 3D scene renderingtasks. Compared to traditional ray-tracing-based rendering algorithms,Instant-NGP achieves significant improvements in computation speed andmemory usage. Furthermore, Instant-NGP exhibits high scalability andversatility, making it suitable for a wide range of 3D scene renderingtasks.</p><figure><img src="1.jpg" alt="framework" /><figcaption aria-hidden="true">framework</figcaption></figure><figure><img src="2.jpg" alt="framework" /><figcaption aria-hidden="true">framework</figcaption></figure><p>​ After read the paper, I run it’s code successfully, but it’s codewritten by CUDA, I just learned how to use.</p><p>The paper "Shape, Pose, and Appearance from a Single Image viaBootstrapped Radiance Field Inversion" proposes an iterativeoptimization framework based on bootstrapped radiance field inversionfor estimating the 3D shape, pose, and appearance of objects from asingle image. The method iteratively optimizes the estimated radiancefield and object geometry while utilizing a deep neural network toestimate the object's appearance. The main advantage of this method isits ability to estimate the 3D shape, pose, and appearance of objectsfrom a single image without requiring multiple images or priorknowledge.</p><p>​ Specifically, the method first uses an initial radiance field togenerate a set of virtual images, which are then compared to theoriginal image to compute an error function. Next, the method updatesthe estimated radiance field and object geometry using the errorfunction and employs a deep neural network to estimate the object'sappearance. By iteratively optimizing the estimated radiance field andobject geometry while utilizing a deep neural network to estimate theobject's appearance, the method achieves the goal of estimating the 3Dshape, pose, and appearance of objects from a single image.</p><p>​ The method is evaluated on multiple datasets, demonstrating itseffectiveness in estimating the 3D shape, pose, and appearance ofobjects from a single image. Additionally, data augmentation techniquesand loss functions are used to improve the accuracy of theestimations.</p><p>​ The paper’s code has not run successfully, and I will debug the codein the next week.</p>]]></content>
    
    
    <categories>
      
      <category>工作总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基础知识</title>
    <link href="/posts/14876/"/>
    <url>/posts/14876/</url>
    
    <content type="html"><![CDATA[<h4 id="基本方法">基本方法</h4><ol type="1"><li>np(torch).stack([], axis=)函数</li></ol><p>作用：是将一组数组沿着新的轴堆叠起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], <br>              [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br><br>b = np.array([[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>],<br>              [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>]])<br><br>c = np.stack([a, b], <span class="hljs-number">0</span>) <span class="hljs-comment"># axis = 0</span><br><br><span class="hljs-comment"># 按照原数组直接堆叠</span><br><span class="hljs-comment"># array([[[ 1,  2,  3],</span><br><span class="hljs-comment">#         [ 4,  5,  6]],</span><br><br><span class="hljs-comment">#         [[ 7,  8,  9],</span><br><span class="hljs-comment">#         [10, 11, 12]]])</span><br><br>c = np.stack([a, b], <span class="hljs-number">1</span>) <span class="hljs-comment"># axis = 1</span><br><br><span class="hljs-comment"># 根据列表中数组的行堆叠</span><br><span class="hljs-comment"># array([[[ 1,  2,  3],</span><br><span class="hljs-comment">#         [ 7,  8,  9]],</span><br><br><span class="hljs-comment">#        [[ 4,  5,  6],</span><br><span class="hljs-comment">#         [10, 11, 12]]])</span><br><br>c = np.stack([a, b], <span class="hljs-number">2</span>) <span class="hljs-comment"># axis = 2</span><br><br><span class="hljs-comment"># 根据列表中数组的列d</span><br><span class="hljs-comment"># array([[[ 1,  7],</span><br><span class="hljs-comment">#         [ 2,  8],</span><br><span class="hljs-comment">#         [ 3,  9]],</span><br><br><span class="hljs-comment">#        [[ 4, 10],</span><br><span class="hljs-comment">#         [ 5, 11],</span><br><span class="hljs-comment">#         [ 6, 12]]])</span><br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>np.meshgrid()</li></ol><p><code>np.meshgrid</code>是一个函数，用于生成多维网格矩阵。它接受多个一维张量作为输入，并返回与输入张量数相同的输出张量列表，每个输出张量包含相应维度中输入张量值的重复。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 二维</span><br>x = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) <span class="hljs-comment"># 3</span><br>y = torch.tensor([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<span class="hljs-comment"># 2</span><br><br>X, Y = np.meshgrid(x, y)<br><span class="hljs-built_in">print</span>(X)<br><span class="hljs-comment"># tensor([[1, 2, 3],</span><br><span class="hljs-comment">#         [1, 2, 3]])</span><br><br><span class="hljs-built_in">print</span>(Y)<br><span class="hljs-comment"># tensor([[4, 4, 4],</span><br><span class="hljs-comment">#         [5, 5, 5]])</span><br><br><span class="hljs-comment"># 三维</span><br>z = torch.tensor([<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>])<br><br>X,Y,Z = np.meshgrid(x,y,z)<br><br><span class="hljs-built_in">print</span>(X.shape, Y.shape, Z.shape)<br><span class="hljs-comment"># torch.Size([2, 3, 3]) torch.Size([2, 3, 3]) torch.Size([2, 3, 3])</span><br><br><span class="hljs-built_in">print</span>(X)  <span class="hljs-comment"># 2*3*3</span><br><span class="hljs-comment"># tensor([[[1, 2, 3],</span><br><span class="hljs-comment">#          [1, 2, 3],</span><br><span class="hljs-comment">#          [1, 2, 3]],</span><br><br><span class="hljs-comment">#         [[1, 2, 3],</span><br><span class="hljs-comment">#          [1, 2, 3],</span><br><span class="hljs-comment">#          [1, 2, 3]]])</span><br><br><span class="hljs-built_in">print</span>(Y)<br><span class="hljs-comment"># tensor([[[4, 4, 4],</span><br><span class="hljs-comment">#          [5, 5, 5],</span><br><span class="hljs-comment">#          [6, 6, 6]],</span><br><br><span class="hljs-comment">#         [[4, 4, 4],</span><br><span class="hljs-comment">#          [5, 5, 5],</span><br><span class="hljs-comment">#          [6, 6, 6]]])</span><br><br><span class="hljs-built_in">print</span>(Z)<br><span class="hljs-comment"># tensor([[[6, 6, 6],</span><br><span class="hljs-comment">#          [6, 6, 6],</span><br><span class="hljs-comment">#          [6, 6, 6]],</span><br><br><span class="hljs-comment">#         [[7, 7, 7],</span><br><span class="hljs-comment">#          [7, 7, 7],</span><br><span class="hljs-comment">#          [7, 7, 7]]])</span><br><br><br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>torch.meshgrid()</li></ol><p><code>torch.meshgrid</code>是一个函数，用于生成多维网格矩阵。它接受多个一维张量作为输入，并返回与输入张量数相同的输出张量列表，每个输出张量包含相应维度中输入张量值的重复。torch和numpy的作用是一样的，但是其x,y的坐标有所区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 二维</span><br>x = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) <span class="hljs-comment"># 3</span><br>y = torch.tensor([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<span class="hljs-comment"># 2</span><br><br>X, Y = torch.meshgrid(x, y)<br><br><span class="hljs-built_in">print</span>(X) <span class="hljs-comment"># i j </span><br><br><span class="hljs-comment"># tensor([[1, 1],</span><br><span class="hljs-comment">#         [2, 2],</span><br><span class="hljs-comment">#         [3, 3]])</span><br><br><span class="hljs-built_in">print</span>(Y)<br><span class="hljs-comment"># tensor([[4, 5],</span><br><span class="hljs-comment">#         [4, 5],</span><br><span class="hljs-comment">#         [4, 5]])</span><br><br><span class="hljs-comment">#三维</span><br>z = torch.tensor([<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>]) <span class="hljs-comment"># 3   </span><br><br>X,Y,Z = torch.meshgrid(x,y,z)<br><br><span class="hljs-built_in">print</span>(X)<span class="hljs-comment"># 3*2*3</span><br><span class="hljs-comment"># tensor([[[1, 1, 1],</span><br><span class="hljs-comment">#          [1, 1, 1]],</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment">#         [[2, 2, 2],</span><br><span class="hljs-comment">#          [2, 2, 2]],</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment">#         [[3, 3, 3],</span><br><span class="hljs-comment">#          [3, 3, 3]]])</span><br><br><span class="hljs-built_in">print</span>(Y)<br><span class="hljs-comment"># tensor([[[4, 4, 4],</span><br><span class="hljs-comment">#          [5, 5, 5]],</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment">#         [[4, 4, 4],</span><br><span class="hljs-comment">#          [5, 5, 5]],</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment">#         [[4, 4, 4],</span><br><span class="hljs-comment">#          [5, 5, 5]]])</span><br><br><span class="hljs-built_in">print</span>(Z)<br><span class="hljs-comment"># tensor([[[6, 7, 8],</span><br><span class="hljs-comment">#          [6, 7, 8]],</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment">#         [[6, 7, 8],</span><br><span class="hljs-comment">#          [6, 7, 8]],</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment">#         [[6, 7, 8],</span><br><span class="hljs-comment">#          [6, 7, 8]]])</span><br></code></pre></td></tr></table></figure><ol start="4" type="1"><li>torch.norm()</li></ol><p><code>torch.norm()</code>函数用于计算张量的范数。它可以计算给定维度上的向量范数或矩阵范数，并返回一个张量。默认为2范数，可以设置p指定范数。同时可以设置<code>keepdim</code>来决定是否保留原先的维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># Example 1: Compute the L2-norm of a vector</span><br>x = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=torch.float32)<br>l2_norm = torch.norm(x, p=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(l2_norm)   <span class="hljs-comment"># Output: tensor(3.7417)</span><br><br><span class="hljs-comment"># Example 2: Compute the Frobenius norm of a matrix</span><br>A = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]], dtype=torch.float32)<br>fro_norm = torch.norm(A, p=<span class="hljs-string">&#x27;fro&#x27;</span>)<br><span class="hljs-built_in">print</span>(fro_norm)  <span class="hljs-comment"># Output: tensor(5.4772)</span><br><br><span class="hljs-comment"># Example 3: Compute the L1-norm of a vector along a specific dimension</span><br>B = torch.tensor([[<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [-<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, -<span class="hljs-number">6</span>]], dtype=torch.float32)<br>l1_norm = torch.norm(B, p=<span class="hljs-number">1</span>, dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(l1_norm)   <span class="hljs-comment"># Output: tensor([6., 15.])</span><br><br></code></pre></td></tr></table></figure><ol start="5" type="1"><li>torch.gather()</li></ol><p>作用：沿着由dim指定的轴收集数值，作为取值的索引。</p><p><code>torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor</code></p><blockquote><p>input (Tensor) – 目标变量，输入 dim (int) – 需要沿着取值的坐标轴index (LongTensor) – 需要取值的索引矩阵 sparse_grad (bool,optional) –如果为真，输入将是一个稀疏张量 out (Tensor, optional) – 输出</p><p>index矩阵作为当前轴的索引，剩下的轴依旧按照顺序排序[0-n]</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># dim = 0</span><br><br><span class="hljs-built_in">input</span> = [<br>    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>],<br>    [<span class="hljs-number">1.0</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.2</span>, <span class="hljs-number">1.3</span>],<br>    [<span class="hljs-number">2.0</span>, <span class="hljs-number">2.1</span>, <span class="hljs-number">2.2</span>, <span class="hljs-number">2.3</span>]<br>]<span class="hljs-comment">#shape [3,4]</span><br><span class="hljs-built_in">input</span> = torch.tensor(<span class="hljs-built_in">input</span>)<br>length = torch.LongTensor([<br>    [<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>],<br>    [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],<br>    [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]<br>])<span class="hljs-comment">#[4,4]</span><br>out = torch.gather(<span class="hljs-built_in">input</span>, dim=<span class="hljs-number">0</span>, index=length)<br><span class="hljs-built_in">print</span>(out)<br><br><span class="hljs-comment"># 结果</span><br><span class="hljs-comment"># tensor([[2.0000, 2.1000, 2.2000, 2.3000],</span><br><span class="hljs-comment">#         [1.0000, 1.1000, 1.2000, 1.3000],</span><br><span class="hljs-comment">#         [0.0000, 0.1000, 0.2000, 0.3000],</span><br><span class="hljs-comment">#         [0.0000, 1.1000, 2.2000, 0.3000]])</span><br><br></code></pre></td></tr></table></figure><p><span class="math display">\[取值索引矩阵=\left[\begin{array}{llll}X_{20} &amp; X_{21} &amp; X_{22} &amp; X_{23} \\X_{10} &amp; X_{11} &amp; X_{12} &amp; X_{13} \\X_{00} &amp; X_{01} &amp; X_{02} &amp; X_{03} \\X_{00} &amp; X_{11} &amp; X_{22} &amp; X_{03}\end{array}\right]\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># dim = 1</span><br><br><span class="hljs-built_in">input</span> = [<br>    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>],<br>    [<span class="hljs-number">1.0</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.2</span>, <span class="hljs-number">1.3</span>],<br>    [<span class="hljs-number">2.0</span>, <span class="hljs-number">2.1</span>, <span class="hljs-number">2.2</span>, <span class="hljs-number">2.3</span>]<br>]<span class="hljs-comment">#shape [3,4]</span><br><span class="hljs-built_in">input</span> = torch.tensor(<span class="hljs-built_in">input</span>)<br>length = torch.LongTensor([<br>    [<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>],<br>    [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]<br>])<span class="hljs-comment">#[3,4]</span><br>out = torch.gather(<span class="hljs-built_in">input</span>, dim=<span class="hljs-number">1</span>, index=length)<br><span class="hljs-built_in">print</span>(out)<br><br><br><span class="hljs-comment"># 结果</span><br><span class="hljs-comment"># tensor([[0.2000, 0.2000, 0.2000, 0.2000],</span><br><span class="hljs-comment">#         [1.1000, 1.1000, 1.1000, 1.1000],</span><br><span class="hljs-comment">#         [2.0000, 2.1000, 2.2000, 2.0000]])</span><br><br></code></pre></td></tr></table></figure><p><span class="math display">\[取值索引矩阵 =\left[\begin{array}{llll}\mathrm{X}_{02} &amp; \mathrm{X}_{02} &amp; \mathrm{X}_{02} &amp;\mathrm{X}_{02} \\\mathrm{X}_{11} &amp; \mathrm{X}_{11} &amp; \mathrm{X}_{11} &amp;\mathrm{X}_{11} \\\mathrm{X}_{20} &amp; \mathrm{X}_{21} &amp; \mathrm{X}_{22} &amp;\mathrm{X}_{20}\end{array}\right]\]</span></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>每周总结(23.05.15-23.05.19)</title>
    <link href="/posts/20521/"/>
    <url>/posts/20521/</url>
    
    <content type="html"><![CDATA[<div align = "center"><H3>工作总结</div><p><strong>日期：2023.05.15-2023.05.19</strong></p><h4 id="paper">1.Paper</h4><p><strong>Title：</strong><code>NeRF++: Analyzing and Improving Neural Radiance Fields.</code></p><p><strong>总结：</strong>这篇论文主要针对神经辐射场(NeRF_方法进行了分析和改进。NeRF是一种用于3D重建和渲染的深度学习模型，它通过建立场景中每个点的颜色和密度函数来表示3D信息，并可以生成极高质量的逼真图像。但该方法在训练和测试时存在一些问题，如计算时间和内存开销大、不稳定性等。</p><blockquote><p>为此，NeRF++通过引入多项新技术对NeRF进行改进，包括：</p><ol type="1"><li>首先，文章提出了一个自适应采样策略，使得更多的采样点可以在物体表面上，从而减少了无效样本，提高了模型的精度。</li><li>其次，文章通过增加正则化项和提高网络深度，在保持模型性能的同时，显著减少了内存和计算时间的消耗。</li><li>另外，文章还提出了一种新的光源采样方法，可以在较短的时间内对复杂光照情况下的场景进行快速且准确的渲染。</li><li>最后，文章还提出了一种自动曝光控制（auto-exposurecontrol）方法，可以帮助模型更好地适应不同的场景光照情况。</li></ol><p>NeRF++的实验结果表明，该方法在精度和效率方面都有所提高，可以很好地解决NeRF方法存在的问题。这些改进使得NeRF++方法成为一个更加稳定、可扩展、适用于复杂场景的深度学习3D重建和渲染工具，具有广泛的应用前景。#### 2.Code</p></blockquote><p>继续研究学习NeRF-Pytorch代码，对其进行逐一调试分析，了解NeRF的具体运作原理与其使用的一些方法，进度：3/4。搭建NeRF++环境，并成功运行其代码，得到结果。</p><h4 id="other">3.Other</h4><ol type="1"><li><p>通过对代码的调试，学习到了一些新的知识，同时也对论文中提出的一些方法有了更加深刻的了解。</p><ul><li><p>位置编码</p><p>NeRF在将位置信息输入到MLP中进行预测前，对位置信息进行了位置编码，从而解决对图像中高频信息预测不准确，得到了结果模糊的问题。</p><p>NeRF通过一系列的[sin,cos]函数编码位置信息，将位置信息从3维增加到60维，如何再作为MLP网络的输入。</p></li><li><p>MLP网络</p><p>NeRF有一个8层的MLP网络，第一层的输入维度为60，输出为256，<code>第五层为跨越连接层</code>，其输入的维度额外增加60，输出仍然保持256，该方法可以增强模型的表示能力和泛化性能，并减少训练时间和复杂度，这8层线性连接层的激活函数都为ReLU。再经历8层连接层后，额外增加一层256特征的线性层，其输出维度为128，用来输出点的密度density，同时这一层加入方向坐标维度用来输出RGB信息。</p></li></ul></li><li><p>学习了一些YOLO目标检测的知识。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>工作总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>双指针</title>
    <link href="/posts/58208/"/>
    <url>/posts/58208/</url>
    
    <content type="html"><![CDATA[<h4 id="力扣27-移除元素">力扣27 <ahref="https://leetcode.cn/problems/remove-element/description/">移除元素</a></h4><p>给你一个数组<code>nums</code>和一个值<code>val</code>，你需要原地移除所有数值等于<code>val</code>的元素，并返回移除后数组的新长度。不要使用额外的数组空间，你必须仅使用<code>O(1)</code>额外空间并原地修改输入数组。元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。</p><p><strong>示例：</strong></p><p>输入：nums = [3,2,2,3], val = 3 输出：2, nums = [2,2]解释：函数应该返回新的长度 2, 并且 nums 中的前两个元素均为2。你不需要考虑数组中超出新长度后面的元素。例如，函数返回的新长度为 2，而 nums = [2,2,3,3] 或 nums = [2,2,0,0]，也会被视作正确答案。</p><blockquote><p><strong>思路：</strong>对数组的元素进行操作，可以使用双指针方法，设定一个快指针和一个慢指针。此题因为只需要变换后数组的长度，因此可以采用两种不同的双指针方法。</p><ol type="1"><li><p>如果快指针索引的元素不等于val，则将这个元素赋值给慢指针索引的元素；如果等于val，则快指针指向下一个索引，并且不赋值给慢指针索引的元素。</p></li><li><p>将数组分为有效部分和无效部分，即前后两段</p><ul><li>前半段是有效部分，存储的是不等于<code>val</code>的元素。</li><li>后半段是无效部分，存储的是等于<code>val</code>的元素。</li></ul><p>最终返回有效部分的下标(数组元素顺序发生改变)</p></li></ol></blockquote><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeElement</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], val: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        slow = <span class="hljs-number">0</span>                        <span class="hljs-comment">#慢指针</span><br>        <span class="hljs-keyword">for</span> fast <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):   <span class="hljs-comment">#快指针</span><br>            <span class="hljs-keyword">if</span>(val != nums[fast]):      <span class="hljs-comment">#判断元素</span><br>                nums[slow] = nums[fast] <br>                slow += <span class="hljs-number">1</span>               <span class="hljs-comment">#慢指针移动</span><br>        <span class="hljs-keyword">return</span> slow <br></code></pre></td></tr></table></figure><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeElement</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], val: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        end = <span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span><br>        start = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> start &lt;= end:                 <span class="hljs-comment">#双指针</span><br>            <span class="hljs-keyword">if</span> nums[start] == val:<br>                nums[start], nums[end] = nums[end], nums[start]     <span class="hljs-comment">#交换元素(前面为有效元素，后面为无效元素)</span><br>                start -= <span class="hljs-number">1</span>                  <span class="hljs-comment">#前指针不动</span><br>                end -= <span class="hljs-number">1</span>                    <span class="hljs-comment">#后指针移动</span><br>            start += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> start<br></code></pre></td></tr></table></figure><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">removeElement</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums, <span class="hljs-type">int</span> val)</span> </span>&#123;<br>        <span class="hljs-type">int</span> idx = <span class="hljs-number">0</span>;                            <span class="hljs-comment">//前指针</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> x : nums)                      <span class="hljs-comment">//后指针</span><br>            <span class="hljs-keyword">if</span>(x != val)<br>                nums[idx++] = x;                <span class="hljs-comment">//元素不等于val时，将元素赋值到前指针位置</span><br>        <span class="hljs-keyword">return</span> idx;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">removeElement</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums, <span class="hljs-type">int</span> val)</span> </span>&#123;<br>        <span class="hljs-type">int</span> end = nums.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>;                      <span class="hljs-comment">//后指针</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> start = <span class="hljs-number">0</span>; start &lt;= end; start++) &#123;    <span class="hljs-comment">//前指针</span><br>            <span class="hljs-keyword">if</span> (nums[start] == val) &#123;<br>                <span class="hljs-built_in">swap</span>(nums[start--], nums[end--]);       <span class="hljs-comment">//交换元素(前面为有效元素，后面为无效元素)</span><br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> end + <span class="hljs-number">1</span>;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h4 id="力扣80-删除有序数组中的重复项-ii">力扣80 <ahref="https://leetcode.cn/problems/remove-duplicates-from-sorted-array-ii/description/">删除有序数组中的重复项II</a></h4><p>给你一个有序数组<code>nums</code>，请你<strong>原地</strong>删除重复出现的元素，使得出现次数超过两次的元素<strong>只出现两次</strong>，返回删除后数组的新长度。</p><p>不要使用额外的数组空间，你必须在 <strong>原地修改输入数组</strong>并在使用O(1)额外空间的条件下完成。</p><p><strong>示例 1：</strong></p><p>输入：nums = [1,1,1,2,2,3] 输出：5, nums = [1,1,2,2,3]解释：函数应返回新长度 length = 5, 并且原数组的前五个元素被修改为 1, 1,2, 2, 3 。 不需要考虑数组中超出新长度后面的元素。</p><blockquote><p><strong>思路：</strong>本题为<ahref="https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array/">26.删除有序数组中的重复项</a>的进阶，由这两个题目可以总结出一个规律。对于原地修改的有序数组来说，使用双指针。</p><ul><li><strong>慢指针slow</strong>:指向当前即将放置元素的位置；则<code>slow - 2</code>是刚才已经放置了元素的位置。</li><li><strong>快指针fast</strong>: 向后遍历所有元素；</li></ul><p>判断条件：因为数组中的元素最多出现2次，因此定义<code>slow &lt; 2</code>，让其前两个元素均通过，然后再加上一个判断条件<code>nums[fast] != nums[slow-2]</code>，即相同的元素超过两个之后，慢指针<code>slow</code>不再前进，快指针<code>fast</code>继续前进，直到满足判断条件，<code>slow</code>指针位置被覆盖，同时继续前进。</p><p>PS:对于<code>26题</code>，则直接把判断条件改为<code>1</code>即可。</p></blockquote><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeDuplicates</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        slow = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> fast <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-keyword">if</span> slow &lt; <span class="hljs-number">2</span> <span class="hljs-keyword">or</span> nums[fast] != nums[slow-<span class="hljs-number">2</span>]: <span class="hljs-comment"># 判断条件，是否覆盖，同时慢指针是否前进</span><br>                nums[slow] = nums[fast]   <span class="hljs-comment"># 元素覆盖</span><br>                slow += <span class="hljs-number">1</span>   <span class="hljs-comment"># 慢指针前进</span><br>        <span class="hljs-keyword">return</span> slow<br>    <br>    <span class="hljs-comment"># or </span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeDuplicates</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        slow = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> nums:<br>            <span class="hljs-keyword">if</span> slow &lt; <span class="hljs-number">2</span> <span class="hljs-keyword">or</span> nums[slow-<span class="hljs-number">2</span>]  != x: <span class="hljs-comment"># 判断条件，是否覆盖，同时慢指针是否前进</span><br>                nums[slow] = x   <span class="hljs-comment"># 元素覆盖</span><br>                slow += <span class="hljs-number">1</span>   <span class="hljs-comment"># 慢指针前进</span><br>        <span class="hljs-keyword">return</span> slow<br></code></pre></td></tr></table></figure><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">removeDuplicates</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums)</span> </span>&#123;<br>        <span class="hljs-type">int</span> slow = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> num : nums)&#123;<br>            <span class="hljs-keyword">if</span>(slow &lt; <span class="hljs-number">2</span> || nums[slow<span class="hljs-number">-2</span>] != num) <span class="hljs-comment">// 判断条件，是否覆盖，同时慢指针是否前进</span><br>                nums[slow++] = num;<span class="hljs-comment">// 元素覆盖，慢指针前进</span><br>        &#125;<br>        <span class="hljs-keyword">return</span> slow;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h4 id="总结">总结</h4><p>双指针法（快慢指针法）：<strong>通过一个快指针和慢指针在一个for循环下完成两个for循环的工作。</strong></p><p>定义快慢指针</p><ul><li>快指针：寻找新数组的元素 ，新数组就是不含有目标元素的数组</li><li>慢指针：指向更新 新数组下标的位置</li></ul><p>可以删除有序数组的重复元素以及去除指定的元素。对于双指针，一定得先弄清楚判断条件，什么时候慢指针移动，什么时候不应该移动！</p>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>刷题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>每周总结(23.05.08-23.05.12)</title>
    <link href="/posts/48714/"/>
    <url>/posts/48714/</url>
    
    <content type="html"><![CDATA[<div align = "center"><H3>工作总结</div><p><strong>日期：2023.05.08-2023.05.12</strong></p><h4 id="paper">1.Paper</h4><p><strong>Title：</strong><code>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</code></p><p><strong>总结：</strong>NeRF是一种基于神经网络的视图综合方法，可以从有限数量的输入视图合成高质量、高分辨率的新视图。</p><blockquote><ul><li>采集输入数据：从不同角度拍摄一组图片，同时记录相机参数和深度值信息。</li><li>输入：5D坐标(3D位置坐标+2D方向坐标)</li><li>输出：通过MLP多层感知机转换成对应的($RGB$)</li><li>预测新视角：对于给定的任意视角，计算该视角与每个空间位置的交点，并利用预先训练好的神经辐射场计算颜色和透明度值。最后，基于光线投射算法，将这些颜色值合成为一张新的图像。</li></ul></blockquote><h4 id="code">2.Code</h4><p>学习NeRF-pytorch代码,已经看了1/4的代码，了解了代码的输入，数据的转换以及坐标轴的转换。</p><h4 id="other">3.Other</h4><p>学习了视图变换与相机参数的一些基本知识。</p><p>了解了视图变换，包括</p><blockquote><ul><li>相机变换(camera/view transformation)</li><li>投影变换(projection transformation)</li><li>视口变换(viewport transformation)</li></ul></blockquote><ol type="1"><li>投影变换是NeRF中的重要变换，包括</li></ol><blockquote><ul><li>正交投影(orthographic projection)</li><li>透视投影(perspection projection)</li></ul></blockquote><p>它们的变换矩阵分别如下：</p><p><strong>透视投影矩阵：</strong> <span class="math display">\[M_{persp \to -ortho} = P=\left[\begin{array}{cccc}n &amp; 0 &amp; 0&amp; 0 \\0 &amp; n &amp; 0 &amp; 0 \\0 &amp; 0 &amp; n+f &amp; -nf \\0&amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span> <strong>正交投影矩阵：</strong> <span class="math display">\[M_{orth}=\left[\begin{array}{cccc}\frac{2}{l-r} &amp; 0 &amp; 0 &amp; 0 \\0 &amp; \frac{2}{b-t}&amp; 0 &amp; 0 \\0 &amp; 0 &amp; \frac{2}{n-f} &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{cccc}1 &amp; 0 &amp; 0 &amp; -\frac{r+l}{2} \\0 &amp; 1 &amp; 0 &amp; -\frac{t+b}{2} \\0 &amp; 0 &amp; 1 &amp; -\frac{n+f}{2} \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]=\left[\begin{array}{cccc}\frac{2}{l-r} &amp; 0 &amp; 0 &amp; -\frac{r+l}{2} \\0 &amp; \frac{2}{b-t}&amp; 0 &amp; -\frac{t+b}{2} \\0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{2} \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span></p><ol start="2" type="1"><li><p>相机参数</p><p><strong>相机外参T:</strong> <span class="math display">\[T=\left[\begin{array}{cc}R &amp; t \\0^T &amp; 1\\\end{array}\right]\]</span></p><p><strong>相机内参K:</strong> <span class="math display">\[K = \left|\begin{array}{ccc}    f_x &amp; 0 &amp; u_0 \\    0 &amp; f_y &amp; v_0 \\    0 &amp; 0  &amp; 1    \end{array}\right|\]</span></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>工作总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>螺旋矩阵</title>
    <link href="/posts/52933/"/>
    <url>/posts/52933/</url>
    
    <content type="html"><![CDATA[<h4 id="力扣59-螺旋矩阵-ii">力扣59 <ahref="https://leetcode.cn/problems/spiral-matrix-ii/description/">螺旋矩阵II</a></h4><p>给你一个正整数<code>n</code>，生成一个包含<code>1</code>到<spanclass="math inline">\(n^2\)</span>所有元素，且元素按顺时针顺序螺旋排列的<code>n x n</code>正方形矩阵<code>matrix</code>。</p><p><strong>示例：</strong></p><p>输入：n = 3 输出：[[1,2,3],[8,9,4],[7,6,5]]</p><figure><img src="1.jpg?100×100" alt="示例" /><figcaption aria-hidden="true">示例</figcaption></figure><blockquote><p><strong>思路</strong>：该题不需要特别的算法，主要是靠模拟，设定边界。</p><p>​生成一个<code>n×n</code>的矩阵，然后模拟顺时针向内环绕的过程(边界很重要)，模拟过程包括从左到右，从上到下，从右到左，从下到上。分别设定上下左右边界<code>t,b,l,r</code>。每行或每列填充完之后，进行对应的行或列的收缩。当填充的数字满足<spanclass="math inline">\(n^2\)</span>时，结束循环。</p></blockquote><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generateMatrix</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        ans = [[<span class="hljs-number">0</span>]*n <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)] <span class="hljs-comment"># 生成n*n的二维数组</span><br>        l = t = <span class="hljs-number">0</span><br>        r = b = n-<span class="hljs-number">1</span> <br>        num = <span class="hljs-number">1</span><br>        nums = n*n<br>        <span class="hljs-keyword">while</span> num &lt;= nums:          <span class="hljs-comment"># 从外圈到内圈，每次循环一圈</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(l, r+<span class="hljs-number">1</span>): <span class="hljs-comment"># 从左到右</span><br>                ans[t][i] = num<br>                num += <span class="hljs-number">1</span><br>            t += <span class="hljs-number">1</span>                  <span class="hljs-comment"># 上边界下移</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t, b+<span class="hljs-number">1</span>): <span class="hljs-comment"># 从上到下</span><br>                ans[i][r] = num<br>                num += <span class="hljs-number">1</span><br>            r -= <span class="hljs-number">1</span>                  <span class="hljs-comment"># 右边界左移</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(r, l-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>): <span class="hljs-comment"># 从右到左</span><br>                ans[b][i] = num<br>                num += <span class="hljs-number">1</span><br>            b -= <span class="hljs-number">1</span>                  <span class="hljs-comment"># 下边界上移</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(b, t-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>): <span class="hljs-comment"># 从下到上</span><br>                ans[i][l] = num<br>                num += <span class="hljs-number">1</span><br>            l += <span class="hljs-number">1</span>                  <span class="hljs-comment"># 左边界右移</span><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generateMatrix</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        nums = [[<span class="hljs-number">0</span>] * n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]<br>        startx, starty = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>               <span class="hljs-comment"># 起始点</span><br>        loop, mid = n // <span class="hljs-number">2</span>, n // <span class="hljs-number">2</span>          <span class="hljs-comment"># 迭代次数、n为奇数时，矩阵的中心点</span><br>        count = <span class="hljs-number">1</span>                           <span class="hljs-comment"># 计数</span><br><br>        <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, loop + <span class="hljs-number">1</span>) :      <span class="hljs-comment"># 每循环一层偏移量加1，偏移量从1开始</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(starty, n - offset) :    <span class="hljs-comment"># 从左至右，左闭右开</span><br>                nums[startx][i] = count<br>                count += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(startx, n - offset) :    <span class="hljs-comment"># 从上至下</span><br>                nums[i][n - offset] = count<br>                count += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n - offset, starty, -<span class="hljs-number">1</span>) : <span class="hljs-comment"># 从右至左</span><br>                nums[n - offset][i] = count<br>                count += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n - offset, startx, -<span class="hljs-number">1</span>) : <span class="hljs-comment"># 从下至上</span><br>                nums[i][starty] = count<br>                count += <span class="hljs-number">1</span>                <br>            startx += <span class="hljs-number">1</span>         <span class="hljs-comment"># 更新起始点</span><br>            starty += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> n % <span class="hljs-number">2</span> != <span class="hljs-number">0</span> :<span class="hljs-comment"># n为奇数时，填充中心点</span><br>            nums[mid][mid] = count <br>        <span class="hljs-keyword">return</span> nums<br></code></pre></td></tr></table></figure><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; <span class="hljs-built_in">generateMatrix</span>(<span class="hljs-type">int</span> n) &#123;<br>        vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; <span class="hljs-built_in">ans</span>(n, <span class="hljs-built_in">vector</span>&lt;<span class="hljs-type">int</span>&gt;(n)); <span class="hljs-comment">//创建一个n*n的二维数组</span><br>        <span class="hljs-type">int</span> l, t = <span class="hljs-number">0</span>;<br>        <span class="hljs-type">int</span> r, b = n<span class="hljs-number">-1</span>;<br>        <span class="hljs-type">int</span> cnt = <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">while</span>(cnt &lt;= n*n)&#123;                      <span class="hljs-comment">//cnt从1开始，每次循环+1，直到n*n</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=l; i&lt;=r; i++)&#123;            <span class="hljs-comment">//从左到右</span><br>                ans[t][i] = cnt;<br>                cnt++;<br>            &#125;   <br>            t++;                                 <span class="hljs-comment">//上边界+1</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=t; i&lt;=b; i++)&#123;            <span class="hljs-comment">//从上到下</span><br>                ans[i][r] = cnt;<br>                cnt++;<br>            &#125;<br>            r--;                                <span class="hljs-comment">//右边界-1</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=r; i&gt;=l; i--)&#123;            <span class="hljs-comment">//从右到左</span><br>                ans[b][i] = cnt;<br>                cnt++;<br>            &#125;<br>            b--;                                <span class="hljs-comment">//下边界-1</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=b; i&gt;=t; i--)&#123;            <span class="hljs-comment">//从下到上</span><br>                ans[i][l] = cnt;<br>                cnt++;<br>            &#125;<br>            l++;                                <span class="hljs-comment">//左边界+1</span><br>        &#125;<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><p>剑指 Offer 29. 顺时针打印矩阵 <ahref="https://leetcode.cn/problems/shun-shi-zhen-da-yin-ju-zhen-lcof/description/">剑指Offer 29. 顺时针打印矩阵</a></p><p>输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字。</p><p><strong>示例：</strong></p><p>输入：matrix = [[1,2,3],[4,5,6],[7,8,9]]输出：[1,2,3,6,9,8,7,4,5]</p><p><strong>限制：</strong></p><ul><li><code>0 &lt;= matrix.length &lt;= 100</code></li><li><code>0 &lt;= matrix[i].length &lt;= 100</code></li></ul><blockquote><p><strong>思路</strong>：该题不需要特别的算法，主要是靠模拟，设定边界。</p><p>模拟顺时针向内环绕的过程(边界很重要)，模拟过程包括从左到右，从上到下，从右到左，从下到上。分别设定上下左右边界<code>t,b,l,r</code>。每行或每列读取完之后，进行对应的行或列的收缩。因为该题中的矩阵并不一定是正方形矩阵，因此判断结束的条件应该为边界是否重合。</p></blockquote><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">spiralOrder</span>(<span class="hljs-params">self, matrix: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> matrix: <span class="hljs-keyword">return</span> []                <span class="hljs-comment">#判断是否为空  </span><br>        l, r, t, b = <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(matrix[<span class="hljs-number">0</span>]) - <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(matrix) - <span class="hljs-number">1</span>  <span class="hljs-comment"># 列， 行</span><br>        result = []<br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(l, r + <span class="hljs-number">1</span>):           <span class="hljs-comment">#从左到右</span><br>                result.append(matrix[t][i])<br>            t +=  <span class="hljs-number">1</span>                             <span class="hljs-comment">#上边界下移</span><br>            <span class="hljs-keyword">if</span> t &gt; b:                           <span class="hljs-comment">#判断是否越界</span><br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t, b + <span class="hljs-number">1</span>):           <span class="hljs-comment">#从上到下   </span><br>                result.append(matrix[i][r])<br>            r -= <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> l &gt; r: <br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(r, l - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):       <span class="hljs-comment">#从右到左</span><br>                result.append(matrix[b][i])<br>            b -= <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> t &gt; b: <br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(b, t - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):       <span class="hljs-comment">#从下到上</span><br>                result.append(matrix[i][l])<br>            l += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> l &gt; r: <br>                <span class="hljs-keyword">break</span><br>        <span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">spiralOrder</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; matrix)</span> </span>&#123;<br>        vector&lt;<span class="hljs-type">int</span>&gt; ans;        <span class="hljs-comment">// 存放结果     </span><br>        <span class="hljs-keyword">if</span>(matrix.<span class="hljs-built_in">empty</span>())&#123;     <span class="hljs-comment">// 矩阵为空</span><br>            <span class="hljs-keyword">return</span> ans;<br>        &#125;<br>        <span class="hljs-type">int</span> l = <span class="hljs-number">0</span>, t = <span class="hljs-number">0</span>;<br>        <span class="hljs-type">int</span> r = matrix[<span class="hljs-number">0</span>].<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span> ; <span class="hljs-comment">// 列</span><br>        <span class="hljs-type">int</span> b = matrix.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>;    <span class="hljs-comment">// 行</span><br>        <span class="hljs-keyword">while</span>(<span class="hljs-literal">true</span>)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=l; i&lt;=r; i++) ans.<span class="hljs-built_in">push_back</span>(matrix[t][i]); <span class="hljs-comment">// 从左到右</span><br>            t++;                                                 <span class="hljs-comment">// 上边界下移</span><br>            <span class="hljs-keyword">if</span>(t&gt;b) <span class="hljs-keyword">break</span>;                                       <span class="hljs-comment">// 上边界大于下边界，退出</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=t; i&lt;=b; i++) ans.<span class="hljs-built_in">push_back</span>(matrix[i][r]); <span class="hljs-comment">// 从上到下</span><br>            r--;<br>            <span class="hljs-keyword">if</span>(l&gt;r) <span class="hljs-keyword">break</span>;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=r; i&gt;=l; i--) ans.<span class="hljs-built_in">push_back</span>(matrix[b][i]); <span class="hljs-comment">// 从右到左</span><br>            b--;<br>            <span class="hljs-keyword">if</span>(t&gt;b) <span class="hljs-keyword">break</span>;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=b; i&gt;=t; i--) ans.<span class="hljs-built_in">push_back</span>(matrix[i][l]); <span class="hljs-comment">// 从下到上</span><br>            l++;<br>            <span class="hljs-keyword">if</span>(l&gt;r) <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h4 id="螺旋矩阵总结">螺旋矩阵总结</h4><p>这种题目并不涉及到什么算法，就是模拟过程，但却十分考察对代码的掌控能力。一定要注意边界的问题，统一循环的变量。</p><p>模拟顺时针画矩阵的过程:</p><ul><li>从左到右</li><li>从上到下</li><li>从右到左</li><li>从下到上</li></ul><p>由外向内一圈一圈这么画下去。同时需要注意限制，需要判断矩阵是正方形，还是不一定是正方形，来给出结束的条件。</p>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>刷题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NeRF源码解读</title>
    <link href="/posts/15494/"/>
    <url>/posts/15494/</url>
    
    <content type="html"><![CDATA[<p>imgs : 根据 .json 文件加载到的所有图像数据。（N，H，W，4）N 代表用于train、test、val 的总数量(4:RGBβ) poses : 转置矩阵。（N，4，4）render_poses : 用于测试的 pose 。（40，4，4） i_split : [[0:train],[train:val], [val:test]]</p><p><em>γ</em>(<em>p</em>)=(sin(20<em>πp</em>),cos(20<em>πp</em>),⋯,sin(2<em>L</em>−1<em>πp</em>),cos(2<em>L</em>−1<em>πp</em>))</p><h4 id="参数">参数</h4><p>images: (138, 400, 400, 4), 图像总数, 高宽分别为400, 400,4通道(RGBA)</p><p>poses: (138, 4, 4), 138个图像对应的相机位姿(4*4的c2w变换矩阵)</p><p>render_poses: (40, 4, 4), 40个用于测试的位姿(4*4的c2w变换矩阵)</p><p>hwf: (3,), 图像的高宽和焦距</p><p>i_split: train[0:100], val[0:13], tea-st[0:25]图像的索引</p><p>include_input: 编码结果是否包括原始坐标</p><p>input_dims: 输入数据的维度</p><p>max_freq_log2: 位置编码函数最大频率 L-1</p><p>num_freqs: 位置编码函数的频率数 L</p><p>log_sampling: 频率是否使用指数增长</p><p>periodic_fns: 编码函数[sin, cos]</p><p>embed_fns: 存储编码函数</p><p>out_dim: 存储编码后的总维度</p><p>embed_fn: 位置编码器</p><p>input_ch: 编码后的总维度(作为MLP输入时的维度)</p><p>use_viewdirs: 使用完整的5D坐标</p><p>embeddirs_fn: 存储方向坐标编码函数</p><p>input_ch_views: 编码后的总维度(方向坐标)</p><p>netdepth: 网络深度 8层</p><p>netwidth: 网络宽度 256 粗糙网络(coarse network)上取 64 点,精细网络(fine network)上取 64 + 128 = 192个点, 一共256个点</p><p>input_ch: 位置坐标输入维度x 63</p><p>output_ch: 输出维度 5 (RGB + density)</p><p>skips: 跳跃连接层 第五层 输入向量维度为319</p><p>input_ch_views: 视角方向输入维度d 27</p><p>use_viewdirs: 是否使用视角方向</p><p>pst_linears: 存储MLP的8个隐藏层(包含一个跨越层，319个特征输入)</p><p>views_linears:存储一个额外的线性层(用于输入方向坐标信息，输出密度density，输入256+27，输出128)</p><p>netdepth_fine: 精细网络深度 8</p><p>netwidth_fine: 精细网络特征 256</p><p>netchunk: 网络批量处理的数量 1024<em>64 rays </em> points</p><p>N_rand: 批量大小(每个梯度的随机射线数量)</p><p>N_samples: 每条射线的粗糙网络采样点数</p><p>N_importance: 每条射线额外的精细网络采样点数</p>]]></content>
    
    
    <categories>
      
      <category>NeRF</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NeRF</tag>
      
      <tag>源码解读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑动窗口</title>
    <link href="/posts/60655/"/>
    <url>/posts/60655/</url>
    
    <content type="html"><![CDATA[<h4 id="力扣209-长度最小的子数组">力扣209 <ahref="https://leetcode.cn/problems/minimum-size-subarray-sum/">长度最小的子数组</a></h4><p>给定一个含有<code>n</code>个正整数的数组和一个正整数<code>s</code>，找出该数组中满足其和<code>≥s</code>的长度最小的连续子数组，并返回其长度。如果不存在符合条件的子数组，返回<code>0</code>。</p><p><strong>示例</strong>：</p><p>输入：s = 7, nums = [2,3,1,2,4,3] 输出：2 解释：子数组 [4,3]是该条件下的长度最小的子数组。 提示： 1 &lt;= target &lt;= 10^9 1 &lt;=nums.length &lt;= 10 1 &lt;= nums[i] &lt;= 10^5</p><blockquote><p><strong>思路</strong>：采用滑动窗口的方法，<code>i</code>为滑动窗口的终止位置，<code>j</code>为滑动窗口的起始位置，<code>Sum</code>记录当前<code>j-&gt;i</code>连续子数组的和，<code>i-j+1</code>为当前连续子数组的长度。</p></blockquote><p><strong>Python：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">s = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>())<br>nums = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">int</span>, <span class="hljs-built_in">input</span>().split()))<br>l = <span class="hljs-built_in">len</span>(nums)<br>Sum = <span class="hljs-number">0</span><br>Len = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)<br>j = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(l):<br>    Sum += nums[i]                 <span class="hljs-comment"># Sum为当前子数组的和</span><br>    <span class="hljs-keyword">while</span> Sum &gt;= s:<br>        Len = <span class="hljs-built_in">min</span>(Len, i - j + <span class="hljs-number">1</span>)  <span class="hljs-comment"># i - j + 1 为当前子数组的长度</span><br>        Sum -= nums[j]             <span class="hljs-comment"># Sum减去当前子数组的起始元素</span><br>        j += <span class="hljs-number">1</span><br>Len = <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> Len == <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>) <span class="hljs-keyword">else</span> Len <span class="hljs-comment"># 如果Len没有被赋值，说明没有符合条件的子数组，返回0</span><br><span class="hljs-built_in">print</span>(Len)<br></code></pre></td></tr></table></figure><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">minSubArrayLen</span><span class="hljs-params">(<span class="hljs-type">int</span> target, vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums)</span> </span>&#123;<br>        <span class="hljs-type">int</span> j=<span class="hljs-number">0</span>;<br>        <span class="hljs-type">int</span> length = INT_MAX;<br>        <span class="hljs-type">int</span> sum = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;nums.<span class="hljs-built_in">size</span>(); i++)&#123;     <span class="hljs-comment">//滑动窗口</span><br>            sum += nums[i];<br>            <span class="hljs-keyword">while</span>(sum &gt;= target)&#123;<br>                length = <span class="hljs-built_in">min</span>(length, i-j+<span class="hljs-number">1</span>); <span class="hljs-comment">//记录最小长度</span><br>                sum -= nums[j];              <span class="hljs-comment">//缩小窗口</span><br>                j++;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> length==INT_MAX ? <span class="hljs-number">0</span> : length;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h4 id="力扣904-水果成篮">力扣904 <ahref="https://leetcode.cn/problems/fruit-into-baskets/description/">水果成篮</a></h4><p>你正在探访一家农场，农场从左到右种植了一排果树。这些树用一个整数数组<code>fruits</code>表示，其中<code>fruits[i]</code>是第<code>i</code>棵树上的水果<strong>种类</strong>。</p><p>你想要尽可能多地收集水果。然而，农场的主人设定了一些严格的规矩，你必须按照要求采摘水果：</p><ul><li>你只有<strong>两个</strong>篮子，并且每个篮子只能装<strong>单一类型</strong>的水果。每个篮子能够装的水果总量没有限制。</li><li>你可以选择任意一棵树开始采摘，你必须从<strong>每棵</strong>树（包括开始采摘的树）上<strong>恰好摘一个水果</strong>。采摘的水果应当符合篮子中的水果类型。每采摘一次，你将会向右移动到下一棵树，并继续采摘。</li><li>一旦你走到某棵树前，但水果不符合篮子的水果类型，那么就必须停止采摘。</li></ul><p>给你一个整数数组<code>fruits</code>，返回你可以收集的水果的<strong>最大</strong>数目。</p><p><strong>示例：</strong></p><p>输入：fruits = [1,2,1] 输出：3 解释：可以采摘全部 3 棵树。</p><blockquote><p><strong>思路</strong>：题目的意思翻译一下就是：一个连续的字串中包含两种不同数字的最大长度，因此可以使用滑动窗口，并且使用字典记录元素的个数。</p><p>当窗口中元素的字典长度小于2，扩大窗口；大于2，开始缩小窗口；</p><p>同时相应元素的字典值减小，当减小到0时pop出字典，字典的长度就会减小1。</p></blockquote><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">totalFruit</span>(<span class="hljs-params">self, fruits: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        cnt = Counter()<br>        ans = left = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> right <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(fruits)): <span class="hljs-comment">#right为窗口右边界</span><br>            cnt[fruits[right]] += <span class="hljs-number">1</span>      <span class="hljs-comment">#当前窗口内元素字典记录加1</span><br>            <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(cnt) &gt; <span class="hljs-number">2</span>:          <span class="hljs-comment">#当窗口内元素种类大于2时，说明此时的窗口已经不满足条件，开始缩小窗口</span><br>                cnt[fruits[left]] -= <span class="hljs-number">1</span>   <span class="hljs-comment">#窗口左边界元素字典记录减1</span><br>                <span class="hljs-keyword">if</span> cnt[fruits[left]] == <span class="hljs-number">0</span>:  <span class="hljs-comment">#如果窗口左边界元素字典记录为0，窗口内不再有该元素，删除该元素</span><br>                    cnt.pop(fruits[left])<br>                left += <span class="hljs-number">1</span>                   <span class="hljs-comment">#窗口左边界右移</span><br>            ans = <span class="hljs-built_in">max</span>(ans, right - left + <span class="hljs-number">1</span>) <span class="hljs-comment">#更新最大窗口</span><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">totalFruit</span>(<span class="hljs-params">self, fruits: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        cnt =  defaultdict(<span class="hljs-built_in">int</span>)<br>        ans = left = <span class="hljs-number">0</span><br>        tot = <span class="hljs-number">0</span>                          <span class="hljs-comment">#当前窗口内元素种类数</span><br>        <span class="hljs-keyword">for</span> right <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(fruits)): <span class="hljs-comment">#right为窗口右边界</span><br>            cnt[fruits[right]] += <span class="hljs-number">1</span>      <span class="hljs-comment">#当前窗口内元素字典记录加1</span><br>            <span class="hljs-keyword">if</span> cnt[fruits[right]] == <span class="hljs-number">1</span>:  <span class="hljs-comment">#如果当前窗口内元素字典记录为1，说明该元素是新元素，tot加1</span><br>                tot += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> tot &gt; <span class="hljs-number">2</span>:               <span class="hljs-comment">#当tot大于2时，说明此时的窗口已经不满足条件，开始缩小窗口</span><br>                cnt[fruits[left]] -= <span class="hljs-number">1</span>   <span class="hljs-comment">#窗口左边界元素字典记录减1</span><br>                <span class="hljs-keyword">if</span> cnt[fruits[left]] == <span class="hljs-number">0</span>: <span class="hljs-comment">#如果窗口左边界元素字典记录为0，窗口内不再有该元素，tot减1</span><br>                    tot -= <span class="hljs-number">1</span><br>                left += <span class="hljs-number">1</span>                   <span class="hljs-comment">#窗口左边界右移</span><br>            ans = <span class="hljs-built_in">max</span>(ans, right - left + <span class="hljs-number">1</span>) <span class="hljs-comment">#更新最大窗口</span><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">totalFruit</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; fruits)</span> </span>&#123;<br>        <span class="hljs-type">int</span> len = fruits.<span class="hljs-built_in">size</span>();<br>        unordered_map&lt;<span class="hljs-type">int</span>, <span class="hljs-type">int</span>&gt; cnt;    <span class="hljs-comment">//创建一个哈希表字典，记录窗口中水果出现的次数</span><br><br>        <span class="hljs-type">int</span> left = <span class="hljs-number">0</span>,  ans = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> right=<span class="hljs-number">0</span>; right &lt; len; right++)&#123; <span class="hljs-comment">//窗口右边界</span><br>            cnt[fruits[right]]++;               <span class="hljs-comment">//当前窗口中的水果的字典值加一</span><br>            <br>            <span class="hljs-keyword">while</span>(cnt.<span class="hljs-built_in">size</span>() &gt; <span class="hljs-number">2</span>)&#123;              <span class="hljs-comment">//如果窗口中水果种类大于2，开始缩小窗口</span><br>                <span class="hljs-keyword">auto</span> it = cnt.<span class="hljs-built_in">find</span>(fruits[left]);   <span class="hljs-comment">//从窗口中左边界开始</span><br>                it-&gt;second--;                       <span class="hljs-comment">//当前水果的字典值减一</span><br>                <span class="hljs-keyword">if</span>(it-&gt;second == <span class="hljs-number">0</span>)&#123;                <span class="hljs-comment">//如果当前水果的次数为0，从哈希表字典中删除</span><br>                    cnt.<span class="hljs-built_in">erase</span>(it);<br>                &#125;<br>                left++;                             <span class="hljs-comment">//左边界右移，缩小窗口</span><br>            &#125;<br>            ans = <span class="hljs-built_in">max</span>(ans, right - left + <span class="hljs-number">1</span>);       <span class="hljs-comment">//更新最大窗口</span><br>        &#125;<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h4 id="力扣76-最小覆盖子串">力扣76 <ahref="https://leetcode.cn/problems/minimum-window-substring/description/">最小覆盖子串</a></h4><p>给你一个字符串<code>s</code>、一个字符串<code>t</code>。返回<code>s</code>中涵盖<code>t</code>所有字符的最小子串。如果<code>s</code>中不存在涵盖<code>t</code>所有字符的子串，则返回空字符串<code>""</code>。</p><p><strong>示例 ：</strong></p><p>输入：s = "ADOBECODEBANC", t = "ABC" 输出："BANC" 解释：最小覆盖子串"BANC" 包含来自字符串 t 的 'A'、'B' 和 'C'。</p><blockquote><p><strong>思路</strong>：用i,j表示滑动窗口的左边界和右边界，通过改变i,j来扩展和收缩滑动窗口，当这个窗口包含的元素满足条件，即包含字符串T的所有元素，记录下这个滑动窗口的长度j-i+1，这些长度中的最小值就是要求的结果。</p></blockquote><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">minWindow</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span>, t: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>        cnt = Counter(t)    <span class="hljs-comment">#记录每个字符出现的次数</span><br>        cntLen = <span class="hljs-built_in">len</span>(t)<br>        left = <span class="hljs-number">0</span><br>        ans = (<span class="hljs-number">0</span>,<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>))<br>        <span class="hljs-keyword">for</span> right <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(s)): <span class="hljs-comment">#right为窗口右边界</span><br>            <span class="hljs-keyword">if</span> cnt[s[right]] &gt; <span class="hljs-number">0</span>:   <span class="hljs-comment">#如果s[right]在t中出现过</span><br>                cntLen -= <span class="hljs-number">1</span>         <span class="hljs-comment">#t中字符总数减1</span><br>            cnt[s[right]] -= <span class="hljs-number">1</span>      <span class="hljs-comment">#当前记录减1</span><br>            <span class="hljs-keyword">while</span> cntLen == <span class="hljs-number">0</span>:      <span class="hljs-comment">#当cntLen为0时，说明此时的窗口已经包含了t中的所有字符</span><br>                <span class="hljs-keyword">if</span> (right - left + <span class="hljs-number">1</span>) &lt; (ans[<span class="hljs-number">1</span>] - ans[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>):  <span class="hljs-comment">#更新最小窗口</span><br>                    ans = (left, right)<br>                <span class="hljs-keyword">if</span> cnt[s[left]] == <span class="hljs-number">0</span>:  <span class="hljs-comment">#如果s[left]为t中的字符</span><br>                    cntLen += <span class="hljs-number">1</span>        <span class="hljs-comment">#t中字符总数加1</span><br>                cnt[s[left]] += <span class="hljs-number">1</span>      <span class="hljs-comment">#当前记录加1</span><br>                <span class="hljs-built_in">print</span>(left, right, ans, cntLen, cnt)<br>                left += <span class="hljs-number">1</span>              <span class="hljs-comment">#窗口左边界右移</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;&#x27;</span> <span class="hljs-keyword">if</span> ans[<span class="hljs-number">1</span>]&gt;<span class="hljs-built_in">len</span>(s) <span class="hljs-keyword">else</span> s[ans[<span class="hljs-number">0</span>]:ans[<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><p><strong>C++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function">string <span class="hljs-title">minWindow</span><span class="hljs-params">(string s, string t)</span> </span>&#123;<br>        <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">need</span><span class="hljs-params">(<span class="hljs-number">128</span>, <span class="hljs-number">0</span>)</span></span>;       <span class="hljs-comment">//记录t中每个字符出现的次数</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">char</span> c : t)&#123;                <br>            need[c]++;                  <span class="hljs-comment">//创建一个哈希表字典</span><br>        &#125;<br>        <span class="hljs-type">int</span> count = t.<span class="hljs-built_in">length</span>();<br>        <span class="hljs-type">int</span> l = <span class="hljs-number">0</span>, r = <span class="hljs-number">0</span>, start = <span class="hljs-number">0</span>, size = INT_MAX;<br>        <span class="hljs-keyword">while</span>(r&lt;s.<span class="hljs-built_in">length</span>())&#123;            <span class="hljs-comment">//滑动窗口右边界</span><br>            <span class="hljs-type">char</span> c = s[r];<br>            <span class="hljs-keyword">if</span>(need[c] &gt; <span class="hljs-number">0</span>)&#123;           <span class="hljs-comment">//如果s中的字符在t中出现过</span><br>                count--;               <span class="hljs-comment">//t中字符出现次数减一</span><br>            &#125;<br>            need[c]--;                 <span class="hljs-comment">//更新哈希表</span><br>            <span class="hljs-keyword">if</span>(count == <span class="hljs-number">0</span>)&#123;            <span class="hljs-comment">//如果t中的字符都在窗口中</span><br>                <span class="hljs-keyword">while</span>(<span class="hljs-literal">true</span>)&#123;<br>                    <span class="hljs-keyword">if</span>(need[s[l]] == <span class="hljs-number">0</span>)&#123;    <span class="hljs-comment">//如果s中的字符在t中，直接跳出循环</span><br>                        <span class="hljs-keyword">break</span>;<br>                    &#125;<br>                    need[s[l]]++;           <span class="hljs-comment">//否则左边界右移，缩小窗口</span><br>                    l++;<br>                &#125;<br>                <span class="hljs-keyword">if</span>(r-l+<span class="hljs-number">1</span>&lt;size)&#123;             <span class="hljs-comment">//更新最小窗口</span><br>                    start = l, size = r-l+<span class="hljs-number">1</span>;<br>                &#125;<br>                need[s[l]]++;               <span class="hljs-comment">//为t中的字符左边界右移，开启新的窗口滑动</span><br>                count++;                    <span class="hljs-comment">//t中字符出现次数加一</span><br>                l++;<br>            &#125;<br>            r++;<br>        &#125;<br>    <span class="hljs-keyword">return</span> size == INT_MAX ? <span class="hljs-string">&quot;&quot;</span> : s.<span class="hljs-built_in">substr</span>(start, size);<br><br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h4 id="滑动窗口总结">滑动窗口总结</h4><p>​所谓滑动窗口，就是不断的调节子序列的起始位置和终止位置，从而得出我们要想的结果。如果窗口中的子串<code>没有满足条件，我们就扩宽窗口，右边界滑动</code>；而一旦<code>满足条件，我们就需要缩小窗口，左边界滑动</code>，直到不满足条件。如果条件是字符串或者是需要记录窗口中元素的情况，我们就需要<code>结合字典</code>，通过计算字典长度来进行求解。</p>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>刷题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>相机参数</title>
    <link href="/posts/63805/"/>
    <url>/posts/63805/</url>
    
    <content type="html"><![CDATA[<h3 id="相机坐标系">1.相机坐标系</h3><p><img src="1.jpg" /></p><p>相机中有四个坐标系</p><ul><li>世界坐标系：可以任意指定<spanclass="math inline">\(x_w\)</span>轴和<spanclass="math inline">\(y_w\)</span>轴</li><li>相机坐标系：原点位于小孔，z轴与光轴重合，<spanclass="math inline">\(x_c\)</span>轴和<spanclass="math inline">\(y_c\)</span>轴平行投影面</li><li>图像坐标系：原点位于光轴和投影面的交点，<spanclass="math inline">\(x_p\)</span>轴和<spanclass="math inline">\(y_p\)</span>轴平行投影面</li><li>像素坐标系：从小孔向投影面方向看，投影面的左上角为原点，uv轴和投影面两边重合</li></ul><p>一般来说，标定的过程分为两个部分：</p><ul><li>第一步是从世界坐标系转为相机坐标系，这一步是三维点到三维点的转换，包括R，T（相机外参，确定了相机在某个三维空间中的位置和朝向）等参数</li><li>第二部是从相机坐标系转为成像平面坐标系（像素坐标系），这一步是三维点到二维点的转换，包括K（相机内参,是对相机物理特性的近似）等参数</li></ul><h3 id="齐次坐标系">2.齐次坐标系</h3><p>齐次坐标(Homogeneouscoordinate)就是将一个原本是n维的向量用一个n+1维向量来表示，是指一个用于投影几何里的坐标系统，如同用于欧氏几何里的笛卡儿坐标一般。给出点的齐次表达式[XY H]，就可求得其二维笛卡尔坐标，即$ [ X : Y : H ] = [ : : ] = [ X : Y :1 ]$， 这个过程称为归一化处理。在几何意义上，相当于把发生在三维空间的变换限制在H=1的平面内。同时在齐次坐标系下<code>(1, 2, 3), (2, 4, 6),(4, 8, 12)</code>对应同一个欧几里得点<code>(1/3, 2/3)</code>，因此这些点是同一个点，这使得在透视空间里，两条平行线可以相交得到了解决(近大远小)。</p><p><img src="3.png?100x100" /></p><p>许多图形应用涉及到几何变换，主要包括<strong>平移、旋转、缩放</strong>。以矩阵表达式来计算这些变换时，平移是矩阵相加，旋转和缩放则是矩阵相乘，引入齐次坐标的目的主要是合并矩阵运算中的乘法和加法。引入齐次坐标后，平移、旋转、缩放可以表示为：<span class="math display">\[\begin{align}   &amp; 平移变换：    \left[\begin{array}{lll}    x^{\prime} &amp; y^{\prime} &amp; 1    \end{array}\right]=\left[\begin{array}{lll}    x &amp; y &amp; 1    \end{array}\right]\left|\begin{array}{ccc}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    \mathrm{~d} x &amp; \mathrm{~d} y &amp; 1    \end{array}\right|    \quad \\   &amp; 旋转变换：    \left[\begin{array}{lll}    x^{\prime} &amp; y^{\prime} &amp; 1    \end{array}\right]=\left[\begin{array}{lll}    x &amp; y &amp; 1    \end{array}\right]\left|\begin{array}{ccc}    \cos{\theta} &amp; \sin{\theta} &amp; 0 \\    -\sin{\theta} &amp; \cos{\theta} &amp; 0 \\    0 &amp; 0  &amp; 1    \end{array}\right|    \quad \\   &amp; 缩放变换：    \left[\begin{array}{lll}    x^{\prime} &amp; y^{\prime} &amp; 1    \end{array}\right]=\left[\begin{array}{lll}    x &amp; y &amp; 1    \end{array}\right]\left|\begin{array}{ccc}    S_x &amp; 0 &amp; 0 \\    0 &amp; S_y &amp; 0 \\    0 &amp; 0  &amp; 1    \end{array}\right|\end{align}\]</span></p><h3 id="相机内外参数">3.相机内外参数</h3><ol type="1"><li><p>相机外部参数</p><blockquote><ul><li>3个旋转矩阵参数R(绕x,y,z轴)</li><li>3个平移矩阵参数T(沿x,y,z轴)</li></ul></blockquote><p>camera to world(c2w)：</p><p>设<span class="math inline">\(P_c\)</span>为<spanclass="math inline">\(P\)</span>在相机坐标系下的坐标，<spanclass="math inline">\(P_w\)</span>是其在世界坐标系下的坐标，可以使用一个旋转矩阵<code>R</code>和一个平移向量<code>t</code>，将<spanclass="math inline">\(P_c\)</span>变换为<spanclass="math inline">\(P_w\)</span>: <span class="math display">\[P_c = RP_w + t\]</span>其中<code>R</code>是一个3×3的旋转矩阵，<code>t</code>是3×1的平移向量，一下为齐次坐标形式：<span class="math display">\[\left[\begin{array}{c}X_{c} \\Y_{c} \\Z_{c}\end{array}\right]=\left[\begin{array}{lll}R_{11} &amp; R_{12} &amp; R_{13} \\R_{21} &amp; R_{22} &amp; R_{23} \\R_{31} &amp; R_{32} &amp; R_{33}\end{array}\right]\left[\begin{array}{l}X_{w} \\Y_{w} \\Z_{w}\end{array}\right]+\left[\begin{array}{l}t_{1} \\t_{2} \\t_{3}\end{array}\right]\quad \Rightarrow  \quad\left[\begin{array}{c}X_{c} \\Y_{c} \\Z_{c} \\1\end{array}\right]=\left[\begin{array}{cccc}R_{11} &amp; R_{12} &amp; R_{13} &amp; t_{1} \\R_{21} &amp; R_{22} &amp; R_{23} &amp; t_{2} \\R_{31} &amp; R_{32} &amp; R_{33} &amp; t_{3} \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{c}X_{w} \\Y_{w} \\Z_{w} \\1\end{array}\right]\]</span></p><p>将旋转矩阵<code>R</code>和平移向量<code>t</code>带入： <spanclass="math display">\[\left[\begin{array}{c}X_{c} \\Y_{c} \\Z_{c} \\1\end{array}\right]=\left[\begin{array}{cc}R &amp; t \\0^T &amp; 1\\\end{array}\right]\left[\begin{array}{c}X_{w} \\Y_{w} \\Z_{w} \\1\end{array}\right]\]</span> 上面就推导出了相机的<code>外部参数T</code>: <spanclass="math display">\[T=\left[\begin{array}{cc}R &amp; t \\0^T &amp; 1\\\end{array}\right]\]</span></p></li><li><p>相机内部参数</p><p>内参矩阵K为： <span class="math display">\[K = \left|\begin{array}{ccc}    f_x &amp; 0 &amp; u_0 \\    0 &amp; f_y &amp; v_0 \\    0 &amp; 0  &amp; 1    \end{array}\right|\]</span> <span class="math inline">\(f_x=\frac{f}{d_x} \:,f_y=\frac{f}{d_y}\)</span></p><ul><li><p>f：焦距，单位毫米</p></li><li><p>dx：像素x方向宽度，单位毫米(一个像素在感光板上是多少毫米)</p></li><li><p>f/dx：使用像素来描述x轴方向焦距的长度</p></li><li><p>f/dy：使用像素来描述y轴方向焦距的长度</p></li><li><p>u0,v0,主点的实际位置，单位也是像素</p></li></ul><blockquote><p>相机中心射出的一条光线在成像平面上的投影点。它也被称为“主光轴交点”或“光心”。</p></blockquote></li></ol><h3 id="参考">参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><ahref="https://www.cnblogs.com/wangguchangqing/p/8126333.html#autoid-0-5-0">SLAM入门之视觉里程计(2)：相机模型(内参数，外参数)</a><a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>计算机图形学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图形学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视图变换 Viewing transformation</title>
    <link href="/posts/64372/"/>
    <url>/posts/64372/</url>
    
    <content type="html"><![CDATA[<h3 id="视图变换-viewing-transformation">1.视图变换 ViewingTransformation</h3><p>视图变换的目的是将三维空间中的点<span class="math inline">\((x, y,z)\)</span>(在世界坐标系中)映射到平面图像中(二维坐标系)，以像素为基本表示单位。类似通过相机拍照得到一张相片。视图变换主要包括三个步骤：</p><blockquote><ul><li>相机变换(camera/view transformation)</li><li>投影变换(projection transformation)</li><li>视口变换(viewport transformation)</li></ul></blockquote><figure><img src="6.png" alt="坐标系定义" /><figcaption aria-hidden="true">坐标系定义</figcaption></figure><h3 id="相机变换-camera-transformation">2.相机变换 CameraTransformation</h3><figure><img src="1.jpg?100×100" alt="相机坐标系移动" /><figcaption aria-hidden="true">相机坐标系移动</figcaption></figure><p>相机变换的目的是得到所有可是物体与相机的相对位置，通常包括平移、旋转、缩放。</p><p>规定相机拍摄方向朝向-Z，相机的位置位于e，相机的正上方用向量t来表示，相机的朝向用g表示，<spanclass="math inline">\(\overrightarrow{e}=(x_e, y_e,z_e)\)</span>。首先将相机点平移至世界坐标原点，平移矩阵为： <spanclass="math display">\[T_{\text {view }}=\left[\begin{array}{cccc}1 &amp; 0 &amp; 0 &amp; -x_{e} \\0 &amp; 1 &amp; 0 &amp; -y_{e} \\0 &amp; 0 &amp; 1 &amp; -z_{e} \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span>然后，对相机坐标进行旋转变换，使其与世界坐标系重合。需要将相机朝向g旋转到-Z轴上，t旋转到Y轴上，再通过g叉乘t的方向旋转到X。然而，这个旋转对应的旋转矩阵并不容易写出，但是如果将Z旋转到-g，将Y旋转到t，将X旋转到g叉积t的方向，直接取旋转矩阵的逆矩阵<spanclass="math inline">\(R^{-1}=(u,v,w)\)</span>即可<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[旋转矩阵(Rotate Matrix)的性质分析](https://www.cnblogs.com/caster99/p/4703033.html)">[1]</span></a></sup>，其中<spanclass="math inline">\(u=\hat{g} \times \hat{t},v=\hat{t},w =-\hat{g}\)</span>，因此旋转矩阵的逆矩阵和旋转矩阵可以写成： <spanclass="math display">\[R_{\text {view }}^{-1}=\left[\begin{array}{cccc}x_{\hat{g} \times \hat{t}} &amp; x_{t} &amp; x_{-g} &amp; 0 \\y_{\hat{g} \times \hat{t}} &amp; y_{t} &amp; y_{-g} &amp; 0 \\z_{\hat{g} \times \hat{t}} &amp; z_{t} &amp; z_{-g} &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\quad \Rightarrow  \quadR_{\text {view }}=\left[\begin{array}{cccc}x_{\hat{g} \times \hat{t}} &amp; y_{\hat{g} \times \hat{t}}  &amp;z_{\hat{g} \times \hat{t}}  &amp; 0 \\x_{t} &amp; y_{t} &amp; z_{t} &amp; 0 \\x_{-g} &amp;  y_{-g} &amp; z_{-g} &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span> 所以最终的相机变换矩阵为：<span class="math inline">\(M_{\text{view }}=R_{\text {view }}T_{\text {view }}\)</span>。</p><blockquote><p>PS:从世界坐标系变换到相机坐标系属于刚体变换：即物体不会发生形变，只需要进行旋转和平移。</p></blockquote><h3 id="投影变换-projection-transformation">3.投影变换 ProjectionTransformation</h3><figure><img src="3.jpg?100×100" alt="投影变换" /><figcaption aria-hidden="true">投影变换</figcaption></figure><p>投影变换的目的是将相机空间中的点映射到<spanclass="math inline">\([-1, 1]^3\)</span>的立方体上，并且相机<spanclass="math inline">\(e=0\)</span>的射线穿过里立方体中心。这样的立方体叫做规范视图体(CanonicalView Volume)或者标准化设备坐标系(normalized devicecoordinates)。投影变换可以分为两个步骤进行：</p><blockquote><ul><li>正交投影(orthographic projection)</li><li>透视投影(perspection projection)</li></ul></blockquote><ol type="1"><li><p>透视投影</p><p>透视投影就是最类似人眼所看东西的方式，遵循近大远小，通过投影到平面来进行解释。</p><figure><img src="4.png?100×100" alt="透视投影" /><figcaption aria-hidden="true">透视投影</figcaption></figure><p>图中的原点代表视点，<code>Z=-n</code>表示近平面(投影平面)，<code>Z=-z</code>表示远平面，需要进行压缩。利用相似三角形可以计算出<spanclass="math inline">\(y^{&#39;}=\frac{n}{z}y,x^{&#39;}=\frac{n}{z}x\)</span>，假定透视矩阵为P，我们计算透视投影后的齐次坐标。<span class="math display">\[P\left[\begin{array}{c}x  \\y  \\z  \\1\end{array}\right]=\left[\begin{array}{c}nx  \\ny  \\?  \\z\end{array}\right]\quad \Rightarrow  \quadP=\left[\begin{array}{cccc}n &amp; 0 &amp; 0 &amp; 0 \\0 &amp; n &amp; 0 &amp; 0 \\? &amp; ? &amp; ? &amp; ? \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span>中间的<code>?</code>可以通过远平面<code>Z=-f</code>和近平面<code>Z=-n</code>的值来计算，当<code>Z=-f</code>时，前面的单独<code>?</code>为<spanclass="math inline">\(f^2\)</span>，当<code>Z=-n</code>时，前面的单独<code>?</code>为<spanclass="math inline">\(n^2\)</span>。假设第三行为<spanclass="math inline">\((0,0,A,B)\)</span>，分别带入<code>n</code>和<code>f</code>可以列出两个等式：<span class="math display">\[\begin{array}{l}A_{n}+B=n^{2} \\A_{f}+B=f^{2} \\\end{array}\quad \Rightarrow \quad\begin{array}{l}A=n+f \\B=-nf \\\end{array}\]</span></p><p>最后的透视投影的变换矩阵为： <span class="math display">\[M_{persp \to -ortho} = P=\left[\begin{array}{cccc}n &amp; 0 &amp; 0&amp; 0 \\0 &amp; n &amp; 0 &amp; 0 \\0 &amp; 0 &amp; n+f &amp; -nf \\0&amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span>通过计算可知，远平面通过透视投影矩阵的变换后，该平面会沿着Z轴的反向移动，即远离近平面。</p></li><li><p>正交投影</p><figure><img src="2.jpg?100×100" alt="正交投影" /><figcaption aria-hidden="true">正交投影</figcaption></figure><p>正交投影变换坐标的相对位置都不会改变，所有光线都是平行传播，只需将物体全部转换到<spanclass="math inline">\([-1,1]^3\)</span>的立方体中，主要的操作有平移和旋转。</p><p>通过计算可以求得立方体的中心点为<span class="math inline">\((x_0,y_0, z_0)\)</span>，其中<span class="math inline">\(x_0=-\frac{r+l}{2},y_0=-\frac{t+b}{2},z_0=-\frac{n+f}{2}\)</span>。因此平移矩阵可以表示为:<span class="math display">\[T=\left[\begin{array}{cccc}1 &amp; 0 &amp; 0 &amp; -\frac{r+l}{2} \\0 &amp; 1 &amp; 0 &amp; -\frac{t+b}{2} \\0 &amp; 0 &amp; 1 &amp; -\frac{n+f}{2} \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span>接下来就需要求缩放矩阵，-1到1的间距为2，而物体在各个轴上的间距分别为<spanclass="math inline">\(\:l-r,b-t,n-f\)</span>，因此，在各个轴方向的缩放因子可以表示为<spanclass="math inline">\(S_x=\frac{2}{l-r},S_y=\frac{2}{b-t},S_z=\frac{2}{n-f}\)</span>。所以缩放矩阵可以表示为：<span class="math display">\[S=\left[\begin{array}{cccc}\frac{2}{l-r} &amp; 0 &amp; 0 &amp; 0 \\0 &amp; \frac{2}{b-t}&amp; 0 &amp; 0 \\0 &amp; 0 &amp; \frac{2}{n-f} &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span>计算完两个矩阵之后，可以知道最终的正交投影矩阵为两个矩阵的乘积<spanclass="math inline">\(M_{orth}=S \times T\)</span>: <spanclass="math display">\[M_{orth}=\left[\begin{array}{cccc}\frac{2}{l-r} &amp; 0 &amp; 0 &amp; 0 \\0 &amp; \frac{2}{b-t}&amp; 0 &amp; 0 \\0 &amp; 0 &amp; \frac{2}{n-f} &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{cccc}1 &amp; 0 &amp; 0 &amp; -\frac{r+l}{2} \\0 &amp; 1 &amp; 0 &amp; -\frac{t+b}{2} \\0 &amp; 0 &amp; 1 &amp; -\frac{n+f}{2} \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]=\left[\begin{array}{cccc}\frac{2}{l-r} &amp; 0 &amp; 0 &amp; -\frac{r+l}{2} \\0 &amp; \frac{2}{b-t}&amp; 0 &amp; -\frac{t+b}{2} \\0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{2} \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span> 3.投影变换矩阵</p><p>通过上述的两个矩阵的乘积可以得出最终的变换矩阵<spanclass="math inline">\(M_{per} = M_{orhto}M_{persp \to -ortho}\)</span><span class="math display">\[M_{per} =\left[\begin{array}{cccc}\frac{2n}{l-r} &amp; 0 &amp; -\frac{l+r}{l-r} &amp; 0 \\0 &amp; \frac{2n}{b-t}&amp; -\frac{b+t}{b-t} &amp; 0 \\0 &amp; 0 &amp; \frac{n+f}{n-f} &amp; -\frac{2nf}{n-f} \\0 &amp; 0 &amp; 1 &amp; 0\end{array}\right]\]</span></p><h3 id="视口变换-viewport-transformation">4.视口变换 viewporttransformation</h3><p>经过上述变换，可以将任意三维空间中的物体投影到标准立方体上，但是之后还需要投影到<spanclass="math inline">\(2\times2\)</span>的二维平面(栅格图像)上进行显示，高度为H，宽度为W，单位为像素。所以，需要将标准立方体中的中的点，转换到屏幕上，所以还是需要先平移，再缩放，形式同正交矩阵：<span class="math display">\[M_{viewport}=\left[\begin{array}{cccc}\frac{W}{2} &amp; 0 &amp; 0 &amp; \frac{W}{2} \\0 &amp; \frac{H}{2}&amp; 0 &amp; \frac{H}{2} \\0 &amp; 0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span></p><blockquote><p>栅格图像(RasterImage)，也称为位图(Bitmap)，是由像素阵列组成的数字图像。在栅格图像中，每个像素都包含一个特定的颜色值或灰度值，以描述图像中相应位置的颜色和亮度。与矢量图形不同，栅格图像是像素化图像，它通常使用像素阵列来表示图像。每个像素都具有一个X和Y坐标，并包含一个或多个数字值来表示其颜色信息。这些数字值通常使用8位或更高位深度来表示，以提供足够的精度来描述图像细节。</p></blockquote></li></ol><blockquote><p>PS:NeRF中只需要进行投影变换。因为NeRF将相对于世界坐标的相机坐标点作为MLP的输入，它也没有使用视口变换，因为信息是从多层感知器(MLP)中隐式查询而不是从测量对象构建的。</p></blockquote><h3 id="参考">参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><ahref="https://www.cnblogs.com/caster99/p/4703033.html">旋转矩阵(RotateMatrix)的性质分析</a><a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><ahref="https://zhuanlan.zhihu.com/p/144323332">计算机图形学基础变换矩阵总结(缩放，旋转，位移)</a><a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>计算机图形学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图形学</tag>
      
      <tag>NeRF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hello-blog</title>
    <link href="/posts/39849/"/>
    <url>/posts/39849/</url>
    
    <content type="html"><![CDATA[<h4 id="开启博客之旅">开启博客之旅~</h4><p>终于把我的博客搭好了，五一假期也结束了，那就开启新的学习之旅吧。享受最后的两年校园时光，做想做的，学想学的，愿毕业时自信又阳光！！！</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
